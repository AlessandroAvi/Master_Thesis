% thesis.tex (also saved as simple.tex) -- a simple thesis document
% for demonstrating dalthesis.cls class file, or to use as a starting
% document for writing a thesis.
% If you are not familiar with TeX and LaTeX, the first thing that you
% can learn that line comments start with the percent sign (%), so
% these lines are ignored by the system.  Feel free to change them or
% delete them.
\documentclass[12pt]{report}
\newcommand{\mychapter}[2]{
    \setcounter{chapter}{#1}
    \setcounter{section}{0}
    \chapter*{#2}
    \addcontentsline{toc}{chapter}{#2}
    }
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pdfpages}
\usepackage[toc,page]{appendix}
\usepackage{afterpage}
\usepackage{multirow}
\usepackage[style=ieee,backend=biber,citetracker=true]{biblatex}


% BIBLIOGRAPHY DEL MARCO
%\usepackage{apacite}
%\usepackage[backend = biber, backref = false, style=apa]{biblatex}


\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{0}%
    \newpage}
\usepackage{hyperref}

\pagenumbering{roman}



\addbibresource{thesis}
%\renewcommand{\footcite}[1]{\cite{#1}\hypercite{#1}}
\begin{document}


\begin{figure}
 \centering 
 \includepdf[pages=-]{Figures/frontespizio.pdf}
\end{figure}

\afterpage{\blankpage}

\chapter*{}
\vspace*{\fill}
\textit{"Frase"} 
\begin{flushright}
Mario Rossi
\end{flushright}
\vspace*{\fill}

\afterpage{\blankpage}





%\afterpage{\blankpage}
\tableofcontents
%\afterpage{\blankpage}
\listoffigures
\listoftables
\afterpage{\blankpage}



%\mainmatter

\mychapter{0}{Introduction}
\label{Intro}
\pagenumbering{arabic}

Machine learning (ML) applications on small devices, known as TinyML, is becoming more and more popular. The usage of this type of technology on micro controllers (MCU) is becoming more and more indispensable and helpful in several fields such as industrial applications, agricultural automation, autonomous driving, and human-machine interaction. One of the main fields in which TinyML is well suited is Internet of Things (IoT). Here, machine learning is applied on small devices and it can be exploited to revolutionize the basics of IoT networks.\\
The ability of embedded systems to perform high-level and smart data elaboration makes it possible for the IoT pipeline to change from cloud computing to edge computing. This transformation comes with great benefits and additional challenges.
First of all, the traffic on IoT networks is drastically reduced. In fact, by performing inferences and predictions directly on the edge the raw data gets compressed into smaller sequences that are dense of information, reducing the quantity of data moving in the IoT networks. This allows to diminish the energy consumption dedicated to the entire system as well as the system responsiveness and efficiency.
Edge computing lowers the traffic in IoT systems but also reduces the computational weight in cloud servers. This results in reduced times of computation and communication between edge and cloud which, if combined with the ability of the MCU to perform autonomous decision, reduces the latency of real time applications improving the overall experience.
Moreover, IoT network privacy issues can be addressed by reducing transmitted data, and consequently reducing the possibility to have unwanted interceptions.
At last the usage of ML on small devices allows to better customize the device, and make the devices better suited for specific jobs. \\
Of course, the application of such a technology comes with a cost, which is the increased complexity and higher amount of vulnerabilities. Thus, it is necessary to set up robust systems that are able to ensure the system security (due to the high number of vulnerable nodes), and high performances, no matter the limitation of the device. It is in fact known that the main downsides of embedded systems and small MCUs are their limited hardware, small memories, and low-capacity batteries. Another important aspect concerning TinyML is the training and deployment of the model, which is typically performed on a powerful device and later loaded on the MCU using compression strategies. The main challenge is how the compression of big and well performing models is performed, especially because the model needs to maintain high accuracy with a low memory footprint. The creation of efficient and optimized compression strategies has been one of the main focus of recent research in the TinyML. \\
Another relevant challenge for the application of ML in IoT systems comes directly from the environment in which IoT smart nodes are deployed. Depending on the specific application, it is usually the case that the context in which an IoT device works is not characterized by a static behaviour. Meaning that the phenomenon to be monitored is able to change or evolve over time, thus data recorded can consequently evolve and change its main features. This can make difficult using ML models because they only perform inference and lack the ability to adapt to changing scenarios. It is clear how devices, set up in this way, are vulnerable to the context drift aforementioned.
By training ML models for a specific context and later deploying them in the real world, it is expected a drop in accuracy which can make the application itself not reliable. It is then obvious how an application of simple ML inference on such environments is not the best solution. To contrast this issue, it is necessary to implement the so called Continual Learning (CL) algorithms. CL is a machine learning approach that allows ML models to perform training in real time and continually keep up-to-date the model weights. The implementation of this method comes with new challenges and limitations which are mainly related to memory management and strategies for the implementation of real time training which also keep in consideration the optimization of resources.
\bigskip

Continual learning methods lead to a real time training based on the data incoming. This allows the model to change and fine tune its weights and structure to better contrast the context drift.
An additional feature that can be easily added to CL is the ability to recognize never seen classes. This, if paired with the model's ability to extend its structure, allows to create a flexible model that is able to allocate new weights and biases for better predictions.
An important problem that tackles basic applications of CL is catastrophic forgetting. Catastrophic forgetting is a phenomenon that occurs when model trained in real time overfits new data. This makes the knowledge related to past tasks be replaced by new knowledge, thus forgetting the initial scenario which leads to a reduction of the model performances over time. This aspect can be reduced by applying preventive mechanism inside the back propagation that control the parameters update. \\
The implementation of CL in industrial applications is not a new topic in the research world, but its implementation on tiny devices is just started to become more and more popular. One common application is CL in industrial scenarios, mainly for monitoring purposes on heavy machines.
The main contributions of this study concern the application of CL in two different applications. The objective is to understand if CL is a feasible solution for TinyML and if its use is actually effective for the generation of autonomous and self adapting models. In this study, a light framework that is easy to connect to a pre trained classification model was developed. The system substitutes the last layer and continually performs updates on weights and biases, and also extends its shape for flexible adaptation to new classes. The system is able to use different state-of-the-art strategies that are tested and compared in two experiments with the aim of understanding if it is possible to: i) maintain or improve the accuracy of the model; ii)contrast catastrophic forgetting; iii) digest and learn classes of never seen data. Both experiments concern the application of ML for the classification of data coming from different sensors. \\
The first application regards the analysis of accelerometer data. In this experiment the user holds the accelerometer sensor in its hand and records a time series of accelerations while drawing letters in the air. The idea is to apply ML to classify the data and recognize the letters written. The model created is initially trained for the recognition of the pattern that characterize the five vowels. Later CL is applied to the experiment and the model is exposed to new data representing three new consonants. The aim of the experiment is to let the ML model learn new patterns by performing a real time training.
The experiment can be considered a simplification of a real world applications, but it is a clear example of how a CL model can behave in these scenarios. This application can be extended in a real-life scenario such as the monitoring of vibration patterns of heavy industrial machinery. \\
The second application concerns the experimentation of CL on a CNN model applied on an OpenMV camera for the visual recognition of digits from the MNIST dataset. The idea consists of initially train the model to recognize only the digits from 0 to 5 and later use the CL framework developed for applying a real time training on the remaining digits. This second experiment can be extended to applications where a camera is used for a visual control of defects on products in a production pipeline.  \\
The work carried out in this study shows that the application of CL on tiny devices is possible. Even though the CL strategies are applied only on the last layer the results are satisfying and in both examples all the classes were correctly digested by the model. These tests show that a model equipped with a CL system is able to expand its knowledge and learn more classes, specifically 3 for the letters example and 4 for the digits example. The devices are able to maintain a reasonable accuracy at the end of the trainings that drop from the original frozen model accuracy by only 10.7\%.
The study performed is a good example that shows the capabilities of these tiny devices. It proves that machine learning applied on MCUs is a technology that has a huge potential and deserves more attention. CL can lead to smarter, more efficient, better performing systems in the IoT field and in industrial applications.
\bigskip

The Thesis is organized as follows. The first chapter contains an introduction to the theoretical aspects about Machine Learning (ML) and Continual Learning (CL). At first the basic concept of Machine Learning are described, then the focus moves towards Continual Learning (CL) where also some state of the art papers are discussed. The chapter then describes some applications of ML on microcontrollers (MCUs) with also a brief explanation of advantages and disadvantages of cloud computing and edge computing. The second chapter briefly explains the hardware used in this study. The study uses two different hardware for two different applications, So the chapter initially describes the STM32 Nucleo MCU, and later it focuses on the OpenMV camera. The chapter concludes with a short explanation of how ML can be applied on tiny devices. In chapter three the system implementation is described. Here initially the steps performed for implementing CL training on MCUs is described, followed by an explanation of the basic structure of the TinyOL system. Then all the algorithms implemented are illustrated in detail with also some considerations about memory and computational power. At last the general idea of the two applications is demonstrated. The fourth chapter shows in detail the experimental setup. Here can be found all the information needed for replicating in detail the tests. The chapter in order describes: i) the collection of the dataset; ii) the training and evaluation of the frozen models; iii) the a detailed execution of the tests using the MCU and a laptop. Chapter five contains the results obtained from the training. At first the detail about the comparison between training performed on a laptop and training performed on an MCU can be found. The the results from all algorithms applied in the experiments of gesture recognition and image classification are explained. The results contain information about accuracy, precision, F1 score, memory used and time of inference. In the last chapter the conclusions about the work done and possible future implementations can be found.


\chapter{Related Works}
\label{relworks}

In this chapter some information about the application of machine learning on tiny devices are given. Then a brief introduction to continual learnin is done with a following explanation of the most relevant state-of-the-art studies regarding continual learning in TinyML world. 

\section{Machine learning in general}

\section{Cloud vs edge inference}


\section{Machine Learning on MCU}
% applicazione ml su mcu, risorse limitate, molto importante power comsumption, design del mcu, paralre di possibili applicazioni, paraldre del fatto che si fa solo inference
% parlare del rpuning, binarization, compression del meodello, state of teh art compression MCUNET


TinyML is a fast growing research area that aims at applying ML on limited devices like micro controllers. This technology has found a rapid grow in the last years especially thanks to the potential demonstrated by its application in several fields like industrial application, agricultural automation, human-computer interactions, autonomous driving. The use of TinyML in all these fields allows to introduce the concept of edge computing, where computations are brought closer to the origin of the data, to devices that up until now have been used only as collector and transmitters of data. Edge computing bring to lots of advantages like low-latency , better privacy, security and reliability to the network end-user. It allows to move the computations from the cloud to the device itself, which brings to higher throughput and improved responsiveness in applications. Speaking about responsiveness, which is a huge deal for real time applications, edge computing permits to decrease the network traffic. This feature comes from the fact that the use of machine learning directly on the device allows to lift a big portion of computation weight given to the central server, thus reducing the amount of data that has to be exchanged on the network, which is also compressed in size and dense in information. \\
One of the main fields in which TinyML is particularly fit and can be exploited with all its potential is for sure the world of IoT. Here the application of edge computing brings to lots of advantages already described before. Of course the implementation of such systems comes with a cost, which in this case is related to the increased complexity on which the MCU works and its limited resources. Machine learning is a field of computer science that is known to be energy and resource demanding. Using such a technology on these tiny devices is a real challenge which already has seen interesting applications and improvements in the research world. The main characteristic of MCU is for sure their limited dimensions and power consumption. This is usually a nice feature that allows to develop small systems that can live for very long period of times in harsh environments without the need of maintenance. 
Lot of focus has also given to the implementation of energy harvesting systems that, not only use very low quantities of energy but also are able to extract energy from the environment in which they live. Limited dimensions has also drawbacks, which are limited memories and limited computational power, all features that do not really match with the application of machine learning. 
In the last decade (??) the research around TinyML revolved around the implementation of efficient framework from both point of view of memory use and power consumption. Some well known systems for deployment of ML models on tiny devices developed by big companies are Tensorflow Lite \ref{}, STM32 CUBE AI \ref{}, PyTorch mobile \ref{}. All these frameworks are used for training a model on a powerful system and later load a compressed version of the model on the MCU. The main concerns is the compression of the model in such a way that it doesn't drop in accuracy even with reduced weights or quatnized values. The second aspect that concerns the use of these tools is the inferenfce performed on the device. Being that procedure computationally heavy it's necessary to be able to optimize the computations. Some research studies focused soecifically on this.\\






The main challenge of TinyML is for sure the successful application of ML on such resource constrained systems. These are in fact designed to be deployed in difficult to reach places and for running for very long times. This implies that the devices should be battery power or equipped with energy harvesting hardware and their power consumption should be limited and optimized. Other limitations concern the limited computational power, which is directly connected to the CPU frequency and the battery management and the available memory. The latter is a very important topic for TinyML. It's in fact known that the application of ML on any type of device requires the usage of great amounts of memory, it's then a big challenge to be able to deploy these systems with very limited memories.\\
The application of ML on MCUs, mobile devices or in general on the edge of IoT systems it's a great advantage that can bring to some improvmenets. The key advantages are:
\begin{itemize}
\item privacy: by having the data directly processed on the node there is no change of violating the privacy policies since the possibility of interception is totally nulled
\item latency: by elaborating data directly on the edge the work load of processing that should be performed by the cloud is limited and so is the transmission of the data itself. This brings to limited time delays and allows the device to perform decisions in real time, improving the performances of real time applications.
\item energy efficiency: the transmission of huge quantities of data from the edge to the cloud takes a big portion of the energy consumption of an IoT system. Even if the application of NN is energy intensive it is an order of magnitude less, thus an improvement.
\end{itemize}


\section{Continual-on line learning}
% spiegare cosa è ctastrophic forgetting, come mai serve, cosa va a risolvere, quali sono i problemi che iontroduce, 
% parlare di cinme CL è trattato nella research, paralkre dlle tipologie di algoritmi, supervised training, frameworks gia sviluppatim metodi di valutazione?

Until recent times the application of ML on MCUs has always been focused on the creation of intelligent small system that maintain good performance with reasonable consumption, limited time of inference and long lifetimes. A major negative aspect of the TinyML solutions is their focus on the inference of streams of data. Which almost always requires the usage of powerful machines for the training of NN models that are later deployed on the MCU. This results in the creation of a static network which is not able to adapt to the data and adjust to different scenarios. The solution to this problem is the creation of a Continual Learning system. \\
CL systems are a variation of the tipical pipeline of ML. The main focus of CL systems is to be able to continuously update the model in order to adapt its structure and parameters to overcome context drift, be able to recognize appearance of new patterns and to avoid catastrophic forgetting. The latter is a problem that is directly introduced by the nature of the paradigm itself. By having a model that is continuously updated with a feedback loop that is directly dependent on the current erross i'ts clear how it's immediate to update the model in such a way that the old tasks are forgetten for the sake of learning the new ones. This could be seen also as a over fitting of the model on the new tasks and if oc course to be avoided. Different algorithms have different ways for contrasting this phenomenon. \\
In todays literature several CL algoprithms and strategies have been already proposed. A well organized summary is proposed in \cite{lesort2020continual}, where the most relevant methods are briefly classified in 4 categories, originally proposed by \autocite{maltoni2019continuous}.
\begin{itemize}
\item Architectural: these algorithms are based on the usage of particular types of structures and architectures. Some common methods are weight-freezing, layer activation or dual-memories-models that try to imitate logn term memory and short term memory.
\item Regularization: this group contains all those approaches that base their ability to retain past memories on the application of particular loss functions. In these loss functions usually a term is added with the aim of performing a feedback that considers both the old knowledge and tries to learn the new data.
\item Rehearsal strategies: in these strategies past informations are periodically revisited by the model. This is done for strengthening the old knowledge and connections. Notice that this methods is not well suited for application on MCUs mainly because of the restricted memories. 
\item Generative Replay: this methods implement similar strategies of the rehearsal. This time the data that is repeated in the models is not actually old data saved in the memory but it's actually data artificially generate by the model itself. 
\end{itemize} 

The type of strategies that better suits an application on MCU are for sure the regularization methods and the architectural methods. Both these groups require little to no extra computation with respect to a simple ML application, thus their strength is intrinsic in the update rules adopted. Some of the most important methods from the state of the art are, LWF, PNN, CWR, EWC, SI. 



\section{Pruning and quantization}





\chapter{Hardware} 
\label{chap:hardware}
In this chapter the hardware used to carry out the experiments is described. The application of ML on MCU soes not require specific types of hardware, but it requires devices that are capable of sustaining those kind of computations. In today's market, lots of off-the-shelf MCUs are already equipped with hardware components that make the device suitable for the job. These microcontroller only need to be quipped correctly with framework and tools that make ML computations more efficient and optimized. Most big companies that propose their product have different families of devices that are specifically built for different purposes, so it is just a matter of choosing the most suitable one. In this study, for both applications, devices based on STM microcontrollers have been used. The gesture recognition application uses an STM32 Nucleo F401-RE, a well performing and easy to use development board. The image classification application uses an OpenMV camera, a device equipped with a camera sensor that uses an STM32 H7 MCU and is programmable in MicroPython. Figure \ref{fig:hardware_all} shows both devices: on the left the Nucleo F401-RE; on the right the OpenMV camera. The reasons that brought to the selection of these two MCUs depends mainly on the on the quick availability for the Nucleo development board and the uniqueness of characteristics for the OpenMV camera.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter2/hardware.jpg} 
    \caption{PLACEHOLDER - da cambiare}
    \label{fig:hardware_all}    
\end{figure}


\section{Gesture recognition hardware}
The gesture recognition experiment is carried out with a Nucleo STM32 F401-RE. This device is a 64 bit microcontroller produced by STMicroelectronics that belongs to the $high-performance$ product line. This product is thought to be a flexible and easy to use development board for fast prototyping, which is also compatible with Arduino Uno shields (the internal GPIOs pin scheme is the same as Arduino Uno). Differently from other devices produced by STMicroelectronics, this MCU does not require the additional debugger/programmer ST-LINK. This component is in fact included in the first part of the device, separated from the rest with visible gaps in the board. The Nucleo can be easily programmed in C or C++ with the all the libraries and tools available in the STM32Cube package. This is a powerful software that can be exploited to perform an initial setup of any STM device, like the definition of all the peripherals and basic microcontroller parameters. The STM32Cube package also includes, for free, libraries for any device produced by STM. Moreover, thanks to STM32Cube, the Nucleo board STM32 F401-RE fully supports the machine learning extension, STM-CUBE-AI \autocite{stm_cube_ai}. This toolkit can be used for compressing and loading ML models on the device and later perform optimized and efficient inference in real time. \\
The main features of the Nucleo development board are summarized in Table \ref{table:specifications_stm}.\\

\begin{table}[]
\centering
\begin{tabular}{|l|c|}
\hline
Processor           & \begin{tabular}[c]{@{}c@{}}ARM 32-bit Cortex-M4 CPU\\ 84 MHz\end{tabular}                 \\ \hline
Memory              & \begin{tabular}[c]{@{}c@{}}SRAM: 96 kB\\ \\ Flash: 512 kB\end{tabular}                    \\ \hline
Phisical attributes & \begin{tabular}[c]{@{}c@{}}Weight: ??g\\ Length, Width, Height: ??x??x3??mm\end{tabular}  \\ \hline
Peripherals         & \begin{tabular}[c]{@{}c@{}}50 GPIOs, SPI, UART, I2C, DAC/ADC,\\ PWM, Timers,\end{tabular} \\ \hline
\end{tabular}
\label{table:specifications_stm}
\caption{PLACEHOLDER - da cambiare}
\end{table}

The gesture recognition application is based on the use of ML for the analysis of time series data. The time series array contains the values recorded from an accelerometer sensor while an user is moving said sensor to write letters in the air. To acquire that kind of data, the Nucleo needs to be equipped with an accelerometer sensor. For doing that, it has been decided to use the Nucleo shield IKS01A2 \autocite{shield_web_page}, which is a device that can be mounted easily on the board simply by aligning the GPIO pins. The shield is equipped with a 3D accelerometer, a pressure sensor and a capacitive digital humidity and temperature sensor. The device communicates with the Nucleo through I2C protocol and it is fully supported by the libraries available on STMCube, that make its use quick and easy. Figure \ref{fig:hardware_stm} shows the two devices separately on the left and mounted on the right.\\

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter2/hardware_stm.jpg} 
    \caption{PLACEHOLDER - da cambiare}
    \label{fig:hardware_stm}    
\end{figure}



\section{Image classification hardware}
For the image classification experiment, an OpenMV cam H7 plus \autocite{abdelkader2017openmv} \autocite{openmv_web_page} is used. This device is an affordable and expandable small board equipped with an STM MCU and a camera sensor with interchangeable lens. The OpenMV camera started as a project \autocite{openmv_project} back in 2013 due to a lack of affordable, small, powerful and easy to use breakout board with cameras. The idea then developed into a kickstarter that gained popularity until reaching what is today, an established product that aims at becoming the standard device for machine vision applications. \\
The device mounts the camera sensor OV5640 and an STM32 H7 MCU and it can be programmed easily with MicroPython through the dedicated OpenMV IDE. The board is equipped with 16 GPIOs thanks to which it can interact with external devices like servo motors, sensors or other MCUs. The GPIOs can be exploited for controlling robots, drones or for machine learning purposes. Some examples of successful applications of OpenMV cameras in robotic systems are: the design of a control system for rolling a ball \autocite{zhou2019design} and the design of a tracking system \autocite{wei2020design}.\\
The camera comes also with the possibility to mount different lenses, sensors and additional modular components. From the official website a complete list is available, some examples are: IR lenses and sensors, telephoto lens, super telephoto lens, ultra wide lens, WI-fi shield, LCD shield, motor shield, ecc.. .
The possibility to add all these different components or modification make the device a very well suited solution for many problems, application and especially for prototipation.\\
Table \ref{table:specifications_openmv} summarizes the specifications of the device used in this study.\\

\begin{table}[]
\centering	
\begin{tabular}{|l|c|}
\hline
Processor           & \begin{tabular}[c]{@{}c@{}}ARM 32-bit Cortex-M7 CPU\\ w/ Double Precision FPU 480 MHz (1027 DMIPS)\end{tabular}                               \\ \hline
Memory              & \begin{tabular}[c]{@{}c@{}}SDRAM: 32 MBs\\ SRAM: 1 MB\\ Flash: ext 32 MB, int 2 MB\\ Expandable with SD cart\end{tabular}                     \\ \hline
Resolution          & \begin{tabular}[c]{@{}c@{}}Grayscale: 640x480 max\\ RGB565: 320x240 max\\ Grayscale JPEG: 640x640 max\\ RGB565 JPEG: 640x480 max\end{tabular} \\ \hline
Phisical attributes & \begin{tabular}[c]{@{}c@{}}Weight: 19g\\ Length, Width, Height: 45x36x30 mm\end{tabular}                                                      \\ \hline
Lens                & \begin{tabular}[c]{@{}c@{}}Focal length: 2.8mm\\ Aperture: F2.0\\ Format: 1/3"\\ HFOV: 70.8°, VFOV:55.6°\end{tabular}                         \\ \hline
Peripherals         & \begin{tabular}[c]{@{}c@{}}GPIOs, interrupts, SPI, UART, I2C, DAC/ADC,\\ PWM, LEDs, removable camera module\end{tabular}                      \\ \hline
\end{tabular}
\label{table:specifications_openmv}
\caption{OpenMV H7+ specifications}
\end{table}

Since the application of this study requires the use of a camera pointing to a screen, a small 3D printed support was created. An already available project from Tingiverse \autocite{tripod_link} was modified to make the mounting of the OpenMV camera possible. The $stl$ files can be found in the GitHub repository of this project \autocite{github_repo}. Figure \ref{fig:hardware_openmv} shows the OpenMV mounted on the 3D printed tripod while pointing to the computer screen during a CL session.\\

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter2/hardware_openmv.jpg} 
    \caption{PLACEHOLDER - da cambiare}
    \label{fig:hardware_openmv}    
\end{figure}

\section{Machine learning support}
In this study applications it is requires to apply machine learning on the MCUs. Even if both examples use microcontrollers developed by ST, the procedure for implementing ML capabilities is not the same. \\
For the Nucleo application it is required to use ST-CUBE-AI \autocite{stm_cube_ai}, an extension pack developed and supported by ST itself. This toolkit can be added quite easily from the CubeMX, a software developed by ST that helps with automatic generation of chunks of code. In this case the addition of the AI pack allows the user to not be bothered by the generation of routines and other tools required for optimized machine learning inference. By using this pack it is possible to compress, load and run ML models in an easy and immediate way with just some function calls.





\chapter{System implementation}
% introduzione la machine learning
% spiegare il funzionamento base, nodi, activation function, layers, 
% CNN, NN fully connected, a cosa servono, quali sono gli obbiettivi specifici
% back propagation, sgd, l rate, batch size
% pruning, quantization
Continual learning is the application of real time training on a model with the aim of generating self adjusting systems that are able to learn from incoming data streams. As already mentioned in Chapter \ref{Intro}, continual learning can lead to many improvements if exploited in IoT applications. The main feature of Continual Learning (CL) is the ability of ML models to adapt weights and biases to learn the environment and continue to be relevant in the application. \\

In this chapter the idea behind the system implemented is described. Then the basic structure of the system developed is explained by following the steps performed for reaching the final implementation. After that all the algorithms ideas are explained together with the modification that have been added. Also short block diagrams have been created for better showing the basic workflow of each algorithm.\\
The application of CL on MCU is still a quite new idea that gained some popularity in recent years. Applications regarding CL have already been explored before, but their applications on tiny devices is still pretty new. Some relevant studies that applied CL on MCU are \autocite{} and \autocite{}. In my study, the idea started from the basics developed and explained in the TinyOL application. TinyOL is a simple implementation of a ML training but applied on a device in real time. Their goal is to be able to train in real time a ML model on samples of data containing information about the vibrational pattern of a PC fan. The model should be able to classify the vibration pattern and understand if the fan is in normal, tilted or stuck mode.\\
The TinyOL system is quite easy but it showed good results in real applications. The basic system structure in this case is based on the addition of a layer at the end of the ML model, this should behave as the classification layer. The layer takes the output elaborated from an autoencoder model and elaborates the output to obtain a classification of the input sample. The idea is that every time a sample is received, the sample is elaborated by the autoencoder model (which is frozen and does not change) and then is passed throught the TinyOL layer. This layer is trained in real time in order to be able to correctly fine tune to new pattersn and learn from scratch new classes.\\
The system implemented in this thesis started from the same idea but applies it on different problems, classification applied on NN and CNN model. In this study the TinyOL layer substitutes the classification layer and at each input samples it performs a supervised learning which allows to perform fine tuning and expansion of known classes. In the following section the creation, application and basic pipeline of my version of TinyOL is explained. 

\section{Basic pipeline of the CL system developed}
\label{basic_system}
% spiegare idea di base del CL in questo caso, training fatto in real time
% spiegare come si ottiene la back propagation, far vedere formule, applicate a softmax
% spiegare il sistema fatto a frozen model + OL layer
% spiegare nel dettaglio i metodi implementati + grafici a blocchi che mostrano come va
% frase di introduzione alle basi del sistema
Machine learning applied on MCUs always featured only application of inference on data inputs. So, to apply CL on embedded devices, it is necessary to develop from scratch a framework that permits the implementation CL strategies. For this study, two off-the-shelf hardware are used and one framework is initially developed on a powerful device and later adapted to the two MCUs. The framework requires to be attached as the last layer of a pre trained model, so it should always be paired with hardware that support toolkits for ML inference in real time.\\
% cosa introduce CL al modello che fino ad ora faceva solo inference
As mentioned before, the main idea was to develop a system starting from the the system developed in the TinyOL study \ref{}. The idea, applied for this study experiments, was too attach an OL system as the last layer of the pre trained model. This aims at enhancing the classification abilities of the model, allowing to: i) fine tune and update the classification weights and biases, which, over time, lead to better performing models that better follow the context drift; ii) enlarge the size of the classification layer, adding the ability of the model to recognize new classes, thus improving the flexibility of the model and reducing the necessity to perform maintenance. The applications seen here are always in supervised settings. This means that at every training step the ground truth label is known and is provided to the algorithm, which then uses it to compute the error and perform back propagation for the parameters update.\\
% iniziare a spiegare come funziona un training step in real time
So the basic idea of the TinyOL system developed is to refresh and update the weights every time a new samples is received. This can be performed simply by implementing the standard ML training strategy. This consists in computing the error committed by the prediction and then propagate the same error back to the weights. The correction on the weights and biases depends directly on how the importance that the parameter has on the prediction and thus the error. To apply a real time training step on a model it is then necessary to know the structure of the model, which allows to understand how the prediction depends on every single weight, allowing then to compute a general formula for the back propagation. In the applications seen in this study, the idea was always to apply CL on classification problems, where the classification is performed in the last layer of a NN model. By having such specific applications, it is possible to compute the rules required for the application of CL on such models. In fact, by having classification on NN models, it is then immediate to compute the update rules for classification layer based on the use of Softmax activation functions.\\
Now consider a simple and short model composed of few layers, where all are fully connected nodes. Consider that the classification layer (the last one) is composed of 5 nodes and the activation function is Softmax and the previous layer has 10 nodes. Since the application of CL in this study wants to train only the last layer, the entire model before the classification layer can be considered as a grey box (or frozen model) that outputs an array of 10 values. It can be said at this point that the update rule required for the real time training depends on: the Softmax function, the output of the grey box, the loss function, and the error between prediction and label. Figure \ref{} contains a block diagram showing the model situation just described. \\

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter3/PLACEHOLDER.jpg} 
    \caption{Place holder}
    \label{fig:placeholder}    
\end{figure}

The basic formulas used in the propagation of a classification layer are the following:

\begin{align} 
	z_i &= \sum_i w_{ij} \: x_j + b_i \label{zi} \\ 
	y_i &= Softmax(z_i) = \frac{e^{z_j}}{\sum_j=1^k e^{z_j}} 	\label{yi} \\
	L^{cross}_i &= - \frac{1}{n} \sum_k [\ y \: ln(y_i) \: +\: (1-t_i) \: ln(1-y_i) ]\ \label{cost_cross} \\
	i   &= 1, 2, ...,k \nonumber \\
	j   &= 1, 2, ...,n \nonumber
\end{align}

Where equation \ref{zi} is the formula that describes the propagation of inputs though a ML node, equation \ref{yi} is the definition of the Softmax function, equation \ref{cost_cross} is the definition of the categorical cross entropy loss function, $k$ is the number of classes known by the mode and $n$ is the number of outputs of the grey box (or number of nodes of the frozen model's last layer).\\
The application of these function to obtain a single formula for the output then becomes:

\begin{align}
	y_i         &=  Softmax(z_i) = \frac{e^{\sum_j=1^n w_{ij} x_j+b_i}}{\sum_j=1^k  e^{\sum_j=1^n w_{ij} x_j+b_i}} \\
	L^{cross}_i &= - \frac{1}{n} \sum_k [\ y \: ln(y_i) \: +\: (1-t_i) \: ln(1-y_i) ]\ 
\end{align}

At this point it is possible to compute the relation between every weight and the error committed by the model. To do so it is necessary to just compute the derivative of the error with respect to the weight of interest. 


\begin{equation}
	E = L^{cross}(y_i) = L^{cross}(Softmax(z_i)) = L^{cross}(Softmax(\sum_i w_{ij} \: x_j + b_i))  
\end{equation}

Where it is immediate to compute the derivative of the error with respect to the weights as a product of derivatives.

\begin{equation}
	\frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial S_i} \cdot \frac{\partial S_i}{\partial z_i} \cdot \frac{\partial z_i}{\partial w_{ij}}  
\end{equation}

Each block is then computes as:

\begin{align}
	\frac{\partial z_i}{\partial w_{ij}} &= \frac{ \partial (\ \sum_i w_{ij} \: x_j + b_i )\ }{ \partial w_{ij}} = x_i \\[10pt]
	%
	\frac{\partial S_i}{\partial z_i} &= \frac{\partial INSERIRE}{\partial z_i} \nonumber \\
	&= \frac{e^{z_i}}{(\sum e^{z_i})^2}- \frac{(e^{z_i})^2}{(\sum e^{z_i})^2} \nonumber \\
	&= Softmax(z_i) - (Softmax(z_i))^2 \nonumber \\
	&= Softmax(z_i) - (1-Softmax(z_i)) \\[10pt]
	%
	ES &= \frac{1}{1} 
\end{align}

Then it is immediate to apply the derivative to a simple feedback loop. The final formula that defines the back propagation on weights and biases of a classification layer that uses Softmax as activation function and categorical cross entropy as loss function is:

\begin{align}
w_{ij} &= w_{ij} - l_{rate} \cdot (\ y_i-t_i )\ \cdot x_j \label{w_update} \\
b_i    &= b_i    - l_{rate} \cdot (\ y_i-t_i )\ \label{b_update} 
\end{align}

Equation \ref{w_update} and \ref{b_update} represent the most simple way for updating a weight and a bias based on the prediction performed by the model, the true label and the output of the frozen model. This formula is the simplest way for training weights, which updates parameters at every sample received.

\section{Algorithms implemented}
This study started from the implementation of this strategy for performing CL learning and later expanded by applying other methods already proposed in the research. As explained in Chapter \ref{relworks}, regularization approaches are strategies that exploit the addition of loss term to the update rule. Thanks to this, it is possible to have some control over the weight update. In today's research some strategies for CL training have been proposed mainly with the aim of contrasting catastrophic forgetting, but these were never applied to small devices. The best performing state-of-the-art regularization strategies are Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI) and Learn Without Forgetting (LWF) \autocite{li2017learning}. In another paper the authors propose another method called Copy Weight with Reinit (CWR) \autocite{lomonaco2017core50}, which has been later improved with the CWR+ version in paper \autocite{maltoni2019continuous}, where also the new method AR1 have been presented. In this study only some of the aforementioned strategies were implemented, namely LWF, CWR and TinyOL, together with small variations used for the implementation of CL trainings that use batches of data and not single samples. \\
In these applications, the algorithms are all applied in the same OL system. The system substitutes the classification layer of a pre trained model, called frozen model. The frozen model is used only as a feature extraction block which provides an array as output which already contains the elaborated data which is ready to be fed through the classification layer. Of course the entire strategy adopted is inside the last layer, which, depending on the strategy adopted, manipulated the weight and biases with different rules.

\subsubsection{TinyOL}
The TinyOL method applied on MCU, as already mentioned, has been initially implemented in paper \autocite{ren2021tinyol}. The strategy is straight forward and it follows the basic ML training step, which consists in computing the error from the prediction and propagating it on the weights and biases through stochastic gradients descend (SGD). Its implementation consists in a couple of $for$ loops for updating the biases and the weights. This said, the weights update rule for this algorithm are:

\begin{align}
    w_{i,j} &= w_{i,j} - \alpha (y_i - t_i) \cdot x_i \\
    b_i     &= b_i - \alpha (y_i - t_i) \\
    where & \: \: i= 0,1..,n  \: \: and \: \:  j=0,1,..,m \nonumber 
\end{align}

Where $y_i$ is the prediction obtained from the OL layer, $t_i$ is the true label, $\alpha$ is the learning rate (tuned by the user), $w_{i,j}$ are the weights of the OL layer, $b_i$ are the biases of the OL layer, $n$ is the max amount of classes known by the OL system and $m$ is the height of the last layer of the frozen model. Note that in all strategies implemented, the value $n$ can change dynamically since the maximum amount of possible classes is not known a priori, or at least it is not known in a real life application. \\
Also a variation of this method has been implemented. The variation takes into consideration the possibility to apply a back propagation that depends on a group of samples (a batch) and not just from the last sample received. The idea for implementing such a variation came thanks to the article \autocite{batch_size_medium}, where the author explores the impact that batch size has on ML training dynamics. The base strategy of the variation is to compute a back propagation that depends on a batch of inputs and not just from the last sample recorded. This helps the model to be less vulnerable to noisy data and outliers. To implement an algorithm with this variation, it is required to maintain memory of all the samples from the batch. This requires the allocation of double the amount of memory necessary for the standard version. This is done by allocating two additional matrices, one called W, which contains the data of old samples related to the weights, the other is called B, which contains the data of old samples related to the biases. These matrices are used as a cumulative memory of the back propagation applied by each training step. Every time a new sample is received and elaborated, the upgrade for each weight and bias is computed and added in the correct spot inside matrices B and W. When a batch is finished, the average back propagation update is computed and applied on the actual matrices of weights and biases. Note that during a batch the OL system performs an inference with the frozen model's output by using matrices w and b, computes the error, computes the back propagations to be applied on W and B, adds the updates in matrices W and B. During an entire batch the weights used for inference are kept constant. When a batch is finished the average is computed and the update rule is applied. Then contents of W and B are deleted and restored to 0. \\
So the update rule at each inference step becomes:

\begin{align}
	W_{i,j} &= W_{i,j} + \alpha (y_i - t_i) \cdot x_i \\
    B_i     &= B_i + \alpha  (y_i - t_i) 
\end{align}

And at the end of every batch the update applied on the real weights is:

\begin{align}
	w_{i,j} &= w_{i,j} - \frac{1}{batch\_size} \cdot W_{i,j} \\
	b_i     &= b_i - \frac{1}{batch\_size} \cdot B_i
\end{align}

Both methods are based on the same basic principle which is quite simple. The method TinyOL requires the use of only one weight matrix and one bias array and their dimension depend on two parameters: the number of classes known by the OL system represented by the value $n$ and the size of the frozen model's last layer, represented by the value $m$. This makes the memory allocated from the method be equal to a total of $(n \times m+n \times 1)\times 4 \: bytes$. The method changes the layer parameters each time a new sample is received, with no constraints. This aspect is the main problem that concerns the TinyOL strategy and makes it vulnerable to catastrophic forgetting. By allowing such a high flexibility to adapt to any kind of data, the model is not protected at all from catastrophic forgetting. The model learns from every single sample that it receives, no matter if the sample is noisy, an outlier or wrongly labelled.\\
On the other hand the method TinyOL with mini batches exploits the same approach but applies a back propagation that is dictated by the average update computed from a group of $k$ samples. Depending on the value of $k$ the group can be considered to be a more or less good representation of the data received. In any case this method should be able to better contrast catastrophic forgetting, noisy data and outliers. \\
Figures \ref{fig:block_diag_OL} and \ref{fig:block_diag_OLwb} contain block diagram showing how the two methods behave.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter3/PLACEHOLDER.jpg} 
    \caption{Place holder - OL}
    \label{fig:block_diag_OL}    
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter3/PLACEHOLDER.jpg} 
    \caption{Place holder - OL with batches}
    \label{fig:block_diag_OLwb}    
\end{figure}

\subsubsection{TinyOL V2}
The TinyOL V2 algorithms is based on the same idea of the original TinyOL. A little intuitable modification was applied in the method with the aim of contrasting catastrophic forgetting. The idea is to contrast the drift that affects the original weights by completely removing the possibility to update those weights. The algorithm applies the same update rule seen before but only on the parameters that represent new classes. The update rules become:

\begin{align}
	w_{i,j} &= w_{i,j} - \alpha (y_i - t_i) \cdot x_i \\
    b_i     &= b_i - \alpha (y_i - t_i) \\
    where   & \: \: i= p,p+1..,n  \: \: and \: \:  j=0,1,..,m \nonumber  
\end{align}

The only difference with respect to the TinyOL method is the iterator $i$, which goes from p to n, where $p$ represents the position of the first unknown class. \\
Also in this case the strategy variation that uses batches of samples was implemented. Again the method requires the use of two additional matrices called W and B, which contain the cumulative back propagation computed at each step. As before, the algorithm follows the same rules as TinyOL with batches, but with the iterator $i$ going from $p$ to $n$. The rule that defines the standard behaviour during training is:

\begin{align}
    W_{i,j} &= W_{i,j} + \alpha (y_i - t_i) \cdot x_i \\
    B_i     &= B_i + \alpha  (y_i - t_i) 
\end{align}

And at the end of every batch the usual update rule is applied on the matrices of weights and biases used for inference.

\begin{align}
    w_{i,j} &= w_{i,j} - \frac{1}{batch\_size} \cdot W_{i,j} \\
    b_i 	    &= b_i - \frac{1}{batch\_size} \cdot B_i \\
    where   & \: \: i= p,p+1..,n  \: \: and \: \:  j=0,1,..,m \nonumber  
\end{align}
As seen for the TinyOL woth batches, the TinyOL V2 with batches allows the model to learn from a bigger group of samples. This ability should help the model to avoid over fitting, outliers and noisy data. \\
In conclusion TinyOL V2 is a simple method that differs from the original strategy only because the update is restricted to the new parameters. By forcing the update on just a portion of the weight and biases, the context drift that would irreversibly modify the original weights, thus forgetting the original knowledge, is completely removed. This helps the algorithm in contrasting catastrophic forgetting but also reduces the ability of the model to perform fine tuning on those classes. Another negative aspect regards the general behaviour of the model. By having a training strategy that updates only a portion of weights, it is not possible to create a model that behaves as optimizer of the loss function. This means that at the end of the training the model is composed of two parts that behave differently at every iteration. One portion of the weights behaves as the original model, while another part of the weights behaves as the most recent version of the model. These two part, when computing a prediction, cannot make the model converge towards an optimized prediction. \\
The method TinyOL V2 requires the same amount of memory used by TinyOL, which means a matrix of size $n \times m$ and an array of size $n \times 1$. On the other hand the method TinyOL V2 with batches requires an additional matrix and array but this time with a reduced size of $(n-p) \times m$ and $(n-p) \times m$. Figures \ref{fig:block_diag_OLV2} and \ref{fig:block_diag_OLV2wb} contain block diagram showing the pipeline of the two methods.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter3/OLV2.png} 
    \caption{Place holder - OLV2}
    \label{fig:block_diag_OLV2}    
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter3/OLV2.png} 
    \caption{Place holder - OLV2}
    \label{fig:block_diag_OLV2wb}    
\end{figure}

\subsubsection{LWF}
The LWF strategy is a regularization approach introduced in \autocite{li2017learning} and later applied with small variation in \autocite{maltoni2019continuous}. The main idea of the method is to contrast catastrophic forgetting by applying a smart loss function and double architecture that is able to combine old and new knowledge in the back propagation of the parameters. The double architecture models refers to the fact that two models are required in order to use this method. In this case the entire OL system has been applied only on the last layer of the model so the double architecture in this case is composed only of a double classification layer. The first is called $tl$, training layer while the second is called $cl$, copy layer. The role of $tl$ is to be continuously updated at each training step with the LWF back propagation rule, while $cl$ is a layer that contains a copy of all the original weights computed in the Tensorflow training, thus it represents the original knowledge of the model. The back propagation rule is based on the idea of fusing the weight updates that the two layer would apply. The fusion of these two updates is done with a weighted average that changes dynamically as the training continues. This of course implies that both layers produce a prediction, which means double computation for the OL system.\\
At this point the only major difference with respect to the TinyOL method is the double inference and the computation of the weighted back propagation. The update to be applied can be computed quite easily again by using SGD which turn out to be a simple weighted sum of two back propagation:
%
\[    \mathcal{L}_{LWF} ( y_i, z_i, t_i) =  (1-\lambda) \cdot{L}_{cross}(y_i, t_i) + \lambda \cdot{L}_{cross}(y_i, z_i) \]
\[ w_{i,j} = w_{i,j} - \alpha \cdot x_i \cdot [ (y_i - t_i)(1-\lambda) + (y_i - z_i)\lambda]  \]
\[ b_i = b_i - \alpha \cdot [ (y_i - t_i)(1-\lambda) + (y_i - z_i)\lambda] \]
\[ \text{where i=0,1..,n  and  j=0,1,..,m } \]
%
Where $y_i$ is the prediction array obtained from the layer $tl$, $z_i$is the prediction array obtained from the layer $cl$, $t_i$ is the ground truth label, $\lambda$ is the variable weight that defines which prediction has more decisional power. \\
The back propagation is composed of two parts, the first defined by $tl$ and the second defined by $cl$. The value $\lambda$ plays a very important role in this update. As explained in \autocite{maltoni2019continuous} its value cannot stay constant because it would be suboptimal. In their application the value follows a discrete function linear with the number of batches encountered, while in oiur case its value needed to be dependant on the only value known in a OL application, the amount of samples elaborated. The update of the loss function weight is the following:
\[ \lambda = \frac{100}{100+ \text{prediction$\_$counter}} \]
Another important note to be said is that in the update rule the implementation follows the variation proposed in \autocite{maltoni2019continuous}, where the loss functions ${L}_{LWF}$ used in the weighted average are not a balance between categorical cross entropy and knowledge distillation but a balance between two categorical cross entropy. This is a little modification that allows for an easier implementation without ruining the performance. \\
Also in this case a version that integrates batches is proposed. This time the method simply updates the values of $cl$ every time a batch is finished. The  algorithm this time simply becomes a fusion between old and knew knowledge where the old knowledge is refresh once in a while. In this way the model can be seen as a model that performs a weighted average in between a fast learning memory and a memory that stops in time. The size of a batch is defined by the value $k$. 
%
\[    \mathcal{L}_{LWF} ( y_i, z_i, t_i) =  (1-\lambda) \cdot{L}_{cross}(y_i, t_i) + \lambda \cdot{L}_{cross}(y_i, z_i) \]
\[ w^{TL}_{i,j} = w^{TL}_{i,j} - \alpha \cdot x_i \cdot [ (y_i - t_i)(1-\lambda) + (y_i - z_i)\lambda]  \]
\[ b^{TL}_i = b^{TL}_i - \alpha \cdot [ (y_i - t_i)(1-\lambda) + (y_i - z_i)\lambda] \]
\[ \text{where i=0,1..,n  and  j=0,1,..,m } \]
%
And at the end of a batch (once every $k$ values are elaborated):
\[ w^{CL}_{i,j} = w^{TL}_{i,j}  \]
\[ b^{CL}_i = b^{TL}_i  \]
This method, being different from the previous, requires also a different $\lambda$ rule. Experimentally it has been found to be well working an update rule defines as follows:
\begin{equation}
\lambda = \left\{
        		\begin{array}{ll}
            		1                                                         & prediction \_ counter \leq batch \_ size \\
            		\frac{\text{batch$\_$size}}{\text{prediction$\_$counter}} & prediction \_ counter >    batch \_ size
        		\end{array}
    		  \right.
\end{equation}
Both the LWF methods require the same amount of memory, which is a double prediction layer of size  $n \times m$ and $n \times 1$. Both methods are quite easy to implement and their strength is defined in the value $\lambda$ and in their update rule. They differ only from the weight updated applied on the matrix $cl$. A negative aspect that characterizes these two methods is the amount of computation required, which can be a problem for tiny devices. By having two layers and the need of two prediction is of course needed the double of computation. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter3/LWF.png} 
    \caption{Place holder - LWF}
    \label{fig:block_diag_LWF}    
\end{figure}

\subsubsection{CWR}
CWR is an architectural approach that exploits the usage of two classification layers and a weighted back propagation rule for performing OL. Again the two classification layers are called $tl$, training layer and $cl$, consolidated layer. The idea is to perform training at each step on $tl$ with the same method as TonyOL and at the end of each batch update $cl$ with a particular rule. The back propagations for the $cl$ at the end of a batch are tge same for biases and weights and are the following:
    \[     cw_{i,j} =  \frac{cw_{i,j} \cdot updates_{i} + tl_{i,j}}{updates_{i} + 1} \] 
    \[     tw_{i,j} =  cw_{i,j}\] 
Where $tw_{i,j}$ are the weights and biases of the training layer, $cw_{i,j}$ are the weights and biases of the consolidated layer, and $updates_{i}$ is an array that behaves as a counter of labels encountered.\\
By using two classification layer that update differently the method tries to replicate the short term memory and long term memory architecture that characterizes biological brains ??? ESSERE SICURO. The layer $tl$ behaves as the short term memory since it gets updated at every single training step, and at each batch it gets reset to the correct values. The layer $cl$ behaves as a long term memory since it never gets reset or cleaned and it gets updated only once every batch with a weighted average. This weighting method depends on the number of times that a specific label appeared in the training batch.\\
Another important aspect of this method is how the prediction is computed and used. While performing only training the method requires only a prediction performed by $tl$ since this needs to get updated by its error. However if an actual prediction is requested to the model also the $cl$ layer should perform the computation and provide an inference. In fact the inference obtained by the consolidated layer is to be considered more relevant and reliable since it is produced by the long term memory. In the case a prediction is required the method needs a double prediction, one from $tl$ and the other from $cl$. Again as said for LWF this is not optimal because of the limits of tiny devices. \\
CWR is a method easy to implement. Its strength are hidden in the double architecture and the update rule that make it possible to merge short term memories and long term memories and also contrast catastrophic forgetting. The memory required for this algorithm is: to weight matrices of size $n \times m$, 2 bias arrays of size $n \times 1$, one array that keeps track of the labels encountered of size $n \times 1$. The amount of computations can change depending if a simple training step is performed or if an inference is required.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter3/CWR.png} 
    \caption{Place holder - CWR}
    \label{fig:block_diag_CWR}    
\end{figure}

\subsubsection{My algorithm}




% INSERIRE MODELLO ESEMPIO

And


\section{Gesture recognition application}

\section{Image classification application}





\chapter{Experimental setup}

In this chapter, the practical aspects of the experiments are described. Initially, the study was developed entirely on the laptop using Python code with the aim of understanding the theoretical behaviour of the methods and the capabilities of CL. Later, the same principle and basic pipeline were ported to the STM Nucleo and the OpenMV camera, respectively. All applications use the same general workflow: i) train the base model on a powerful device for the classification of the basic classes using Tensorflow; ii) manipulate and compress the model if necessary, then load it on the MCU of interest; iii) attach the OL system to the frozen model, then define the basic training parameters and the desired CL strategy. \\
In this chapter, all the most important steps for a good setup of the experiments are described. Specifically, these steps are: the dataset collection, the training of the frozen models, the implementation of the OL system on the MCUs, and the application of the entire system on the device with a short explanation of how the CL trainings are performed.

\section{Dataset collection}
To create and train ML models, it is necessary to have big datasets. In this study, the two applications explored elaborate two types of data. The Nucleo F410-RE application uses time series of data recorded from an accelerometer, while the OpenMV application uses images containing digits from the MNIST dataset \ref{deng2012mnist}. \\

\subsection{Accelerometer dataset}
For the gesture recognition application the dataset was created from scratch. Datasets of this type, containing accelerometer data representing gestures are not very common, so it was necessary to collect it. It was decided to collect time series of 3D accelerometer data of letters written in the air from the user as a proof of concept of a gesture recognition system.
The dataset collection was carried out with the hardware described in Chapter \ref{chap:hardware}. \\
The dataset is composed of 8 different letters, which are A,E,I,O,U and B,R,M. The vowels compose the original classes that are first learned by the frozen model, while the three consonants are the additional classes that are learned later by the CL system. The collection of the dataset is performed by connecting the MCU to a laptop via UART protocol (USB cable). The laptop behaves as a power provider and real-time storage for the data stream. 
To collect the sensor data, it is used a small script that controls UART and I2C communication with some timers and GPIOs. When the Nucleo detects a GPIO interrupt, the user specifies the label. Then the code records data from the sensors with a frequency of 100 Hz for 2 seconds. Meanwhile, the values are also streamed via USB to the laptop which stores data using a serial communication software (MobaXTerm). \\
To make the dataset be composed of samples that better resemble real-life applications, a NIC scenario was artificially imposed. This means that the recorded samples contain both new classes (the consonants) and new pattern of known classes. The latter was introduced by performing motion paths with accentuated characteristics. Some examples are: the accentuated oval shape of the letter O, the speed at which the sensor moves for the letter I, the general size/width/height of all letters, and the radius of the curves for letter R and B.
Figure \ref{fig:letters_motion} shows the general path that was followed while drawing the letters in the air.\\

\begin{figure}[h!]
    \centering
    \includegraphics[width=80mm]{Figures/Chapter4/letters_motion.jpg} 
    \caption{Motion of the accleerometer (NON SI VEDONO LE FRECCE ARANCIO)}
    \label{fig:letters_motion}    
\end{figure}

All the samples received by the MCU are saved in a table format in a text file. The columns of the table contain: the number of samples recorded, the label of the sample and three columns for the accelerations recorded from X, Y, Z axis. Considering that the MCU was set to work at a sampling frequency of 100 Hz for 2 seconds a single sample is composed of 600 values (200 for each axis). 
The final shape of the dataset is 5130 samples, where the vowels have on average 560 samples each, while the consonants have around 760 samples each. \\
Once the dataset is collected, post-processing is performed. This consists of a simple reshape, shuffle and subdivision. To perform the training on the ML architecture, all samples are reshaped by stacking all the rows into a single array from a matrix $3 \times 300$ to an array $1 \times 600$. Then, a subdivision of the dataset is performed. Given that the dataset is needed for two trainings (frozen model training and CL training), it is required to separate it correctly. The frozen model can recognize only vowels, thus its dataset is composed only of vowels. This dataset counts a total of 881 samples with 176 samples from each vowel. The OL model, on the other hand, is trained on all letters, so its dataset contains the remaining vowels and all the consonants. \\
After this, both datasets are divided in training and testing portions, which is done with the usual 80-20\% rule. Figure \ref{fig:flow_dataset_letters} shows how the dataset are divided and balanced for the two trainings regarding the gesture recognition application.

\begin{figure}[h!]
    \centering
    \includegraphics[width=140mm]{Figures/Chapter4/flow_dataset_letters.png} 
    \caption{Stankey diagram showing how the letter dataset is divided}
    \label{fig:flow_dataset_letters}    
\end{figure}

\subsection{Digits recognition dataset}
For the image classification application the well-known MNIST dataset was used. The MNIST dataset is a publicly available large collection of images of handwritten digits. The dataset is well known in academic and research for its small size of images and large quantity of samples. It is composed of 60000 images for training and 10000 images for testing. The images are gray scaled and have a size of $28 \times 28$ pixels. In today's research, the dataset is commonly used for benchmarking ML training, while in academic it is used for basic training of classification and generative models. Figure \ref{fig:mnist_dataset} contains some sample images that of the MNIST dataset.\\

\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter4/mnist_dataset.png} 
    \caption{Example of images of digits in the MNIST dataset}
    \label{fig:mnist_dataset}    
\end{figure}

For this application purpose, the dataset requires pre-processing. Considering that the goal of the frozen model is to correctly recognize the digits from 0 to 5 it is necessary to separate the dataset into two groups, $low\_digits$ and $high\_digits$. By doing this, the $low\_digits$ group is composed of 36017 samples, while the $high\_digits$ group is composed of 23989 samples. For the training of the frozen model, the entire $low\_digits$ group is used for a Tensorflow training, which is further also separated in train, test, and validation. For this reason, the common 70-20-10 rule was used. On the other hand, for the training of the CL model only 5000 samples from the $high\_digits$ group were used. This because from the experience previously obtained from the latter application, it was demonstrated that 500 samples for each class are more than enough for a correct CL session. The CL dataset is then separated in training and testing with the rule 80-20\% for the CL application. \\
Figure \ref{fig:flow_dataset_openmv} shows how the dataset is divided for the training of the frozen model and CL model.

\begin{figure}[h!]
    \centering
    \includegraphics[width=140mm]{Figures/Chapter4/flow_dataset_openmv.png} 
    \caption{Stankey diagram showing how the MNIST dataset is divided}
    \label{fig:flow_dataset_openmv}    
\end{figure}

\section{Frozen model training and evaluation}
Once the datasets are created and post-processed, the training of the frozen models can be performed. Both models require small structures in order to be loaded on small MCUs. In both applications the models perform a classification, which can be achieved simply by imposing the last layer's activation function as a Softmax. All the other layers are used only for feature extraction and their structure and characteristics depend on the type of data to elaborate.
The trainings of the frozen models and their manipulation was carried out with Tensorflow library and Python.\\

\subsection{Gesture recognition model}
In the gesture recognition application, the model elaborates a time series of accelerometer data. The model's structure is composed of only fully connected layers, which makes the structure very simple. Typical applications of ML on time series use LSTM types of models, which are structures well suited for the elaboration of time dependent signals. In this case, the results obtained from a structure composed of only fully connected layers brought to satisfying results, so the model's structure was kept as is. The layer sizes are 600 for the input, 128 for the hidden layers, and 5 for the classification layers. The activations functions are, Softmax for the classification layer, and ReLu for all the other layers. Figure \ref{fig:letter_structure} shows a plot that contains the basic structure of the model used.\\

\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter4/letter_structure.png} 
    \caption{Letter recognition model: basic structure and its separation in frozen model and OL classification layer}
    \label{fig:letter_structure}    
\end{figure}

The output layer's shape consists of 5 nodes because the model predicts 5 classes (the vowels). The number of layers and total parameters is quite low (94,085) respect to typical machine learning models; this makes it very suited for applications on MCUs. \\
The main training parameters are: $Adam$ optimizer, $categorical$ $cross$ $entropy$ as loss function, 20 epochs, and a batch size of 16. 
The accuracy obtained from the testing is 96.83\%. Figure \ref{fig:training_letters} contains two plots. On the left is shown the variation of the accuracy and loss during training, while on the right is shown the accuracy of each class at the end of the Tensorflow training.\\

\begin{figure}[h!]
    \centering
    \includegraphics[width=130mm]{Figures/Chapter4/training_letters.jpg} 
    \caption{Letter recognition training: on the left the variation of loss and validation; on the right the accuracy for each class}
    % Qui il grafico di destra va bene, magari puoi mettere dentro anche i plot di accuratezza e validation accuracy durante il trainin. Mentre il grafico di sinistra potrebbe essere sostituito con il risultato del test, quindi utilizzando il dataset specifico di test e non il risultato alla fine del training. In questo caso si potrebbe mostrare anche la confusion matrix e calcolare le metriche come precision, recall e fscore.
    \label{fig:training_letters}    
\end{figure}

The last step consists of the preparation of the frozen models and the exportation of the model itself. This is necessary because of the particular actions that are performed on the last layer by the OL system. Because it is required to have total control over the weights and biases of the classification layer, it is necessary to have the model separated into two parts. The first one is called the frozen model and is just a $model.h5$ file which contains the truncated version of the trained model (classification model is removed). This portion of the model, once loaded on the MCU, is manipulated by the inference tools and can be used as a grey box. The output of this grey box is just the results from the feature extraction and it can be forwarded in the classification layer for the prediction.
The second part of the model exportation is the classification layer. The weights and biases of this layer are not exported as before, but are saved in a text file in matrix form. By doing this, it is possible to later load them in the MCU's RAM, which allows the OL system to edit and manipulate parameters and the layer shape. 
Figure \ref{fig:letter_structure} shows how the base model was divided into two main parts.

\subsection{Image classification model}
In the image classification application, a Convolutional Neural Network (CNN) architecture was used. These model types are specifically created for the elaboration of images and their main feature is the presence of convolutional layers for the feature extraction followed by NN layers for the classification. Figure \ref{fig:openmv_structure} contains a plot that shows the structure of the model.\\

\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter4/openmv_structure.png} 
    \caption{Image classification model: basic structure and its separation in frozen model and OL classification layer}
    \label{fig:openmv_structure}    
\end{figure}

The model contains two sequential blocks of two convolutional layers followed by Max Pooling. This type of structure allows the ML model to perform initially a feature extraction over the image and to flatten the output matrix to an array. The fully connected layer is used to elaborate the array and later feed the data to the classification layer, where the Softmax activation function is used for providing the probability of each class.\\
The output consists of 6 classes because the frozen model is trained for the recognition of the $low\_ digits$ group of images (i.e. digits from 0 to 5). Note also how despite having a more complex structure the number of parameters in the model is not much higher when compared to the previous application. This allows to have a small model that can be easily deployed on constrained MCUs for enabling fast inference. \\
The relevant training characteristics are: $Adam$ as optimizer, $categorical$ $cross$ $entropy$ as loss function, 30 epochs, and batch size of 64. The final accuracy obtained from the testing of the model is of $99.35\%$. Figure \ref{fig:training_mnist} shows on the right the behaviour of the accuracy and loss during training, and on the left the accuracy of the model for each class of the starting dataset.

\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter4/training_letters.jpg} 
    %Anche qui come prima sarebbe meglio mettere l'accuratezza calcolata con il dataset per il test
    \caption{PLACEHOLDER}
    \label{fig:training_mnist}    
\end{figure}

Another important step that was performed for the OpenMV application is the pruning and quantization of the model. Usually, to deploy models with a high memory footprint it is necessary to apply compression techniques such ass combination of pruning and quantization.
Pruning is a method that reduces the number of connections in a NN model by setting to 0 redundant or non-relevant weights. This helps to reduce the memory occupied by the model, and if done strategically, can reduce the number of computations required for inference. By injecting forced sparsity inside the weight matrix, it is possible to reduce the computations needed, thus improving the inference efficiency. \\
Note that the Flash memory of the OpenMV is capable of storing models with bigger sizes than the one created, so pruning and quantization are not actually required. However, to demonstrate these model capabilities, it was decided to apply compression techniques anyway. The pruning and quantization procedure was carried out with Tensorflow, which makes the process as easy as a training setup. The main characteristics of the pruning are: $Adam$ as optimizer, $categorical$ $cross$ $entropy$ as loss function, 5 epochs, batch size of 32, initial sparsity of 0.5 and final sparsity of 0.8. After pruning the model shows an accuracy of $99.6\%$.\\
After this step, it is also possible to introduce the quantization operation. This is carried out almost automatically by Tensorflow simply by calling the correct functions. The model size along the different steps of the compression are: 230 kB after the first Tensorflow training, 86 kB after pruning and 64 kB after quantization. \\
As in the previous application, the last step consists of the exportation of the model. The frozen model is exported as $model.h5$, while the classification layer parameters are exported in text file and later loaded in the MCU. 

\section{STM F401-RE setup for gesture recognition}
In this section the setup needed for a correct gesture recognition application is described. Here is described how the CL system was implemented on the Nucleo board, hoe the ML model was deployed on the MCU and how the communication laptop-MCU works.\\
In the gesture recognition application, to add ML capabilities to the microcontroller, two software are exploited. The first, STM-CUBE-MX, is a piece of software used for automatically generating chunks of code. This helps the user by lifting a lot of work required for the initial setup of the device. Thanks to this software it is possible to use the UI and easily define the main characteristics of the MCU that is being programmed. Starting frokm the definition of the main clock and the essential characteristics of all the peripherals enabled to the parameters of the communication protocols. The second piece of software is just an extension pack of STM-CUBE-MX, and is called STM-CUBE-AI. This toolkit enhances the capabilities of the MCU by adding the possibility to use machine learning models on the device. The main advantages brought by the extension pack are the possibility to automatically load and process ML models on the flash memory and run optimized routines for inference on the same model. This toolkit also allows the user to compress the model at deploy time. In this case no compression is applied and the model is loaded as a $model.h5$ file.\\
Once the ML abilities have been added to the MCU it is possible to incorporate the CL system inside the code. The system, as already explained in Chapter \ref{basic_system}, is attached at the end of the frozen model, and it is composed mainly of functions for the elaboration of the frozen model's output, management of memory and management of the data stream. The entire system is written in C code and it is almost entirely contained in one single library. The entire project is available on GitHub \autocite{github_repo}.\\
The basic structure of the code applied on the MCU is the following. At first all the most important parameters and variables are created. Depending on the algorithm defined, the right amount of memory is allocated. The weight and bias matrices are then filled with the parameters that have been previously generated by the Tensorflow training. Note that for the application on the Nucleo development board, the file is actually a C library, which contains all the weights and biases written inside a matrix. After all have been set up, the infinite while loop begins. In here, if the "received sample flag" was triggered, the frozen model inference ism performed. Once the feature extraction is done, the output is fed to the OL layer, which will: propagate it through the classification layer, compute the prediction, compare the prediction with the label, compute the error and back propagate the error on the weights using the strategy adopted. After this, a small message (32 bytes) containing the most relevant informations about the training step is generated and sent back through UART.\\
Once the CL system and the ML model are loaded correctly on the MCU, the experiment can begin. To perform a fast, reliable and repeatable experimentation it was decided to develop a small app that controls autonomously the data stream towards the MCU. The app was developed in Python and is executed from the laptop, which stays in sync with the MCU and sends through UART (USB cable) one sample at a time. The script is quite simple and it follows the logic line: load the dataset of accelerometer array data, open the serial port with the specified properties, initialize containers for storing information, start an infinite while loop where the communication laptop-MCU is continuously repeated. \\
When the app is launched, the MCU should already be connected to the laptop, otherwise the serial port cannot be opened. Once the script is launched and the initial setup is done, the app waits for an acknowledgement from the MCU (a message of 2 chars), which signals the laptop that the device is ready to receive data and start the training. Once the ACK (acknowledgement) signal is received, the app sends a sample from the dataset composed of an array of 600 values and the label. The MCU then performs the routine aforementioned. Once the message is received by the laptop, a new sample is sent and the communication starts again. The procedure continues in this way until the training portion of the dataset is completed. After this, the pseudo testing begins. The only difference here, is that the message received by the laptop is stored and used later for the generation of plots and tables regarding the testing of the CL model.\\
A complete training procedure lasts for about 10 minutes. At the end of the communication of all samples the python scripts automatically stops the transmission and generates some plots. Figure \ref{fig:python_stm_diagram} contains a block diagram that summarizes the steps of the app and the communication.

\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter4/PLACEHOLDER.jpg} 
    \caption{PLACEHOLDER}
    \label{fig:python_stm_diagram}    
\end{figure}



\section{OpenMV setup for image classification}
The application on the OpenMV camera is quite similar. Once the dataset, the model, the last layer and the system are prepared, everything can be deployed on the MCU. This time, because of the custom board and custom firmware, the toolchain for the deployment of the model is different. Thanks to it, it is possible to include the ML model inside the MCU files and generate from scratch a new firmware that contains the model parameters and structure. This allows to later use built-in-tools for efficient, optimized and fast inference directly from the MicroPython code. Once the firmware is generated, it can be loaded on the MCU and is immediate to apply machine learning.\\
The system developed for the application of the CL strategies is basically the same as the one explained in the previous example. Initially the code allocates the necessary parameters and memory, and later it enters in an infinite while loop. In here the code continuously waits for a new sample which is then fed to the built-in-tools that perform the frozen model inference. Then, the output is propagated through the CL system. The only major difference in this application is the management of the memory for the matrices. Some problems regarding the allocation of big matrices often blocked the code. It was decided to not have the entire classification layer inside one single matrix, but rather separate it in smaller sections. This can lead to small increases of time for the inference, but it is actually required in order to have resulsts at all. \\
Once the code is loaded in the $main.py$ file and the library $TinyOL.py$ is loaded on the MCU, the experiment can be carried out. Once again a Python app was developed for the creation of fast, reliable and repeatable training sessions. The idea here is not to send data via USB connection, but rather display on a screen the images from the MNIST dataset while the camera is pointing at it. The USB connection is required in any case because the app needs to maintain sync with the camera and also send labels representing the image displayed. This time the app requires the use of Tensorflow (for loading the dataset), OpenCV (for displaying the images on screen) and PySerial (for opening a serial port with the MCU). \\
The app once again is divided in elemental blocks. At first the app loads the dataset and extract from it only the required amount of samples. Then the serial port is opened with the correct characteristics. After that, the app opens two windows, where the first is used for displaying the MNIST digits where the camera should be pointed at, while the second is used for displaying in real time the point of view of the camera. At last, a while loop starts and here the communication laptop-MCU is done. At every single step the laptop sends two messages to the camera. The first contains a small 4 char command which defines the state of the OpenMV camera, the second message is just the label of the image displayed on the screen. Every time these two messages are received, the camera takes a picture and performs actions depending on its state. The possible states are defined by the user (through the python app) and are: 

\begin{itemize}
	\item $snap$ mode: the camera takes a photo, compresses it and sends it via UART to the laptop. This state is used for understanding where the camera is point and if the digit is inside the point of view of the OpenMV. The label received is not used and should be a char containing an $X$.
	\item $elab$ mode: the camera takes a photo, applies a gray scale filter, applies a binarization, compressed the image and sends it via UART to the laptop. This mode is used for understanding what the camera will see  when also basic image manipulation is applied. These manipulations are used in the training for transforming the coloured image into a black and white image. Also in this case the label is discarded and it should contain just an $X$ char. During this state, the app on the laptop slowly shows all the digits from 0 to 9. This permits the user to better point the camera towards the screen.
	\item $trai$ mode: the camera takes a photo, applies inference on the image and later feeds the output to the CL system. No transmission of image is performed towards the laptop because it is easy to de synchronize the devices. The label this time is received an transformed into a hot one encoded array for the computation of the back propagation.
\end{itemize}

Once the training and pseudo testing are performed, the app stops showing digits images and the OpenMV camera stores the results inside its SD. The results can then be amanipulated by another script that will transformt he data in table, plots and confusion matrices. Figure \ref{fig:python_openmv_diagram} contains a block diagram summarizing the flow of the experiment. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=50mm]{Figures/Chapter4/PLACEHOLDER.jpg} 
    \caption{PLACEHOLDER}
    \label{fig:python_openmv_diagram}    
\end{figure}

Figure \ref{fig:openmv_training} shows the OpenMV camera pointed to a computer screen in all the different states. Top right is the camera in $idle$ mode, no stream is received by the laptop. Top left is the camera in $snap$ mode, compressed images are streamed to the laptop. Bottom left is the camera in $elab$ mode, compressed and processed images are streamed to the laptop. Bottom right is the camera in $train$ mode, no stream is available and the camera is performing inference and CL training.

\begin{figure}[h!]
    \centering
    \includegraphics[width=130mm]{Figures/Chapter4/openmv_training.jpg} 
    \caption{OpenMV camera pointing to a screen while in different states of the training. Top left is $idle$ mode, top right is $snap$ mode, bottom left is $elab$ mode, bottom right is $trai$ mode}
    \label{fig:openmv_training}    
\end{figure}

Figures \ref{fig:openmv_pov} shows a better comparison between images taken in the $snap$ and $elab$ mode. As mentioned before in the $snap$ state the photo is captured, compressed and sent, while in $elab$ mode the image is captured, gray scaled, a threshold is applied and later compressed and sent.

\begin{figure}[h!]
    \centering
    \includegraphics[width=140mm]{Figures/Chapter4/openmv_pov.jpg} 
    \caption{Point of view of the OpenMV camera. On the left the original image taken with no compression and no elaboration, in the center an image taken in $snap$ mode, on the right an image taken in $elab$ mode.}
    \label{fig:openmv_pov}    
\end{figure}


\chapter{Experimental results} 

\section{Experiment A: Gesture recognition}

\section{Experiment B: Image classification}

This section contains and explains the results obtained from the test performed. Initially a description about the comparison between simulation and real application is performed with the aim of understanding if the training on the nucleo evolves as the simulation on the laptop. Then the results obtained from the application from the Nucleo are discussed and finally the results from the OpenMV application are described.\\
To understand if the Nucleo STM32 F401-RE is able to perform a real time ML training a study concerning the history of its parameters is done. The idea is to record the variation of the most important parameters from the OL layer at every training step and then compare its evolution to the same parameter evolutioin but recorded from the simulation carried out on the laptop. The parameters of interest are the biases of the OL layer, the predictions obtained from Softmax, the output of the frozen model, 10 weights picked randomically from the weight matrix. The evolution of the parameters is then displayed in a plot with the aim of observing how and if the history recorded from the laptop differs from the history recorded from the MCU. This is done qualitatively simply by looking at superimposition of the two lines. Figure \ref{fig:comparison_frozen} shows one example of comparison for the frozen model outpus, figure \ref{fig:comparison_bias} shows the comparison of the bias evolution, figure \ref{fig:comparison_weights} shows the comparison of the weights evolution, figure \ref{fig:comparison_softmax} shows the difference of the predictions obtained from Softmax.
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/bias_example.png} 
    \caption{PLACEHOLDER}
    \label{fig:comparison_bias}    
\end{figure}
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/weight_example.png} 
    \caption{PLACEHOLDER}
    \label{fig:comparison_weights}    
\end{figure}
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/frozen_example.png} 
    \caption{PLACEHOLDER}
    \label{fig:comparison_frozen}    
\end{figure}
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/softmax_example.png} 
    \caption{PLACEHOLDER}
    \label{fig:comparison_softmax}    
\end{figure}
%
The plots displayed above are examples of the evolution performed, but they are rappresentative of the behaviour of all parameters. It's clear from the results how the two applications are very close, differencing from each other by just a small magnitude and for very few training step. Only one difference can be noted in figure x, where the dofference of the softmax prediction is dofferent from zero but still very little. This error in fact doesn't introduce any problem in the evolution since in the following steps the error goes back to 0. \\
One of the main concerns was about the feature extraction performed by the frozen model. Because of the limited resources of the MCU usually models are compressed to be later loaded on the device. In this case no pruning or quantization have been applied, so the frozen model loaded on the MCU and laptop are exactly the same. The concern regards how the prediction is carried out by the X-CUBE-AI tool on the MCU compared with Tensorflow on the laptop. Figure \ref{fig:comparison_max} contains two examples of comparison of frozen model outputs. The x axis contains the iterator representing the i-th difference computed between the i-th value from the Tensorflow and STM output from the frozen model. On the left the difference that contains the biggest error is displayed, while on the right is displayed the sample that contains the second biggest error. It's clear how the plot on the left is not a correct representation of the MCU behaviour since it has a magnitude far too high when compared to the second biggest error.
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/comparison_difference.png} 
    \caption{PLACEHOLDER}
    \label{fig:comparison_max}    
\end{figure}
%
Thanks to this study it is possible to conclude that a training performed on such a small device is actually possible and in terms of accuracy and precision it is as reliable as a training performed on a powerful device. Because of these plots it is then possible to conclude that the evolution of the model on the MCU is reliable and correct. A model trained on such device is subject to the same exact evolution that would affect the model in the case the training were to be carried on a laptop. From this point on all the experiment and results are obtained from MCUs.
\bigskip

Speaking about the gesture recognition experiment once the training have been carried out it's possible to display the accuracy of every single algorithm for every single class. As mentioned in section REFERENCE SECTION the testing is performed on the last 20 \% of the dataset, so on a total of XXX samples. The bar plots containing the accuracies from every strategy together with their confusion matrices are displayed in Figures \ref{fig:letter_res_OL} \ref{fig:letter_res_OL_batch} \ref{fig:letter_res_OL_v2} \ref{fig:letter_res_OL_v2_batch} \ref{fig:letter_res_LWF} \ref{fig:letter_res_LWF_batch} \ref{fig:letter_res_CWR}.  
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/STM_barPlot_OL.jpg} 
    \caption{PLACEHOLDER}
    \label{fig:letter_res_OL}    
\end{figure}
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/STM_barPlot_OL_batch.jpg} 
    \caption{PLACEHOLDER}
    \label{fig:letter_res_OL_batch}    
\end{figure}
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/STM_barPlot_OL_V2.jpg} 
    \caption{PLACEHOLDER}
    \label{fig:letter_res_OL_v2}    
\end{figure}
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/STM_barPlot_OL_V2_batch.jpg} 
    \caption{PLACEHOLDER}
    \label{fig:letter_res_OL_v2_batch}    
\end{figure}
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/STM_barPlot_LWF.jpg} 
    \caption{PLACEHOLDER}
    \label{fig:letter_res_LWF}    
\end{figure}
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/STM_barPlot_LWF_batch.jpg} 
    \caption{PLACEHOLDER}
    \label{fig:letter_res_LWF_batch}    
\end{figure}
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/STM_barPlot_CWR.jpg} 
    \caption{PLACEHOLDER}
    \label{fig:letter_res_CWR}    
\end{figure}
%
From these plots it is clear how all methods are quite good in digesting new classes and completely fuse them in the classification layer. No method in fact shows a bad learning on a specific class exept for the letter B that in all methods sees a lower accuracy when compared to others. It' easy to see from the cofusion matrix that the letter is not learned in a wrong way but rather the letter is easily confused with the letter R. Most probably this is because the path that has been followed when the dataset has beenc reated differs for just the leg of letter R. \\
Table x and table x contain other important results from the experiment. 
%
\begin{table}[]
\begin{center}
\begin{tabular}{l|cccc}
                     & \multicolumn{1}{c|}{\textbf{Accuracy \%}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Average time\\ inference frozen\\  model in ms\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Average time \\ inference OL\\ layer in ms\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Maximum allocated \\ RAM in kB\end{tabular}}} \\ \hline
\textbf{OL}          & 86.13                                     & 10.65                                                                                                                & 0.99                                                                                                             & 26.1                                                                                                 \\ \cline{1-1}
\textbf{OL batch}    & 86.26                                     & 10.65                                                                                                                & 1.54                                                                                                             & 29.8                                                                                                 \\ \cline{1-1}
\textbf{OL V2}       & 87.98                                     & 10.65                                                                                                                & 1.03                                                                                                             & 26.1                                                                                                 \\ \cline{1-1}
\textbf{OL V2 batch} & 87.98                                     & 10.65                                                                                                                & 1.11                                                                                                             & 29.8                                                                                                 \\ \cline{1-1}
\textbf{LWF}         & 87.61                                     & 10.65                                                                                                                & 3.45                                                                                                             & 29.9                                                                                                 \\ \cline{1-1}
\textbf{LWF batch}   & 86.5                                      & 10.65                                                                                                                & 3.26                                                                                                             & 29.9                                                                                                 \\ \cline{1-1}
\textbf{CWR}         & 88.47                                     & 10.65                                                                                                                & 2.11                                                                                                             & 29.9                                                                                                 \\ \cline{1-1}
\textbf{MY ALG}      & 86.87                                     & 10.65                                                                                                                & 3.54                                                                                                             & 29.9                                                                                                 \\ \cline{1-1}
\end{tabular}
\end{center}
\end{table}
%
\begin{table}[]
\begin{tabular}{cc|cccccccccc}
\multicolumn{1}{c|}{\multirow{2}{*}{Algorithm}} & \multirow{2}{*}{Parameter} & \multicolumn{8}{c|}{Class}                                                                                                                                                                            & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}batch\\ size\end{tabular}}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}learning\\ rate\end{tabular}} \\
\multicolumn{1}{c|}{}                           &                            & \multicolumn{1}{c|}{A} & \multicolumn{1}{c|}{E} & \multicolumn{1}{c|}{I} & \multicolumn{1}{c|}{O} & \multicolumn{1}{c|}{U} & \multicolumn{1}{c|}{B} & \multicolumn{1}{c|}{R} & \multicolumn{1}{c|}{M} & \multicolumn{1}{c|}{}                                                                      &                                                                          \\ \hline
\multirow{3}{*}{OL}                             & Accuracy                   & 0.92                   & 0.93                   & 0.83                   & 0.87                   & 0.86                   & 0.78                   & 0.83                   & 0.93                   & \multirow{3}{*}{16}                                                                        & \multirow{3}{*}{16}                                                      \\ \cline{2-2}
                                                & Precision                  & 0.92                   & 0.93                   & 0.85                   & 0.87                   & 0.86                   & 0.78                   & 0.83                   & 0.92                   &                                                                                            &                                                                          \\ \cline{2-2}
                                                & F1 score                   & 0.92                   & 0.93                   & 0.84                   & 0.87                   & 0.86                   & 0.78                   & 0.83                   & 0.92                   &                                                                                            &                                                                          \\ \cline{1-2}
\multirow{3}{*}{OL batch}                       & Accuracy                   & 0.89                   & 0.96                   & 0.88                   & 0.90                   & 0.89                   & 0.76                   & 0.79                   & 0.93                   & \multirow{3}{*}{16}                                                                        & \multirow{3}{*}{16}                                                      \\ \cline{2-2}
                                                & Precision                  & 0.93                   & 0.97                   & 0.89                   & 0.91                   & 0.85                   & 0.75                   & 0.78                   & 0.93                   &                                                                                            &                                                                          \\ \cline{2-2}
                                                & F1 score                   & 0.91                   & 0.96                   & 0.88                   & 0.90                   & 0.87                   & 0.75                   & 0.78                   & 0.93                   &                                                                                            &                                                                          \\ \cline{1-2}
\end{tabular}
\end{table}

In table x the overall accuracy of the model with specific strategies is displayed. From here it's clear how the algorithm CWR performs the best with an accuracy of xx \%. All methods perform quite good with the lowest accuracy being xx \& from the method OL, which is just a drop of xx \% with respect to the accuracy obtained from the training of the frozen model performed with Tensorflow. Speaking about the time required for a training step the total time can be split in two portions. The first concerns the inference obtainde by the frozen model, which is of course constant for all strategies and takes 10.65 ms. The other part of the training step time is the time taken by the OL layer which contains the time required for the inference of the OL layer and the following computation of the back propagation and update of the layer's weights. From the table is clear how the faster methods are TinyOL and TinyOL v2, which are the only methods that do require only one OL layer, thus reducing the amount of computations require. On the other hand the slowest methods are LWF, LWF batch and MY ALG, which all require a double inference from the two classification layer. The time is in fact more than double the time required by all the other strategies. The last column concerns the amount of RAM allocated by the strategies. This value shows that the TinyOL and TinyOL v2 are the lightest methods since they require only the allocation of 1 weights matrix and 1 bias array. All the other methods require a very similar amount of RAM since they all work with double memories. A little difference of just 100 bytes is due to the allocation of some additional values particular for some strategies.  \\
Another important study that has been carried out is the study of variation of the accuracy while changing the batch size. This study was of particular interest thanks to \ref{}, a blog post that studies in detail the impact that the batch size has on a ML training. The results are show in figure \ref{fig:batch_size_letter}. Here is clear how the only methods that drop their accuracy quite a bit are TinyOL and TinyOL v2, while all the other are able to mantain their accuracy quite constant.
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Figures/Chapter5/batch_size_letters.png} 
    \caption{PLACEHOLDER}
    \label{fig:batch_size_letter}    
\end{figure}
%

\chapter{Conclusion}






LA BIBLIOGRAFIA NON FUZNIONA
\printbibliography


\end{document}
