{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import myLib_barChart as myBar\n",
    "import myLib_confMatrix as myMatrix\n",
    "import myLib_parseData as myParse\n",
    "import myLib_pieChart as myPie\n",
    "import myLib_table as myTable\n",
    "import myLib_testModel as myTest\n",
    "from myLib_testModel import letterToSoftmax\n",
    "import myLib_writeFile as myWrite\n",
    "import myLib_debugFiles as myDebug\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET AND PREPARE TRAIN - TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the functions loadDataFromTxt and parseTrainTest are called. These allow to load the dataset from the txt files into matrices and then separate them in smaller matrices for testing and training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['A' 'E' 'I' 'O' 'U']\n",
      "\n",
      "Raw shape        -> (103600, 5)\n",
      "Tot samples      -> 518\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (361, 600)\n",
      "Test data shape   -> (155, 600)\n"
     ]
    }
   ],
   "source": [
    "vowels_data, vowels_label = myParse.loadDataFromTxt('vowels_OL')\n",
    "OL_data_train_vow, OL_label_train_vow, OL_data_test_vow, OL_label_test_vow = myParse.parseTrainTest(vowels_data, vowels_label, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['B']\n",
      "\n",
      "Raw shape        -> (39400, 5)\n",
      "Tot samples      -> 197\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (136, 600)\n",
      "Test data shape   -> (59, 600)\n"
     ]
    }
   ],
   "source": [
    "B_data, B_label = myParse.loadDataFromTxt('B_dataset')\n",
    "B_train_data, B_train_label, B_test_data, B_test_label = myParse.parseTrainTest(B_data, B_label, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['M']\n",
      "\n",
      "Raw shape        -> (39000, 5)\n",
      "Tot samples      -> 195\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (135, 600)\n",
      "Test data shape   -> (58, 600)\n"
     ]
    }
   ],
   "source": [
    "M_data, M_label = myParse.loadDataFromTxt('M_dataset')\n",
    "M_train_data, M_train_label, M_test_data, M_test_label = myParse.parseTrainTest(M_data, M_label, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['R']\n",
      "\n",
      "Raw shape        -> (39000, 5)\n",
      "Tot samples      -> 195\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (135, 600)\n",
      "Test data shape   -> (58, 600)\n"
     ]
    }
   ],
   "source": [
    "R_data, R_label = myParse.loadDataFromTxt('R_dataset')\n",
    "R_train_data, R_train_label, R_test_data, R_test_label = myParse.parseTrainTest(R_data, R_label, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a dataset of all letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this smaller section all the previous matrices are stacked together and then shuffled in order to create two big matrices that contain all the letters for training and testing. The training dataset is also shuffled, in order to shuffle it differently change the seed value inside the function myParse.shuffleDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_all has shape  (767, 600)\n",
      "label_all has shape (767,)\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix that contains all the train data\n",
    "data_all = OL_data_train_vow\n",
    "data_all = np.vstack(( data_all, B_train_data))\n",
    "data_all = np.vstack(( data_all, R_train_data))\n",
    "data_all = np.vstack(( data_all, M_train_data))\n",
    "# Create an array that contains all the train labels\n",
    "label_all = OL_label_train_vow\n",
    "label_all = np.hstack(( label_all, B_train_label))\n",
    "label_all = np.hstack(( label_all, R_train_label))\n",
    "label_all = np.hstack(( label_all, M_train_label))\n",
    "# Shuffle the matrix and the label\n",
    "data_all, label_all = myParse.shuffleDataset(data_all, label_all)\n",
    "\n",
    "print('data_all has shape  ' + str(data_all.shape))\n",
    "print('label_all has shape ' + str(label_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_test has shape  (330, 600)\n",
      "label_test has shape (330,)\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix that contains all the train data\n",
    "data_test = OL_data_test_vow\n",
    "data_test = np.vstack(( data_test, B_test_data))\n",
    "data_test = np.vstack(( data_test, R_test_data))\n",
    "data_test = np.vstack(( data_test, M_test_data))\n",
    "# Create an array that contains all the train labels\n",
    "label_test = OL_label_test_vow\n",
    "label_test = np.hstack(( label_test, B_test_label))\n",
    "label_test = np.hstack(( label_test, R_test_label))\n",
    "label_test = np.hstack(( label_test, M_test_label))\n",
    "\n",
    "print('data_test has shape  ' + str(data_test.shape))\n",
    "print('label_test has shape ' + str(label_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method for loading the dataset is to load it from the txt file \"training_file\". This file is an already shuffled dataset. I can use this for both feeding data in this simulation and also to the STM in order to have the closes behaviour possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['A' 'B' 'E' 'I' 'M' 'O' 'R' 'U']\n",
      "\n",
      "Raw shape        -> (220400, 5)\n",
      "Tot samples      -> 1102\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (770, 600)\n",
      "Test data shape   -> (330, 600)\n"
     ]
    }
   ],
   "source": [
    "data, label = myParse.loadDataFromTxt('training_file')\n",
    "data_train, label_train, data_test, label_test = myParse.parseTrainTest(data, label, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class Data_Container is just a container that I created in order to have all the dataset in a single object. This is useful for the plotting functions because it allows me to give as input to the function just one object and not the entire list of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Container(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.R_test_data       = R_test_data\n",
    "        self.R_test_label      = R_test_label\n",
    "        self.B_test_data       = B_test_data\n",
    "        self.B_test_label      = B_test_label\n",
    "        self.M_test_data       = M_test_data\n",
    "        self.M_test_label      = M_test_label\n",
    "        self.R_test_data       = R_test_data\n",
    "        self.OL_data_test_vow  = OL_data_test_vow\n",
    "        self.OL_label_test_vow = OL_label_test_vow\n",
    "        \n",
    "OL_testing_data = Data_Container()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the content of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block is used only to check the type of letters that are inside the datasets that I imported. It's used in order to see if the datasets are created and saved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOWELS DATASET SANITY CHECK\n",
      "    The letters found are:              ['O', 'U', 'A', 'E', 'I']\n",
      "    And for each letter the counter is: [103. 103. 103. 103. 103.   1.   0.   0.   0.]\n",
      "\n",
      "B DATASET SANITY CHECK\n",
      "    The letters found are:              ['B']\n",
      "    And for each letter the counter is: [194.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      "\n",
      "R DATASET SANITY CHECK\n",
      "    The letters found are:              ['R']\n",
      "    And for each letter the counter is: [192.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      "\n",
      "M DATASET SANITY CHECK\n",
      "    The letters found are:              ['M']\n",
      "    And for each letter the counter is: [192.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      "\n",
      "TRAIN DATASET SANITY CHECK\n",
      "    The letters found are:              ['R', 'E', 'O', 'A', 'I', 'U', 'M', 'B']\n",
      "    And for each letter the counter is: [139.  75.  72.  70.  76.  66. 135. 136.   1.]\n",
      "\n",
      "TEST DATASET SANITY CHECK\n",
      "    The letters found are:              ['R', 'U', 'I', 'M', 'B', 'O', 'A', 'E']\n",
      "    And for each letter the counter is: [52. 37. 28. 60. 61. 31. 32. 28.  1.]\n"
     ]
    }
   ],
   "source": [
    "print('VOWELS DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(vowels_label)\n",
    "print('\\nB DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(B_label)\n",
    "print('\\nR DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(R_label)\n",
    "print('\\nM DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(M_label)\n",
    "print('\\nTRAIN DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(label_train)\n",
    "print('\\nTEST DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD TF TRAINED MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I load the frozen model. The frozen model is the NN that has been trained with keras on the PC. The script that trains this model is called 'run_trainFroznModel.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.abspath('')\n",
    "MODEL_PATH = ROOT_PATH + \"/Saved_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(MODEL_PATH + 'Original_model/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TINY OL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the main part of the continual learning study is found. Here can be found the functions used for implementing the different algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is an implementation of the softmx function. I had to use this because I noticed that the sofmtax function used from keras and other methods for computing the sotmax operation gave different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc_softmax(array):\n",
    "    \"\"\" Computes softmax of an array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array_like\n",
    "        Is the array of which I want to compute the softmax operation\n",
    "    \"\"\"\n",
    "    \n",
    "    if(len(array.shape)==2):\n",
    "        array = array[0]\n",
    "        \n",
    "    size    = len(array)\n",
    "    ret_ary = np.zeros([len(array)])\n",
    "    m       = array[0]\n",
    "    sum_val = 0\n",
    "\n",
    "    for i in range(0, size):\n",
    "        if(m<array[i]):\n",
    "            m = array[i]\n",
    "\n",
    "    for i in range(0, size):\n",
    "        sum_val += np.exp(array[i] - m)\n",
    "\n",
    "    constant = m + np.log(sum_val)\n",
    "    for i in range(0, size):\n",
    "        ret_ary[i] = np.exp(array[i] - constant)\n",
    "        \n",
    "    return ret_ary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyOL class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is just a container for all the informations that are required in order to use correctly a tinyOL model. The idea is to createa  container in which everything is stored and then simply change the method for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Layer(object):\n",
    "    def __init__(self, model):\n",
    "\n",
    "        # Related to the layer\n",
    "        self.ML_frozen = keras.models.Sequential(model.layers[:-1])  # extract the last layer from the original model\n",
    "        self.ML_frozen.compile()\n",
    "        self.W = np.array(model.layers[-1].get_weights()[0])    # extract the weights from the last layer\n",
    "        self.b = np.array(model.layers[-1].get_weights()[1])    # extract the biases from the last layer\n",
    "        self.label = ['A', 'E', 'I', 'O', 'U']                  # the origina model knows only the vowels\n",
    "        self.l_rate = 0                                         # learning rate that changes depending on the algorithm\n",
    "        self.W_counter = np.zeros(self.W.shape)\n",
    "        \n",
    "        self.width = self.W.shape[0]        # shape of the weights matrix\n",
    "        \n",
    "        \n",
    "        # Related to the results fo the model\n",
    "        self.confusion_matrix = []          # container for the confusion matrix\n",
    "        self.correct_ary = []               # array that contains the number of correct prediction for each letter\n",
    "        self.mistake_ary = []               # array that contains the number of mistaken prediction for each letter\n",
    "        self.totals_ary = []                # array that contains the number of total prediction for each letter\n",
    "        \n",
    "        self.macro_avrg_precision = 0       \n",
    "        self.macro_avrg_recall = 0\n",
    "        self.macro_avrg_F1score = 0\n",
    "        \n",
    "        self.title = ''       # title that will be displayed on plots\n",
    "        self.filename = ''    # name of the files to be saved (plots, charts, conf matrix)\n",
    "        \n",
    "        \n",
    "    # Function that is used for the prediction of the model saved in this class\n",
    "    def predict(self, x):\n",
    "        mat_prod = np.array(np.matmul(x, self.W) + self.b)\n",
    "        return  myFunc_softmax(mat_prod) # othwerwise do it with keras|also remove np.array()| tf.nn.softmax(mat_prod) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyOL functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used in all methods before the feed forward of the OL layer. This function is required because it checks if the input letter is already known. If this is not true it will increse the dimension of the last layre (weight matrix and biases array) and also save the new letter in the 'known classes' array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkLabelKnown(model, current_label):\n",
    "    \n",
    "    found = 0\n",
    "    \n",
    "    for i in range(0, len(model.label)):\n",
    "        if(current_label == model.label[i]):\n",
    "            found = 1\n",
    "        \n",
    "        \n",
    "    # If the label is not known\n",
    "    if(found==0):\n",
    "        print(f'\\n\\n    New letter detected -> letter \\033[1m{current_label}\\033[0m \\n')\n",
    "\n",
    "        model.label.append(current_label)   # Add new letter to label\n",
    "        \n",
    "        # Increase weights and biases dimensions\n",
    "        model.W = np.hstack((model.W, np.zeros([model.width,1])))\n",
    "        model.W_counter = np.hstack((model.W_counter, np.zeros((model.width,1))))\n",
    "        model.b = np.hstack((model.b, np.zeros([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here the functions that implement the different methods can be found. The explanation of the code od these function is not here but it can be found in the paper \"Continuous learning in single incremental taskscenarios\" and some schemes can be found in my presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb = 771\n",
    "\n",
    "bias_pc     = np.zeros((numb,8))\n",
    "\n",
    "pred_pc     = np.zeros((numb,8))\n",
    "pre_soft_pc = np.zeros((numb,8))\n",
    "\n",
    "weight_pc   = np.zeros((numb, 80))\n",
    "\n",
    "selected_w = [46,13,107,3,57,65,127,81,89,70,\n",
    "                143,239,142,158,207,189,172,230,156,208,\n",
    "                374,359,375,371,303,298,350,257,349,333,\n",
    "                402,502,485,461,489,479,454,508,485,480,\n",
    "                527,565,614,517,528,613,625,623,587,521,\n",
    "                712,742,685,746,759,747,754,702,653,640,\n",
    "                775,809,798,853,804,840,828,788,890,819,\n",
    "                906,1019,911,1005,1016,953,1016,987,961,1023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "46%128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL(model, x, y_true):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with OL METHOD - STOCHASTICH\\n')\n",
    "   \n",
    "    cntr = 1\n",
    "    learn_rate  = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "                \n",
    "    # Cycle over all samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label)\n",
    "               \n",
    "        # PPREDICTION\n",
    "        y_ML   = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])           \n",
    "        \n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "        \n",
    "        for j in range(0,model.W.shape[0]):\n",
    "            # Update weights\n",
    "            deltaW = np.multiply(cost, y_ML[0,j])\n",
    "            dW     = np.multiply(deltaW, learn_rate)\n",
    "            model.W_counter[j,:] += dW\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "        # Update biases\n",
    "        db      = np.multiply(cost, learn_rate)\n",
    "        model.b = model.b-db\n",
    "        \n",
    "        # TO BE REMOVED LATER **********\n",
    "        # SAVE THE WEIGHTS IN A MATRIX\n",
    "        if(i<numb):\n",
    "            \n",
    "            for q in range(0, 4):\n",
    "                bias_pc[i,q] = np.copy(model.b[q])\n",
    "                pred_pc[i,q] = np.copy(y_pred[q])\n",
    "            print(model.W[46,0])\n",
    "\n",
    "            for q in range(0, 80):\n",
    "                if(int(selected_w[q]/128) < model.W.shape[1] ):\n",
    "                    weight_pc[i,q] = np.copy(model.W[selected_w[q]%128, int(selected_w[q]/128)])\n",
    "            \n",
    "            gigio = np.copy(np.array(np.matmul(y_ML, model.W) + model.b))\n",
    "            gigio = gigio[0]\n",
    "            \n",
    "            pre_soft_pc[i,0] = np.copy(gigio[0])\n",
    "            pre_soft_pc[i,1] = np.copy(gigio[1])\n",
    "            pre_soft_pc[i,2] = np.copy(gigio[2])\n",
    "            pre_soft_pc[i,3] = np.copy(gigio[3])\n",
    "            pre_soft_pc[i,4] = np.copy(gigio[4])\n",
    "            \n",
    "            if(len(cost)>5):\n",
    "                bias_pc[i,5] = np.copy(model.b[5])\n",
    "                pred_pc[i,5] = np.copy(y_pred[5])\n",
    "                pre_soft_pc[i,5] = np.copy(gigio[5])\n",
    "            if(len(cost)>6):\n",
    "                bias_pc[i,6] = np.copy(model.b[6])\n",
    "                pred_pc[i,6] = np.copy(y_pred[6])\n",
    "                pre_soft_pc[i,6] = np.copy(gigio[6])\n",
    "            if(len(cost)>7):\n",
    "                bias_pc[i,7] = np.copy(model.b[7])\n",
    "                pred_pc[i,7] = np.copy(y_pred[7])\n",
    "                pre_soft_pc[i,7] = np.copy(gigio[7])\n",
    "                \n",
    "        # *********************************\n",
    "\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL MINI BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL_miniBatch(model, x, y_true, batch_size):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with OL METHOD - MINI BATCH\\n')\n",
    "    \n",
    "    cntr=1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "    sum_gradW = np.zeros([model.W.shape[0], 8])\n",
    "    sum_gradB = np.zeros([1, 8])\n",
    "            \n",
    "    # Cycle over all samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label)        \n",
    "                \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "        \n",
    "        if(i%batch_size==0):\n",
    "                model.W = model.W - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,:w]\n",
    "                model.b = model.b - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,:w]\n",
    "\n",
    "                sum_gradW = np.zeros([h, 8])  #reset each batch  \n",
    "                sum_gradB = np.zeros([1, 8])  #reset each batch   \n",
    "        \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "\n",
    "        for j in range(0,h): \n",
    "            # Update weights\n",
    "            tmp = np.multiply(cost, y_ML[0,j]) \n",
    "            deltaW = np.zeros([1,8])\n",
    "            deltaW[0,:w] = tmp  \n",
    "            sum_gradW[j,:] += deltaW[0,:]\n",
    "\n",
    "        # Update biases\n",
    "        deltaB = np.zeros([1,8])\n",
    "        deltaB[0,:w] = cost\n",
    "        sum_gradB += deltaB\n",
    "\n",
    "        # If last iteration\n",
    "        if(i==tot_samples-1):\n",
    "            model.W = model.W - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,:w]\n",
    "            model.b = model.b - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,:w]\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def trainOneEpoch_OL_v2(model, x, y_true):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with CWR METHOD - STOCASTICH \\n')\n",
    "    \n",
    "    cntr=1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "                \n",
    "    # Cycle over every sample\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "                \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "        cost[0] = 0\n",
    "        cost[1] = 0\n",
    "        cost[2] = 0\n",
    "        cost[3] = 0\n",
    "\n",
    "        for j in range(0,model.W.shape[0]):\n",
    "            # Update weights\n",
    "            deltaW = np.multiply(cost, y_ML[0,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW[:]\n",
    "\n",
    "        # Update biases\n",
    "        db = np.multiply(cost, learn_rate)\n",
    "        model.b[5:] = model.b[5:]-db[5:]\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2 MINI BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL_v2_miniBatch(model, x, y_true, batch_size):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with CWR - MINI BATCH \\n ')  \n",
    "\n",
    "    cntr=1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "    sum_gradW = np.zeros([model.W.shape[0], 8])\n",
    "    sum_gradB = np.zeros([1, 8])\n",
    "           \n",
    "    # Cycle over all input samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "                \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "        \n",
    "        # If beginning of batch\n",
    "        if(i%batch_size==0):\n",
    "                model.W[:,5:] = model.W[:,5:] - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,5:w]\n",
    "                model.b[5:]   = model.b[5:]   - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,5:w]\n",
    "                sum_gradW = np.zeros([h, 8])  # reset\n",
    "                sum_gradB = np.zeros([1, 8])  # reset\n",
    "            \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "\n",
    "        for j in range(0,h):  \n",
    "            # Update weights\n",
    "            tmp = np.multiply(cost, y_ML[0,j]) \n",
    "            deltaW = np.zeros([1,8])\n",
    "            deltaW[0,:tmp.shape[0]] = tmp  \n",
    "            sum_gradW[j,:] += deltaW[0,:]\n",
    "\n",
    "        # Update biases\n",
    "        deltaB = np.zeros([1,8])\n",
    "        deltaB[0,:cost.shape[0]] = cost\n",
    "        sum_gradB += deltaB\n",
    "\n",
    "        # If last iteration\n",
    "        if(i==tot_samples-1):\n",
    "            model.W[:,5:] = model.W[:,5:] - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,5:w]\n",
    "            model.b[5:]   = model.b[5:]   - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,5:w]\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpochOL_LWF(model, x, y_true):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with LWF - STOCHASTIC\\n ') \n",
    "    \n",
    "    lam  = 0\n",
    "    cntr = 1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "    y_LWF = np.zeros([1, 8])    # Define container for LWF\n",
    "\n",
    "    # DEFINE ORIGINAL WEIGHTS AND BIASES\n",
    "    LWF_w = model.W\n",
    "    LWF_b = model.b\n",
    "         \n",
    "    # Cycle over every sample\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "                \n",
    "        w = model.W.shape[1]\n",
    "        h = model.W.shape[0]\n",
    "        \n",
    "        # va da 1 a 0\n",
    "        lam = 100/(100+cntr)    #1-i/493    #  1/(20+cntr)        #\n",
    "             \n",
    "        # PREDICTIONS\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "        \n",
    "        mat_prod = np.array(np.matmul(y_ML, LWF_w) + LWF_b)\n",
    "        y_LWF[0,:5] = my_Softmax(mat_prod)   \n",
    "          \n",
    "        \n",
    "        # BACKPROPAGATION        \n",
    "        cost_norm = y_pred-y_true_soft\n",
    "        cost_LWF  = y_pred-y_LWF[0,:w]\n",
    "\n",
    "        for j in range(0,h):\n",
    "            # Update weights\n",
    "            deltaW_norm = np.multiply(cost_norm,1-lam)\n",
    "            deltaW_LWF  = np.multiply(cost_LWF, lam)\n",
    "            deltaW      = np.multiply(deltaW_norm+deltaW_LWF, y_ML[0,j])\n",
    "            dW          = np.multiply(deltaW, learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "        # Update biases\n",
    "        db_norm = np.multiply(cost_norm, 1-lam)\n",
    "        db_LWF  = np.multiply(cost_LWF, lam)\n",
    "        db      = np.multiply(db_norm+db_LWF, learn_rate)\n",
    "        model.b = model.b-db\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF MINI BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpochOL_LWF_v2(model, x, y_true, batch_size):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with LWF - MINI BATCH\\n')\n",
    "    \n",
    "    lam  = 0\n",
    "    cntr = 1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "        \n",
    "    w = model.W.shape[1]\n",
    "        \n",
    "    LWF_w = np.zeros([model.W.shape[0], 8]) # alocate\n",
    "    LWF_b = np.zeros([1, 8]) \n",
    "    y_LWF = np.zeros([1, 8])\n",
    "    \n",
    "    LWF_w[:,:w] = model.W   # copy from TF\n",
    "    LWF_b[0,:w] = model.b\n",
    "    \n",
    "    # For every sample in the dataset given\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "                \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "        \n",
    "        # END OF BATCH\n",
    "        if(i%batch_size==0 and i!=0):            \n",
    "            LWF_w[:,:w] = np.copy(model.W)    # update the LWF w matrix\n",
    "            LWF_b[0,:w] = np.copy(model.b)    # update the LWF b matrix\n",
    "                \n",
    "        #lam = 1/(2+cntr)\n",
    "        if(cntr<batch_size):\n",
    "            lam = 1\n",
    "        else:\n",
    "            lam = batch_size/cntr  #(cntr/493)   va da 0 a 1\n",
    "    \n",
    "        # PREDICTION - Frozen + OL\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])  \n",
    "        \n",
    "        mat_prod = np.array(np.matmul(y_ML, LWF_w[:,:w]) + LWF_b[0,:w])\n",
    "        y_LWF[0,:w] = my_Softmax(mat_prod)        \n",
    "\n",
    "\n",
    "         \n",
    "        # ---- BACKPROPAGATION | MINI BATCH + LWF        \n",
    "        cost_norm = y_pred-y_true_soft\n",
    "        cost_LWF  = y_pred-y_LWF[0,:w]\n",
    "        \n",
    "        lam_cost_norm = np.multiply(cost_norm, 1-lam)\n",
    "        lam_cost_LWF  = np.multiply(cost_LWF,  lam)\n",
    "\n",
    "        for j in range(0,h):\n",
    "\n",
    "            # Update weights\n",
    "            deltaW = np.multiply(lam_cost_norm+lam_cost_LWF, y_ML[0,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW          \n",
    "            \n",
    "        # Update biases \n",
    "        db = np.multiply(lam_cost_norm+lam_cost_LWF, learn_rate)\n",
    "        model.b = model.b-db   \n",
    "                        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_CWR(model, x, y_true, batch_size):\n",
    "        \n",
    "    print('**********************************\\nPerforming training CWR \\n ')  \n",
    "\n",
    "    cntr=1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "    TW = np.zeros([model.W.shape[0], 8])\n",
    "    TB = np.zeros([1, 8])\n",
    "    found_lett = np.zeros([1,8])\n",
    "               \n",
    "    # Cycle over all input samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "        \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "                \n",
    "        # If beginning of batch\n",
    "        if(i%batch_size==0 and i!=0): \n",
    "            for k in range(0, w):\n",
    "                if(found_lett[0,k]!=0):\n",
    "                    tempW = np.multiply(model.W[:,k], found_lett[0,k])\n",
    "                    tempB = np.multiply(model.b[k]  , found_lett[0,k])\n",
    "                    model.W[:,k] = np.multiply(tempW+TW[:,k], 1/(found_lett[0,k]+1))\n",
    "                    model.b[k]   = np.multiply(tempB+TB[0,k], 1/(found_lett[0,k]+1))\n",
    "                    \n",
    "            TW[:h,:w]  = np.copy(model.W)\n",
    "            TB[0,:w]   = np.copy(model.b)\n",
    "            found_lett = np.zeros([1,8])  # reset\n",
    "        elif(i==0):\n",
    "            TW = np.zeros([h, 8])         # reset  \n",
    "            TB = np.zeros([1, 8])         # reset  \n",
    "            found_lett = np.zeros([1,8])  # reset\n",
    "                \n",
    "        found_lett[0,np.argmax(y_true_soft[i,:])] += 1  # update the letter counter\n",
    "            \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        mat_prod = np.array(np.matmul(y_ML, TW) + TB)\n",
    "        y_pred = my_Softmax(mat_prod)        \n",
    "\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred[:w]-y_true_soft\n",
    "\n",
    "        # Update weights\n",
    "        for j in range(0,h):\n",
    "            deltaW = np.multiply(cost, y_ML[0,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            TW[j,:w] = TW[j,:w] - dW\n",
    "\n",
    "        # Update biases\n",
    "        db = np.multiply(cost, learn_rate)\n",
    "        TB[0,:w] = TB[0,:w]-db\n",
    "\n",
    "        # If last iteration\n",
    "        if(i==tot_samples-1):\n",
    "            for k in range(5, w):\n",
    "                if(found_lett[0,k]!=0):\n",
    "                    tempW = np.multiply(model.W[:,k], found_lett[0,k])\n",
    "                    tempB = np.multiply(model.b[k]  , found_lett[0,k])\n",
    "                    model.W[:,k] = np.multiply(tempW+TW[:,k], 1/(found_lett[0,k]+1))\n",
    "                    model.b[k]   = np.multiply(tempB+TB[0,k], 1/(found_lett[0,k]+1))\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dfine some important values for the training and then actually train each single OL layer witha  different method. The 0 and 1 below are used in order to activate or deactivate the training and the following plots of a specific method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_OL = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE WHICH TRAINING AND PLOTS TO SHOW\n",
    "\n",
    "KERAS      = 1\n",
    "OL_vowels  = 0\n",
    "OL         = 1\n",
    "OL_mini    = 0\n",
    "LWF        = 0\n",
    "LWF_mini   = 0\n",
    "OL_v2      = 0\n",
    "OL_v2_mini = 0\n",
    "CWR        = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define KERAS model (just create the class, actually don't train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(KERAS==1):\n",
    "    Model_KERAS = Custom_Layer(model)\n",
    "    Model_KERAS.title = 'KERAS'\n",
    "    Model_KERAS.filename = 'KERAS'\n",
    "    Model_KERAS.label = ['A','E','I','O','U','B','R','M']\n",
    "    # DO NOT PERFORM TRAINING, KEEP IT AS IT IS, IT'S THE ORIGINAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL only on vowels (just to see if the OL model makes the keras better or worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(OL_vowels==1):\n",
    "    Model_OL_vowels = Custom_Layer(model)\n",
    "    Model_OL_vowels.title = 'VOWELS'\n",
    "    Model_OL_vowels.filename = 'OL_vowels'\n",
    "    Model_OL_vowels.l_rate = 0.000005\n",
    "    \n",
    "    trainOneEpoch_OL(Model_OL_vowels, OL_data_train_vow, OL_label_train_vow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Performing training with OL METHOD - STOCHASTICH\n",
      "\n",
      "\n",
      "\n",
      "    New letter detected -> letter \u001b[1mR\u001b[0m \n",
      "\n",
      "0.06653882530927657\n",
      "    Currently at 0.13% of dataset0.06653882530927657\n",
      "    Currently at 0.26% of dataset0.06653882530927657\n",
      "    Currently at 0.39% of dataset0.06653882530927657\n",
      "    Currently at 0.52% of dataset0.06653882530927657\n",
      "    Currently at 0.65% of dataset0.06653882530927657\n",
      "    Currently at 0.78% of dataset0.06653882530927657\n",
      "    Currently at 0.91% of dataset0.06653882530927657\n",
      "    Currently at 1.04% of dataset0.05859420937299795\n",
      "    Currently at 1.17% of dataset0.05859420937299795\n",
      "    Currently at 1.3% of dataset0.05859420937299795\n",
      "    Currently at 1.43% of dataset0.05859420937299795\n",
      "    Currently at 1.56% of dataset0.05859420937299795\n",
      "    Currently at 1.69% of dataset0.05859420937299795\n",
      "    Currently at 1.82% of dataset0.05859420937299795\n",
      "    Currently at 1.95% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mM\u001b[0m \n",
      "\n",
      "0.05859420937299795\n",
      "    Currently at 2.08% of dataset0.05859420937299795\n",
      "    Currently at 2.21% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mB\u001b[0m \n",
      "\n",
      "0.05859420937299795\n",
      "    Currently at 2.34% of dataset0.05859420937299795\n",
      "    Currently at 2.47% of dataset0.05859420937299795\n",
      "    Currently at 2.6% of dataset0.05859420937299795\n",
      "    Currently at 2.73% of dataset0.05859420937299795\n",
      "    Currently at 2.86% of dataset0.05859420937299795\n",
      "    Currently at 2.99% of dataset0.05859420937299795\n",
      "    Currently at 3.12% of dataset0.05859420937299795\n",
      "    Currently at 3.25% of dataset0.05859420937299795\n",
      "    Currently at 3.38% of dataset0.05859420937299795\n",
      "    Currently at 3.51% of dataset0.05859420937299795\n",
      "    Currently at 3.64% of dataset0.05859420937299795\n",
      "    Currently at 3.77% of dataset0.05859420937299795\n",
      "    Currently at 3.9% of dataset0.05859420937299795\n",
      "    Currently at 4.03% of dataset0.05859420937299795\n",
      "    Currently at 4.16% of dataset0.05859420937299795\n",
      "    Currently at 4.29% of dataset0.05859420937299795\n",
      "    Currently at 4.42% of dataset0.05859420937299795\n",
      "    Currently at 4.55% of dataset0.05859420937299795\n",
      "    Currently at 4.68% of dataset0.05859420937299795\n",
      "    Currently at 4.81% of dataset0.05859420937299795\n",
      "    Currently at 4.94% of dataset0.05859420937299795\n",
      "    Currently at 5.06% of dataset0.05859420937299795\n",
      "    Currently at 5.19% of dataset0.05859420937299795\n",
      "    Currently at 5.32% of dataset0.05859420937299795\n",
      "    Currently at 5.45% of dataset0.05859420937299795\n",
      "    Currently at 5.58% of dataset0.05859420937299795\n",
      "    Currently at 5.71% of dataset0.05859420937299795\n",
      "    Currently at 5.84% of dataset0.05859420937299795\n",
      "    Currently at 5.97% of dataset0.05859420937299795\n",
      "    Currently at 6.1% of dataset0.05859420937299795\n",
      "    Currently at 6.23% of dataset0.05859420937299795\n",
      "    Currently at 6.36% of dataset0.05859420937299795\n",
      "    Currently at 6.49% of dataset0.05859420937299795\n",
      "    Currently at 6.62% of dataset0.05859420937299795\n",
      "    Currently at 6.75% of dataset0.05859420937299795\n",
      "    Currently at 6.88% of dataset0.05859420937299795\n",
      "    Currently at 7.01% of dataset0.05859420937299795\n",
      "    Currently at 7.14% of dataset0.05859420937299795\n",
      "    Currently at 7.27% of dataset0.05859420937299795\n",
      "    Currently at 7.4% of dataset0.05859420937299795\n",
      "    Currently at 7.53% of dataset0.05859420937299795\n",
      "    Currently at 7.66% of dataset0.05859420937299795\n",
      "    Currently at 7.79% of dataset0.05859420937299795\n",
      "    Currently at 7.92% of dataset0.05859420937299795\n",
      "    Currently at 8.05% of dataset0.05859420937299795\n",
      "    Currently at 8.18% of dataset0.05859420937299795\n",
      "    Currently at 8.31% of dataset0.05859420937299795\n",
      "    Currently at 8.44% of dataset0.05859420937299795\n",
      "    Currently at 8.57% of dataset0.05859420937299795\n",
      "    Currently at 8.7% of dataset0.05859420937299795\n",
      "    Currently at 8.83% of dataset0.05859420937299795\n",
      "    Currently at 8.96% of dataset0.05859420937299795\n",
      "    Currently at 9.09% of dataset0.05859420937299795\n",
      "    Currently at 9.22% of dataset0.05859420937299795\n",
      "    Currently at 9.35% of dataset0.05859420937299795\n",
      "    Currently at 9.48% of dataset0.05859420937299795\n",
      "    Currently at 9.61% of dataset0.05859420937299795\n",
      "    Currently at 9.74% of dataset0.05859420937299795\n",
      "    Currently at 9.87% of dataset0.05859420937299795\n",
      "    Currently at 10.0% of dataset0.05859420937299795\n",
      "    Currently at 10.13% of dataset0.05859420937299795\n",
      "    Currently at 10.26% of dataset0.05859420937299795\n",
      "    Currently at 10.39% of dataset0.05859420937299795\n",
      "    Currently at 10.52% of dataset0.05859420937299795\n",
      "    Currently at 10.65% of dataset0.05859420937299795\n",
      "    Currently at 10.78% of dataset0.05859420937299795\n",
      "    Currently at 10.91% of dataset0.05859420937299795\n",
      "    Currently at 11.04% of dataset0.05859420937299795\n",
      "    Currently at 11.17% of dataset0.05859420937299795\n",
      "    Currently at 11.3% of dataset0.05859420937299795\n",
      "    Currently at 11.43% of dataset0.05859420937299795\n",
      "    Currently at 11.56% of dataset0.05859420937299795\n",
      "    Currently at 11.69% of dataset0.05859420937299795\n",
      "    Currently at 11.82% of dataset0.05859420937299795\n",
      "    Currently at 11.95% of dataset0.05859420937299795\n",
      "    Currently at 12.08% of dataset0.05859420937299795\n",
      "    Currently at 12.21% of dataset0.05859420937299795\n",
      "    Currently at 12.34% of dataset0.05859420937299795\n",
      "    Currently at 12.47% of dataset0.05859420937299795\n",
      "    Currently at 12.6% of dataset0.05859420937299795\n",
      "    Currently at 12.73% of dataset0.05859420937299795\n",
      "    Currently at 12.86% of dataset0.05859420937299795\n",
      "    Currently at 12.99% of dataset0.05859420937299795\n",
      "    Currently at 13.12% of dataset0.05859420937299795\n",
      "    Currently at 13.25% of dataset0.05859420937299795\n",
      "    Currently at 13.38% of dataset0.05859420937299795\n",
      "    Currently at 13.51% of dataset0.05859420937299795\n",
      "    Currently at 13.64% of dataset0.05859420937299795\n",
      "    Currently at 13.77% of dataset0.05859420937299795\n",
      "    Currently at 13.9% of dataset0.05859420937299795\n",
      "    Currently at 14.03% of dataset0.05859420937299795\n",
      "    Currently at 14.16% of dataset0.05859420937299795\n",
      "    Currently at 14.29% of dataset0.05859420937299795\n",
      "    Currently at 14.42% of dataset0.05859420937299795\n",
      "    Currently at 14.55% of dataset0.05859420937299795\n",
      "    Currently at 14.68% of dataset0.05859420937299795\n",
      "    Currently at 14.81% of dataset0.05859420937299795\n",
      "    Currently at 14.94% of dataset0.05859420937299795\n",
      "    Currently at 15.06% of dataset0.05859420937299795\n",
      "    Currently at 15.19% of dataset0.05859420937299795\n",
      "    Currently at 15.32% of dataset0.05859420937299795\n",
      "    Currently at 15.45% of dataset0.05859420937299795\n",
      "    Currently at 15.58% of dataset0.05859420937299795\n",
      "    Currently at 15.71% of dataset0.05859420937299795\n",
      "    Currently at 15.84% of dataset0.05859420937299795\n",
      "    Currently at 15.97% of dataset0.05859420937299795\n",
      "    Currently at 16.1% of dataset0.05859420937299795\n",
      "    Currently at 16.23% of dataset0.05859420937299795\n",
      "    Currently at 16.36% of dataset0.05859420937299795\n",
      "    Currently at 16.49% of dataset0.05859420937299795\n",
      "    Currently at 16.62% of dataset0.05859420937299795\n",
      "    Currently at 16.75% of dataset0.05859420937299795\n",
      "    Currently at 16.88% of dataset0.05859420937299795\n",
      "    Currently at 17.01% of dataset0.05859420937299795\n",
      "    Currently at 17.14% of dataset0.05859420937299795\n",
      "    Currently at 17.27% of dataset0.05859420937299795\n",
      "    Currently at 17.4% of dataset0.05859420937299795\n",
      "    Currently at 17.53% of dataset0.05859420937299795\n",
      "    Currently at 17.66% of dataset0.05859420937299795\n",
      "    Currently at 17.79% of dataset0.05859420937299795\n",
      "    Currently at 17.92% of dataset0.05859420937299795\n",
      "    Currently at 18.05% of dataset0.05859420937299795\n",
      "    Currently at 18.18% of dataset0.05859420937299795\n",
      "    Currently at 18.31% of dataset0.05859420937299795\n",
      "    Currently at 18.44% of dataset0.05859420937299795\n",
      "    Currently at 18.57% of dataset0.05859420937299795\n",
      "    Currently at 18.7% of dataset0.05859420937299795\n",
      "    Currently at 18.83% of dataset0.05859420937299795\n",
      "    Currently at 18.96% of dataset0.05859420937299795\n",
      "    Currently at 19.09% of dataset0.05859420937299795\n",
      "    Currently at 19.22% of dataset0.05859420937299795\n",
      "    Currently at 19.35% of dataset0.05671157749891348\n",
      "    Currently at 19.48% of dataset0.05671157749891348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Currently at 19.61% of dataset0.05671157749891348\n",
      "    Currently at 19.74% of dataset0.05671157749891348\n",
      "    Currently at 19.87% of dataset0.05671157749891348\n",
      "    Currently at 20.0% of dataset0.05671157749891348\n",
      "    Currently at 20.13% of dataset0.05671157749891348\n",
      "    Currently at 20.26% of dataset0.05671157749891348\n",
      "    Currently at 20.39% of dataset0.05671157749891348\n",
      "    Currently at 20.52% of dataset0.05671157749891348\n",
      "    Currently at 20.65% of dataset0.05671157749891348\n",
      "    Currently at 20.78% of dataset0.05671157749891348\n",
      "    Currently at 20.91% of dataset0.05671157749891348\n",
      "    Currently at 21.04% of dataset0.05671157749891348\n",
      "    Currently at 21.17% of dataset0.05671157749891348\n",
      "    Currently at 21.3% of dataset0.05671157749891348\n",
      "    Currently at 21.43% of dataset0.05671157749891348\n",
      "    Currently at 21.56% of dataset0.05671157749891348\n",
      "    Currently at 21.69% of dataset0.05671157749891348\n",
      "    Currently at 21.82% of dataset0.05671157749891348\n",
      "    Currently at 21.95% of dataset0.05671157749891348\n",
      "    Currently at 22.08% of dataset0.05671157749891348\n",
      "    Currently at 22.21% of dataset0.05671157749891348\n",
      "    Currently at 22.34% of dataset0.05671157749891348\n",
      "    Currently at 22.47% of dataset0.05671157749891348\n",
      "    Currently at 22.6% of dataset0.05671157749891348\n",
      "    Currently at 22.73% of dataset0.05671157749891348\n",
      "    Currently at 22.86% of dataset0.05671157749891348\n",
      "    Currently at 22.99% of dataset0.05671157749891348\n",
      "    Currently at 23.12% of dataset0.05671157749891348\n",
      "    Currently at 23.25% of dataset0.05671157749891348\n",
      "    Currently at 23.38% of dataset0.05671157749891348\n",
      "    Currently at 23.51% of dataset0.05671157749891348\n",
      "    Currently at 23.64% of dataset0.05671157749891348\n",
      "    Currently at 23.77% of dataset0.05671157749891348\n",
      "    Currently at 23.9% of dataset0.05671157749891348\n",
      "    Currently at 24.03% of dataset0.05671157749891348\n",
      "    Currently at 24.16% of dataset0.05671157749891348\n",
      "    Currently at 24.29% of dataset0.05671157749891348\n",
      "    Currently at 24.42% of dataset0.05671157749891348\n",
      "    Currently at 24.55% of dataset0.05671157749891348\n",
      "    Currently at 24.68% of dataset0.05671157749891348\n",
      "    Currently at 24.81% of dataset0.05671157749891348\n",
      "    Currently at 24.94% of dataset0.05671157749891348\n",
      "    Currently at 25.06% of dataset0.05671157749891348\n",
      "    Currently at 25.19% of dataset0.05671157749891348\n",
      "    Currently at 25.32% of dataset0.05671157749891348\n",
      "    Currently at 25.45% of dataset0.05671157749891348\n",
      "    Currently at 25.58% of dataset0.05671157749891348\n",
      "    Currently at 25.71% of dataset0.05671157749891348\n",
      "    Currently at 25.84% of dataset0.05671157749891348\n",
      "    Currently at 25.97% of dataset0.05671157749891348\n",
      "    Currently at 26.1% of dataset0.05671157749891348\n",
      "    Currently at 26.23% of dataset0.05671157749891348\n",
      "    Currently at 26.36% of dataset0.05671157749891348\n",
      "    Currently at 26.49% of dataset0.05671157749891348\n",
      "    Currently at 26.62% of dataset0.05671157749891348\n",
      "    Currently at 26.75% of dataset0.05671157749891348\n",
      "    Currently at 26.88% of dataset0.05671157749891348\n",
      "    Currently at 27.01% of dataset0.05671157749891348\n",
      "    Currently at 27.14% of dataset0.05671157749891348\n",
      "    Currently at 27.27% of dataset0.05671157749891348\n",
      "    Currently at 27.4% of dataset0.048814095199108794\n",
      "    Currently at 27.53% of dataset0.048814095199108794\n",
      "    Currently at 27.66% of dataset0.048814095199108794\n",
      "    Currently at 27.79% of dataset0.048814095199108794\n",
      "    Currently at 27.92% of dataset0.048814095199108794\n",
      "    Currently at 28.05% of dataset0.048814095199108794\n",
      "    Currently at 28.18% of dataset0.048814095199108794\n",
      "    Currently at 28.31% of dataset0.048814095199108794\n",
      "    Currently at 28.44% of dataset0.048814095199108794\n",
      "    Currently at 28.57% of dataset0.048814095199108794\n",
      "    Currently at 28.7% of dataset0.048814095199108794\n",
      "    Currently at 28.83% of dataset0.048814095199108794\n",
      "    Currently at 28.96% of dataset0.048814095199108794\n",
      "    Currently at 29.09% of dataset0.048814095199108794\n",
      "    Currently at 29.22% of dataset0.048814095199108794\n",
      "    Currently at 29.35% of dataset0.048814095199108794\n",
      "    Currently at 29.48% of dataset0.048814095199108794\n",
      "    Currently at 29.61% of dataset0.048814095199108794\n",
      "    Currently at 29.74% of dataset0.048814095199108794\n",
      "    Currently at 29.87% of dataset0.04452027157545157\n",
      "    Currently at 30.0% of dataset0.04452027157545157\n",
      "    Currently at 30.13% of dataset0.04452027157545157\n",
      "    Currently at 30.26% of dataset0.04452027157545157\n",
      "    Currently at 30.39% of dataset0.04452027157545157\n",
      "    Currently at 30.52% of dataset0.04452027157545157\n",
      "    Currently at 30.65% of dataset0.04452027157545157\n",
      "    Currently at 30.78% of dataset0.04452027157545157\n",
      "    Currently at 30.91% of dataset0.04452027157545157\n",
      "    Currently at 31.04% of dataset0.04452027157545157\n",
      "    Currently at 31.17% of dataset0.04452027157545157\n",
      "    Currently at 31.3% of dataset0.04452027157545157\n",
      "    Currently at 31.43% of dataset0.04452027157545157\n",
      "    Currently at 31.56% of dataset0.04452027157545157\n",
      "    Currently at 31.69% of dataset0.04452027157545157\n",
      "    Currently at 31.82% of dataset0.04452027157545157\n",
      "    Currently at 31.95% of dataset0.04452027157545157\n",
      "    Currently at 32.08% of dataset0.04452027157545157\n",
      "    Currently at 32.21% of dataset0.04452027157545157\n",
      "    Currently at 32.34% of dataset0.04452027157545157\n",
      "    Currently at 32.47% of dataset0.04452027157545157\n",
      "    Currently at 32.6% of dataset0.04452027157545157\n",
      "    Currently at 32.73% of dataset0.04452027157545157\n",
      "    Currently at 32.86% of dataset0.04452027157545157\n",
      "    Currently at 32.99% of dataset0.04452027157545157\n",
      "    Currently at 33.12% of dataset0.04452027157545157\n",
      "    Currently at 33.25% of dataset0.04452027157545157\n",
      "    Currently at 33.38% of dataset0.04452027157545157\n",
      "    Currently at 33.51% of dataset0.04452027157545157\n",
      "    Currently at 33.64% of dataset0.04452027157545157\n",
      "    Currently at 33.77% of dataset0.04452027157545157\n",
      "    Currently at 33.9% of dataset0.04452027157545157\n",
      "    Currently at 34.03% of dataset0.044516221115027094\n",
      "    Currently at 34.16% of dataset0.044516221115027094\n",
      "    Currently at 34.29% of dataset0.044516221115027094\n",
      "    Currently at 34.42% of dataset0.044516221115027094\n",
      "    Currently at 34.55% of dataset0.044516221115027094\n",
      "    Currently at 34.68% of dataset0.044516221115027094\n",
      "    Currently at 34.81% of dataset0.044516221115027094\n",
      "    Currently at 34.94% of dataset0.044516221115027094\n",
      "    Currently at 35.06% of dataset0.044516221115027094\n",
      "    Currently at 35.19% of dataset0.044516221115027094\n",
      "    Currently at 35.32% of dataset0.044516221115027094\n",
      "    Currently at 35.45% of dataset0.044516221115027094\n",
      "    Currently at 35.58% of dataset0.044516221115027094\n",
      "    Currently at 35.71% of dataset0.044516221115027094\n",
      "    Currently at 35.84% of dataset0.044516221115027094\n",
      "    Currently at 35.97% of dataset0.044516221115027094\n",
      "    Currently at 36.1% of dataset0.044516221115027094\n",
      "    Currently at 36.23% of dataset0.044516221115027094\n",
      "    Currently at 36.36% of dataset0.044516221115027094\n",
      "    Currently at 36.49% of dataset0.044516221115027094\n",
      "    Currently at 36.62% of dataset0.044516221115027094\n",
      "    Currently at 36.75% of dataset0.044516221115027094\n",
      "    Currently at 36.88% of dataset0.044516221115027094\n",
      "    Currently at 37.01% of dataset0.044516221115027094\n",
      "    Currently at 37.14% of dataset0.044516221115027094\n",
      "    Currently at 37.27% of dataset0.044516221115027094\n",
      "    Currently at 37.4% of dataset0.044516221115027094\n",
      "    Currently at 37.53% of dataset0.044516221115027094\n",
      "    Currently at 37.66% of dataset0.044516221115027094\n",
      "    Currently at 37.79% of dataset0.044516221115027094\n",
      "    Currently at 37.92% of dataset0.044516221115027094\n",
      "    Currently at 38.05% of dataset0.044516221115027094\n",
      "    Currently at 38.18% of dataset0.044516221115027094\n",
      "    Currently at 38.31% of dataset0.044516221115027094\n",
      "    Currently at 38.44% of dataset0.044516221115027094\n",
      "    Currently at 38.57% of dataset0.044516221115027094\n",
      "    Currently at 38.7% of dataset0.044516221115027094\n",
      "    Currently at 38.83% of dataset0.044516221115027094\n",
      "    Currently at 38.96% of dataset0.044516221115027094\n",
      "    Currently at 39.09% of dataset0.044516221115027094\n",
      "    Currently at 39.22% of dataset0.044516221115027094\n",
      "    Currently at 39.35% of dataset0.044516221115027094\n",
      "    Currently at 39.48% of dataset0.044516221115027094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Currently at 39.61% of dataset0.044516221115027094\n",
      "    Currently at 39.74% of dataset0.044516221115027094\n",
      "    Currently at 39.87% of dataset0.044516221115027094\n",
      "    Currently at 40.0% of dataset0.044516221115027094\n",
      "    Currently at 40.13% of dataset0.044516221115027094\n",
      "    Currently at 40.26% of dataset0.044516221115027094\n",
      "    Currently at 40.39% of dataset0.044516221115027094\n",
      "    Currently at 40.52% of dataset0.044516221115027094\n",
      "    Currently at 40.65% of dataset0.044516221115027094\n",
      "    Currently at 40.78% of dataset0.044516221115027094\n",
      "    Currently at 40.91% of dataset0.044516221115027094\n",
      "    Currently at 41.04% of dataset0.044516221115027094\n",
      "    Currently at 41.17% of dataset0.044516221115027094\n",
      "    Currently at 41.3% of dataset0.044516221115027094\n",
      "    Currently at 41.43% of dataset0.044516221115027094\n",
      "    Currently at 41.56% of dataset0.044516221115027094\n",
      "    Currently at 41.69% of dataset0.044516221115027094\n",
      "    Currently at 41.82% of dataset0.044516221115027094\n",
      "    Currently at 41.95% of dataset0.044516221115027094\n",
      "    Currently at 42.08% of dataset0.044516221115027094\n",
      "    Currently at 42.21% of dataset0.044516221115027094\n",
      "    Currently at 42.34% of dataset0.044516221115027094\n",
      "    Currently at 42.47% of dataset0.044516221115027094\n",
      "    Currently at 42.6% of dataset0.044516221115027094\n",
      "    Currently at 42.73% of dataset0.044516221115027094\n",
      "    Currently at 42.86% of dataset0.044516221115027094\n",
      "    Currently at 42.99% of dataset0.044516221115027094\n",
      "    Currently at 43.12% of dataset0.044516221115027094\n",
      "    Currently at 43.25% of dataset0.044516221115027094\n",
      "    Currently at 43.38% of dataset0.044516221115027094\n",
      "    Currently at 43.51% of dataset0.044516221115027094\n",
      "    Currently at 43.64% of dataset0.044516221115027094\n",
      "    Currently at 43.77% of dataset0.044516221115027094\n",
      "    Currently at 43.9% of dataset0.044516221115027094\n",
      "    Currently at 44.03% of dataset0.044516221115027094\n",
      "    Currently at 44.16% of dataset0.044516221115027094\n",
      "    Currently at 44.29% of dataset0.044516221115027094\n",
      "    Currently at 44.42% of dataset0.044516221115027094\n",
      "    Currently at 44.55% of dataset0.044516221115027094\n",
      "    Currently at 44.68% of dataset0.044516221115027094\n",
      "    Currently at 44.81% of dataset0.044516221115027094\n",
      "    Currently at 44.94% of dataset0.044516221115027094\n",
      "    Currently at 45.06% of dataset0.044516221115027094\n",
      "    Currently at 45.19% of dataset0.044516221115027094\n",
      "    Currently at 45.32% of dataset0.044516221115027094\n",
      "    Currently at 45.45% of dataset0.044516221115027094\n",
      "    Currently at 45.58% of dataset0.044516221115027094\n",
      "    Currently at 45.71% of dataset0.044516221115027094\n",
      "    Currently at 45.84% of dataset0.044516221115027094\n",
      "    Currently at 45.97% of dataset0.044516221115027094\n",
      "    Currently at 46.1% of dataset0.044516221115027094\n",
      "    Currently at 46.23% of dataset0.044516221115027094\n",
      "    Currently at 46.36% of dataset0.044516221115027094\n",
      "    Currently at 46.49% of dataset0.044516221115027094\n",
      "    Currently at 46.62% of dataset0.044516221115027094\n",
      "    Currently at 46.75% of dataset0.044516221115027094\n",
      "    Currently at 46.88% of dataset0.044516221115027094\n",
      "    Currently at 47.01% of dataset0.044516221115027094\n",
      "    Currently at 47.14% of dataset0.044516221115027094\n",
      "    Currently at 47.27% of dataset0.044516221115027094\n",
      "    Currently at 47.4% of dataset0.044516221115027094\n",
      "    Currently at 47.53% of dataset0.044516221115027094\n",
      "    Currently at 47.66% of dataset0.044516221115027094\n",
      "    Currently at 47.79% of dataset0.044516221115027094\n",
      "    Currently at 47.92% of dataset0.044516221115027094\n",
      "    Currently at 48.05% of dataset0.044516221115027094\n",
      "    Currently at 48.18% of dataset0.044516221115027094\n",
      "    Currently at 48.31% of dataset0.044516221115027094\n",
      "    Currently at 48.44% of dataset0.044516221115027094\n",
      "    Currently at 48.57% of dataset0.044516221115027094\n",
      "    Currently at 48.7% of dataset0.044516221115027094\n",
      "    Currently at 48.83% of dataset0.044516221115027094\n",
      "    Currently at 48.96% of dataset0.044516221115027094\n",
      "    Currently at 49.09% of dataset0.044516221115027094\n",
      "    Currently at 49.22% of dataset0.044516221115027094\n",
      "    Currently at 49.35% of dataset0.044516221115027094\n",
      "    Currently at 49.48% of dataset0.044516221115027094\n",
      "    Currently at 49.61% of dataset0.044516221115027094\n",
      "    Currently at 49.74% of dataset0.044516221115027094\n",
      "    Currently at 49.87% of dataset0.044516221115027094\n",
      "    Currently at 50.0% of dataset0.044516221115027094\n",
      "    Currently at 50.13% of dataset0.044516221115027094\n",
      "    Currently at 50.26% of dataset0.044516221115027094\n",
      "    Currently at 50.39% of dataset0.044516221115027094\n",
      "    Currently at 50.52% of dataset0.044516221115027094\n",
      "    Currently at 50.65% of dataset0.044516221115027094\n",
      "    Currently at 50.78% of dataset0.044516221115027094\n",
      "    Currently at 50.91% of dataset0.044516221115027094\n",
      "    Currently at 51.04% of dataset0.044516221115027094\n",
      "    Currently at 51.17% of dataset0.044516221115027094\n",
      "    Currently at 51.3% of dataset0.044516221115027094\n",
      "    Currently at 51.43% of dataset0.044516221115027094\n",
      "    Currently at 51.56% of dataset0.044516221115027094\n",
      "    Currently at 51.69% of dataset0.044516221115027094\n",
      "    Currently at 51.82% of dataset0.044516221115027094\n",
      "    Currently at 51.95% of dataset0.044516221115027094\n",
      "    Currently at 52.08% of dataset0.044516221115027094\n",
      "    Currently at 52.21% of dataset0.044516221115027094\n",
      "    Currently at 52.34% of dataset0.044516221115027094\n",
      "    Currently at 52.47% of dataset0.044516221115027094\n",
      "    Currently at 52.6% of dataset0.044516221115027094\n",
      "    Currently at 52.73% of dataset0.044516221115027094\n",
      "    Currently at 52.86% of dataset0.044516221115027094\n",
      "    Currently at 52.99% of dataset0.044516221115027094\n",
      "    Currently at 53.12% of dataset0.044516221115027094\n",
      "    Currently at 53.25% of dataset0.044516221115027094\n",
      "    Currently at 53.38% of dataset0.044516221115027094\n",
      "    Currently at 53.51% of dataset0.044516221115027094\n",
      "    Currently at 53.64% of dataset0.044516221115027094\n",
      "    Currently at 53.77% of dataset0.044516221115027094\n",
      "    Currently at 53.9% of dataset0.044516221115027094\n",
      "    Currently at 54.03% of dataset0.044516221115027094\n",
      "    Currently at 54.16% of dataset0.044516221115027094\n",
      "    Currently at 54.29% of dataset0.044516221115027094\n",
      "    Currently at 54.42% of dataset0.044516221115027094\n",
      "    Currently at 54.55% of dataset0.044516221115027094\n",
      "    Currently at 54.68% of dataset0.044516221115027094\n",
      "    Currently at 54.81% of dataset0.044516221115027094\n",
      "    Currently at 54.94% of dataset0.044516221115027094\n",
      "    Currently at 55.06% of dataset0.044516221115027094\n",
      "    Currently at 55.19% of dataset0.044516221115027094\n",
      "    Currently at 55.32% of dataset0.044516221115027094\n",
      "    Currently at 55.45% of dataset0.044516221115027094\n",
      "    Currently at 55.58% of dataset0.044516221115027094\n",
      "    Currently at 55.71% of dataset0.044516221115027094\n",
      "    Currently at 55.84% of dataset0.044516221115027094\n",
      "    Currently at 55.97% of dataset0.044516221115027094\n",
      "    Currently at 56.1% of dataset0.044516221115027094\n",
      "    Currently at 56.23% of dataset0.044516221115027094\n",
      "    Currently at 56.36% of dataset0.044516221115027094\n",
      "    Currently at 56.49% of dataset0.044516221115027094\n",
      "    Currently at 56.62% of dataset0.044516221115027094\n",
      "    Currently at 56.75% of dataset0.044516221115027094\n",
      "    Currently at 56.88% of dataset0.044516221115027094\n",
      "    Currently at 57.01% of dataset0.044516221115027094\n",
      "    Currently at 57.14% of dataset0.044516221115027094\n",
      "    Currently at 57.27% of dataset0.044516221115027094\n",
      "    Currently at 57.4% of dataset0.044516221115027094\n",
      "    Currently at 57.53% of dataset0.044516221115027094\n",
      "    Currently at 57.66% of dataset0.044516221115027094\n",
      "    Currently at 57.79% of dataset0.044516221115027094\n",
      "    Currently at 57.92% of dataset0.044516221115027094\n",
      "    Currently at 58.05% of dataset0.044516221115027094\n",
      "    Currently at 58.18% of dataset0.044516221115027094\n",
      "    Currently at 58.31% of dataset0.044516221115027094\n",
      "    Currently at 58.44% of dataset0.044516221115027094\n",
      "    Currently at 58.57% of dataset0.044516221115027094\n",
      "    Currently at 58.7% of dataset0.044516221115027094\n",
      "    Currently at 58.83% of dataset0.044516221115027094\n",
      "    Currently at 58.96% of dataset0.044516221115027094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Currently at 59.09% of dataset0.044516221115027094\n",
      "    Currently at 59.22% of dataset0.044516221115027094\n",
      "    Currently at 59.35% of dataset0.044516221115027094\n",
      "    Currently at 59.48% of dataset0.044516221115027094\n",
      "    Currently at 59.61% of dataset0.044516221115027094\n",
      "    Currently at 59.74% of dataset0.044516221115027094\n",
      "    Currently at 59.87% of dataset0.044516221115027094\n",
      "    Currently at 60.0% of dataset0.044516221115027094\n",
      "    Currently at 60.13% of dataset0.044516221115027094\n",
      "    Currently at 60.26% of dataset0.044516221115027094\n",
      "    Currently at 60.39% of dataset0.044516221115027094\n",
      "    Currently at 60.52% of dataset0.044516221115027094\n",
      "    Currently at 60.65% of dataset0.044516221115027094\n",
      "    Currently at 60.78% of dataset0.044516221115027094\n",
      "    Currently at 60.91% of dataset0.044516221115027094\n",
      "    Currently at 61.04% of dataset0.044516221115027094\n",
      "    Currently at 61.17% of dataset0.044516221115027094\n",
      "    Currently at 61.3% of dataset0.044516221115027094\n",
      "    Currently at 61.43% of dataset0.044516221115027094\n",
      "    Currently at 61.56% of dataset0.044516221115027094\n",
      "    Currently at 61.69% of dataset0.044516221115027094\n",
      "    Currently at 61.82% of dataset0.044516221115027094\n",
      "    Currently at 61.95% of dataset0.044516221115027094\n",
      "    Currently at 62.08% of dataset0.044516221115027094\n",
      "    Currently at 62.21% of dataset0.044516221115027094\n",
      "    Currently at 62.34% of dataset0.044516221115027094\n",
      "    Currently at 62.47% of dataset0.044516221115027094\n",
      "    Currently at 62.6% of dataset0.044516221115027094\n",
      "    Currently at 62.73% of dataset0.044516221115027094\n",
      "    Currently at 62.86% of dataset0.044516221115027094\n",
      "    Currently at 62.99% of dataset0.044516221115027094\n",
      "    Currently at 63.12% of dataset0.044516221115027094\n",
      "    Currently at 63.25% of dataset0.044516221115027094\n",
      "    Currently at 63.38% of dataset0.044516221115027094\n",
      "    Currently at 63.51% of dataset0.044516221115027094\n",
      "    Currently at 63.64% of dataset0.044516221115027094\n",
      "    Currently at 63.77% of dataset0.044516221115027094\n",
      "    Currently at 63.9% of dataset0.044516221115027094\n",
      "    Currently at 64.03% of dataset0.044516221115027094\n",
      "    Currently at 64.16% of dataset0.044516221115027094\n",
      "    Currently at 64.29% of dataset0.044516221115027094\n",
      "    Currently at 64.42% of dataset0.044516221115027094\n",
      "    Currently at 64.55% of dataset0.044516221115027094\n",
      "    Currently at 64.68% of dataset0.044516221115027094\n",
      "    Currently at 64.81% of dataset0.044516221115027094\n",
      "    Currently at 64.94% of dataset0.044516221115027094\n",
      "    Currently at 65.06% of dataset0.044516221115027094\n",
      "    Currently at 65.19% of dataset0.044516221115027094\n",
      "    Currently at 65.32% of dataset0.044516221115027094\n",
      "    Currently at 65.45% of dataset0.044516221115027094\n",
      "    Currently at 65.58% of dataset0.044516221115027094\n",
      "    Currently at 65.71% of dataset0.044516221115027094\n",
      "    Currently at 65.84% of dataset0.044516221115027094\n",
      "    Currently at 65.97% of dataset0.044516221115027094\n",
      "    Currently at 66.1% of dataset0.044516221115027094\n",
      "    Currently at 66.23% of dataset0.044516221115027094\n",
      "    Currently at 66.36% of dataset0.044516221115027094\n",
      "    Currently at 66.49% of dataset0.044516221115027094\n",
      "    Currently at 66.62% of dataset0.044516221115027094\n",
      "    Currently at 66.75% of dataset0.044516221115027094\n",
      "    Currently at 66.88% of dataset0.044516221115027094\n",
      "    Currently at 67.01% of dataset0.044516221115027094\n",
      "    Currently at 67.14% of dataset0.044516221115027094\n",
      "    Currently at 67.27% of dataset0.044516221115027094\n",
      "    Currently at 67.4% of dataset0.044516221115027094\n",
      "    Currently at 67.53% of dataset0.044516221115027094\n",
      "    Currently at 67.66% of dataset0.044516221115027094\n",
      "    Currently at 67.79% of dataset0.044516221115027094\n",
      "    Currently at 67.92% of dataset0.044516221115027094\n",
      "    Currently at 68.05% of dataset0.044516221115027094\n",
      "    Currently at 68.18% of dataset0.044516221115027094\n",
      "    Currently at 68.31% of dataset0.044516221115027094\n",
      "    Currently at 68.44% of dataset0.044516221115027094\n",
      "    Currently at 68.57% of dataset0.044516221115027094\n",
      "    Currently at 68.7% of dataset0.044516221115027094\n",
      "    Currently at 68.83% of dataset0.044516221115027094\n",
      "    Currently at 68.96% of dataset0.044516221115027094\n",
      "    Currently at 69.09% of dataset0.044516221115027094\n",
      "    Currently at 69.22% of dataset0.044516221115027094\n",
      "    Currently at 69.35% of dataset0.044516221115027094\n",
      "    Currently at 69.48% of dataset0.044516221115027094\n",
      "    Currently at 69.61% of dataset0.044516221115027094\n",
      "    Currently at 69.74% of dataset0.044516221115027094\n",
      "    Currently at 69.87% of dataset0.044516221115027094\n",
      "    Currently at 70.0% of dataset0.044516221115027094\n",
      "    Currently at 70.13% of dataset0.044516221115027094\n",
      "    Currently at 70.26% of dataset0.044516221115027094\n",
      "    Currently at 70.39% of dataset0.044516221115027094\n",
      "    Currently at 70.52% of dataset0.044516221115027094\n",
      "    Currently at 70.65% of dataset0.044516221115027094\n",
      "    Currently at 70.78% of dataset0.044516221115027094\n",
      "    Currently at 70.91% of dataset0.044516221115027094\n",
      "    Currently at 71.04% of dataset0.044516221115027094\n",
      "    Currently at 71.17% of dataset0.044516221115027094\n",
      "    Currently at 71.3% of dataset0.044516221115027094\n",
      "    Currently at 71.43% of dataset0.044516221115027094\n",
      "    Currently at 71.56% of dataset0.044516221115027094\n",
      "    Currently at 71.69% of dataset0.044516221115027094\n",
      "    Currently at 71.82% of dataset0.044516221115027094\n",
      "    Currently at 71.95% of dataset0.044516221115027094\n",
      "    Currently at 72.08% of dataset0.044516221115027094\n",
      "    Currently at 72.21% of dataset0.044516221115027094\n",
      "    Currently at 72.34% of dataset0.044516221115027094\n",
      "    Currently at 72.47% of dataset0.044516221115027094\n",
      "    Currently at 72.6% of dataset0.044516221115027094\n",
      "    Currently at 72.73% of dataset0.044516221115027094\n",
      "    Currently at 72.86% of dataset0.044516221115027094\n",
      "    Currently at 72.99% of dataset0.044516221115027094\n",
      "    Currently at 73.12% of dataset0.044516221115027094\n",
      "    Currently at 73.25% of dataset0.044516221115027094\n",
      "    Currently at 73.38% of dataset0.044516221115027094\n",
      "    Currently at 73.51% of dataset0.044516221115027094\n",
      "    Currently at 73.64% of dataset0.044516221115027094\n",
      "    Currently at 73.77% of dataset0.044516221115027094\n",
      "    Currently at 73.9% of dataset0.044516221115027094\n",
      "    Currently at 74.03% of dataset0.044516221115027094\n",
      "    Currently at 74.16% of dataset0.044516221115027094\n",
      "    Currently at 74.29% of dataset0.044516221115027094\n",
      "    Currently at 74.42% of dataset0.044516221115027094\n",
      "    Currently at 74.55% of dataset0.044516221115027094\n",
      "    Currently at 74.68% of dataset0.044516221115027094\n",
      "    Currently at 74.81% of dataset0.044516221115027094\n",
      "    Currently at 74.94% of dataset0.044516221115027094\n",
      "    Currently at 75.06% of dataset0.044516221115027094\n",
      "    Currently at 75.19% of dataset0.044516221115027094\n",
      "    Currently at 75.32% of dataset0.044516221115027094\n",
      "    Currently at 75.45% of dataset0.044516221115027094\n",
      "    Currently at 75.58% of dataset0.044516221115027094\n",
      "    Currently at 75.71% of dataset0.044516221115027094\n",
      "    Currently at 75.84% of dataset0.044516221115027094\n",
      "    Currently at 75.97% of dataset0.044516221115027094\n",
      "    Currently at 76.1% of dataset0.044516221115027094\n",
      "    Currently at 76.23% of dataset0.044516221115027094\n",
      "    Currently at 76.36% of dataset0.044516221115027094\n",
      "    Currently at 76.49% of dataset0.044516221115027094\n",
      "    Currently at 76.62% of dataset0.044516221115027094\n",
      "    Currently at 76.75% of dataset0.044516221115027094\n",
      "    Currently at 76.88% of dataset0.044516221115027094\n",
      "    Currently at 77.01% of dataset0.044516221115027094\n",
      "    Currently at 77.14% of dataset0.044516221115027094\n",
      "    Currently at 77.27% of dataset0.044516221115027094\n",
      "    Currently at 77.4% of dataset0.044516221115027094\n",
      "    Currently at 77.53% of dataset0.044516221115027094\n",
      "    Currently at 77.66% of dataset0.044516221115027094\n",
      "    Currently at 77.79% of dataset0.044516221115027094\n",
      "    Currently at 77.92% of dataset0.044516221115027094\n",
      "    Currently at 78.05% of dataset0.044516221115027094\n",
      "    Currently at 78.18% of dataset0.04355361695023803\n",
      "    Currently at 78.31% of dataset0.04355361695023803\n",
      "    Currently at 78.44% of dataset0.04355361695023803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Currently at 78.57% of dataset0.04355361695023803\n",
      "    Currently at 78.7% of dataset0.04355361695023803\n",
      "    Currently at 78.83% of dataset0.04355361695023803\n",
      "    Currently at 78.96% of dataset0.04355361695023803\n",
      "    Currently at 79.09% of dataset0.04355361695023803\n",
      "    Currently at 79.22% of dataset0.04355361695023803\n",
      "    Currently at 79.35% of dataset0.04355361695023803\n",
      "    Currently at 79.48% of dataset0.04355361695023803\n",
      "    Currently at 79.61% of dataset0.04355361695023803\n",
      "    Currently at 79.74% of dataset0.04355361695023803\n",
      "    Currently at 79.87% of dataset0.04355361695023803\n",
      "    Currently at 80.0% of dataset0.04355361695023803\n",
      "    Currently at 80.13% of dataset0.04355361695023803\n",
      "    Currently at 80.26% of dataset0.04355361695023803\n",
      "    Currently at 80.39% of dataset0.04355361695023803\n",
      "    Currently at 80.52% of dataset0.04355361695023803\n",
      "    Currently at 80.65% of dataset0.04355361695023803\n",
      "    Currently at 80.78% of dataset0.04355361695023803\n",
      "    Currently at 80.91% of dataset0.04355361695023803\n",
      "    Currently at 81.04% of dataset0.04355361695023803\n",
      "    Currently at 81.17% of dataset0.04355361695023803\n",
      "    Currently at 81.3% of dataset0.04355361695023803\n",
      "    Currently at 81.43% of dataset0.04355361695023803\n",
      "    Currently at 81.56% of dataset0.04355361695023803\n",
      "    Currently at 81.69% of dataset0.04355361695023803\n",
      "    Currently at 81.82% of dataset0.04355361695023803\n",
      "    Currently at 81.95% of dataset0.04355361695023803\n",
      "    Currently at 82.08% of dataset0.04355361695023803\n",
      "    Currently at 82.21% of dataset0.04355361695023803\n",
      "    Currently at 82.34% of dataset0.04355361695023803\n",
      "    Currently at 82.47% of dataset0.04355361695023803\n",
      "    Currently at 82.6% of dataset0.04355361695023803\n",
      "    Currently at 82.73% of dataset0.04355361695023803\n",
      "    Currently at 82.86% of dataset0.04355361695023803\n",
      "    Currently at 82.99% of dataset0.04355361695023803\n",
      "    Currently at 83.12% of dataset0.04355361695023803\n",
      "    Currently at 83.25% of dataset0.04355361695023803\n",
      "    Currently at 83.38% of dataset0.04355361695023803\n",
      "    Currently at 83.51% of dataset0.04355361695023803\n",
      "    Currently at 83.64% of dataset0.04355361695023803\n",
      "    Currently at 83.77% of dataset0.04355361695023803\n",
      "    Currently at 83.9% of dataset0.04355361695023803\n",
      "    Currently at 84.03% of dataset0.04355361695023803\n",
      "    Currently at 84.16% of dataset0.04355361695023803\n",
      "    Currently at 84.29% of dataset0.04355361695023803\n",
      "    Currently at 84.42% of dataset0.04355361695023803\n",
      "    Currently at 84.55% of dataset0.04355361695023803\n",
      "    Currently at 84.68% of dataset0.04355361695023803\n",
      "    Currently at 84.81% of dataset0.04355361695023803\n",
      "    Currently at 84.94% of dataset0.04355361695023803\n",
      "    Currently at 85.06% of dataset0.04355361695023803\n",
      "    Currently at 85.19% of dataset0.04355361695023803\n",
      "    Currently at 85.32% of dataset0.04355361695023803\n",
      "    Currently at 85.45% of dataset0.04355361695023803\n",
      "    Currently at 85.58% of dataset0.04355361695023803\n",
      "    Currently at 85.71% of dataset0.04355361695023803\n",
      "    Currently at 85.84% of dataset0.04355361695023803\n",
      "    Currently at 85.97% of dataset0.04355361695023803\n",
      "    Currently at 86.1% of dataset0.04355361695023803\n",
      "    Currently at 86.23% of dataset0.04355361695023803\n",
      "    Currently at 86.36% of dataset0.04355361695023803\n",
      "    Currently at 86.49% of dataset0.04355361695023803\n",
      "    Currently at 86.62% of dataset0.04355361695023803\n",
      "    Currently at 86.75% of dataset0.04355361695023803\n",
      "    Currently at 86.88% of dataset0.04355361695023803\n",
      "    Currently at 87.01% of dataset0.04355361695023803\n",
      "    Currently at 87.14% of dataset0.04355361695023803\n",
      "    Currently at 87.27% of dataset0.04355361695023803\n",
      "    Currently at 87.4% of dataset0.04355361695023803\n",
      "    Currently at 87.53% of dataset0.04355361695023803\n",
      "    Currently at 87.66% of dataset0.04355361695023803\n",
      "    Currently at 87.79% of dataset0.04355361695023803\n",
      "    Currently at 87.92% of dataset0.04355361695023803\n",
      "    Currently at 88.05% of dataset0.04355361695023803\n",
      "    Currently at 88.18% of dataset0.04355361695023803\n",
      "    Currently at 88.31% of dataset0.04355361695023803\n",
      "    Currently at 88.44% of dataset0.04355361695023803\n",
      "    Currently at 88.57% of dataset0.04355361695023803\n",
      "    Currently at 88.7% of dataset0.04355361695023803\n",
      "    Currently at 88.83% of dataset0.04355361695023803\n",
      "    Currently at 88.96% of dataset0.04355361695023803\n",
      "    Currently at 89.09% of dataset0.04355361695023803\n",
      "    Currently at 89.22% of dataset0.04355361695023803\n",
      "    Currently at 89.35% of dataset0.04355361695023803\n",
      "    Currently at 89.48% of dataset0.04355361695023803\n",
      "    Currently at 89.61% of dataset0.04355361695023803\n",
      "    Currently at 89.74% of dataset0.04355361695023803\n",
      "    Currently at 89.87% of dataset0.04355361695023803\n",
      "    Currently at 90.0% of dataset0.04355361695023803\n",
      "    Currently at 90.13% of dataset0.04355361695023803\n",
      "    Currently at 90.26% of dataset0.04355361695023803\n",
      "    Currently at 90.39% of dataset0.04355361695023803\n",
      "    Currently at 90.52% of dataset0.04355361695023803\n",
      "    Currently at 90.65% of dataset0.04355361695023803\n",
      "    Currently at 90.78% of dataset0.04355361695023803\n",
      "    Currently at 90.91% of dataset0.04355361695023803\n",
      "    Currently at 91.04% of dataset0.04355361695023803\n",
      "    Currently at 91.17% of dataset0.04355361695023803\n",
      "    Currently at 91.3% of dataset0.04355361695023803\n",
      "    Currently at 91.43% of dataset0.04355361695023803\n",
      "    Currently at 91.56% of dataset0.04355361695023803\n",
      "    Currently at 91.69% of dataset0.04355361695023803\n",
      "    Currently at 91.82% of dataset0.04355361695023803\n",
      "    Currently at 91.95% of dataset0.04355361695023803\n",
      "    Currently at 92.08% of dataset0.04355361695023803\n",
      "    Currently at 92.21% of dataset0.04355361695023803\n",
      "    Currently at 92.34% of dataset0.04355361695023803\n",
      "    Currently at 92.47% of dataset0.04355361695023803\n",
      "    Currently at 92.6% of dataset0.04355361695023803\n",
      "    Currently at 92.73% of dataset0.04355361695023803\n",
      "    Currently at 92.86% of dataset0.04355361695023803\n",
      "    Currently at 92.99% of dataset0.04355361695023803\n",
      "    Currently at 93.12% of dataset0.04355361695023803\n",
      "    Currently at 93.25% of dataset0.04355361695023803\n",
      "    Currently at 93.38% of dataset0.04355361695023803\n",
      "    Currently at 93.51% of dataset0.04355361695023803\n",
      "    Currently at 93.64% of dataset0.04355361695023803\n",
      "    Currently at 93.77% of dataset0.04355361695023803\n",
      "    Currently at 93.9% of dataset0.04355361695023803\n",
      "    Currently at 94.03% of dataset0.04355361695023803\n",
      "    Currently at 94.16% of dataset0.04355361695023803\n",
      "    Currently at 94.29% of dataset0.04355361695023803\n",
      "    Currently at 94.42% of dataset0.04355361695023803\n",
      "    Currently at 94.55% of dataset0.04355361695023803\n",
      "    Currently at 94.68% of dataset0.04355361695023803\n",
      "    Currently at 94.81% of dataset0.04355361695023803\n",
      "    Currently at 94.94% of dataset0.04355361695023803\n",
      "    Currently at 95.06% of dataset0.04355361695023803\n",
      "    Currently at 95.19% of dataset0.04355361695023803\n",
      "    Currently at 95.32% of dataset0.04355361695023803\n",
      "    Currently at 95.45% of dataset0.04355361695023803\n",
      "    Currently at 95.58% of dataset0.04355361695023803\n",
      "    Currently at 95.71% of dataset0.04355361695023803\n",
      "    Currently at 95.84% of dataset0.04355361695023803\n",
      "    Currently at 95.97% of dataset0.04355361695023803\n",
      "    Currently at 96.1% of dataset0.04355361695023803\n",
      "    Currently at 96.23% of dataset0.04355361695023803\n",
      "    Currently at 96.36% of dataset0.04355361695023803\n",
      "    Currently at 96.49% of dataset0.04355361695023803\n",
      "    Currently at 96.62% of dataset0.04355361695023803\n",
      "    Currently at 96.75% of dataset0.04355361695023803\n",
      "    Currently at 96.88% of dataset0.04355361695023803\n",
      "    Currently at 97.01% of dataset0.04355361695023803\n",
      "    Currently at 97.14% of dataset0.04355361695023803\n",
      "    Currently at 97.27% of dataset0.04355361695023803\n",
      "    Currently at 97.4% of dataset0.04355361695023803\n",
      "    Currently at 97.53% of dataset0.04355361695023803\n",
      "    Currently at 97.66% of dataset0.04355361695023803\n",
      "    Currently at 97.79% of dataset0.03342783722611693\n",
      "    Currently at 97.92% of dataset0.03342783722611693\n",
      "    Currently at 98.05% of dataset0.03342783722611693\n",
      "    Currently at 98.18% of dataset0.03342783722611693\n",
      "    Currently at 98.31% of dataset0.03342783722611693\n",
      "    Currently at 98.44% of dataset0.03342783722611693\n",
      "    Currently at 98.57% of dataset0.03342783722611693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Currently at 98.7% of dataset0.03342783722611693\n",
      "    Currently at 98.83% of dataset0.03342783722611693\n",
      "    Currently at 98.96% of dataset0.03342783722611693\n",
      "    Currently at 99.09% of dataset0.03342783722611693\n",
      "    Currently at 99.22% of dataset0.03342783722611693\n",
      "    Currently at 99.35% of dataset0.03342783722611693\n",
      "    Currently at 99.48% of dataset0.03342783722611693\n",
      "    Currently at 99.61% of dataset0.03342783722611693\n",
      "    Currently at 99.74% of dataset0.03342783722611693\n",
      "    Currently at 99.87% of dataset0.03342783722611693\n",
      "    Currently at 100.0% of dataset"
     ]
    }
   ],
   "source": [
    "if(OL==1):\n",
    "    Model_OL_all_mixed = Custom_Layer(model)\n",
    "    Model_OL_all_mixed.title = 'OL' \n",
    "    Model_OL_all_mixed.filename = 'OL'\n",
    "    Model_OL_all_mixed.l_rate = 0.00005 # 0.000005 true value jupyter\n",
    "\n",
    "    trainOneEpoch_OL(Model_OL_all_mixed, data_train, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROVA_PATH = ROOT_PATH + '\\\\Debug_files\\\\prova.txt'\n",
    "\n",
    "with open(PROVA_PATH,'w') as data_file: # open file\n",
    "\n",
    "    for q in range(0,80):        \n",
    "        data_file.write('\\'' + str('w') + str(q)+'\\'' + ',')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights, biases and the out of frozen layer from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_stm       = myDebug.debug_loadBiasSMT()\n",
    "weight_stm     = myDebug.debug_loadWeightsSTM()\n",
    "out_forzen_stm = myDebug.debug_loadOutputFrozenSMT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confront biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 276\n",
      "n bias     vowel\n",
      "  0          A             -0.00335938714       PC\n",
      "                           -0.00335936300       STM\n",
      "\u001b[1m\u001b[92m                            0.00000002414       difference\u001b[0m\u001b[0m \n",
      "\n",
      "  1          E             -0.00328160813       PC\n",
      "                           -0.00327951800       STM\n",
      "\u001b[1m\u001b[92m                            0.00000209013       difference\u001b[0m\u001b[0m \n",
      "\n",
      "  2          I              0.00665358657       PC\n",
      "                            0.00660358600       STM\n",
      "\u001b[1m\u001b[92m                           -0.00005000057       difference\u001b[0m\u001b[0m \n",
      "\n",
      "  3          O              0.00329789503       PC\n",
      "                            0.00329789500       STM\n",
      "\u001b[1m\u001b[92m                           -0.00000000003       difference\u001b[0m\u001b[0m \n",
      "\n",
      "  4          U             0.00000000000       PC\n",
      "                            0.00030356800       STM\n",
      "\u001b[1m\u001b[91m                            0.00030356800       difference\u001b[0m\u001b[0m \n",
      "\n",
      "  5          R             -0.00018002994       PC\n",
      "                           -0.00015543500       STM\n",
      "\u001b[1m\u001b[91m                            0.00002459494       difference\u001b[0m\u001b[0m \n",
      "\n",
      "  6          M              0.00010900119       PC\n",
      "                            0.00010900100       STM\n",
      "\u001b[1m\u001b[92m                           -0.00000000019       difference\u001b[0m\u001b[0m \n",
      "\n",
      "  7          B             -0.00000913636       PC\n",
      "                            0.00001415500       STM\n",
      "\u001b[1m\u001b[91m                            0.00002329136       difference\u001b[0m\u001b[0m \n",
      "\n",
      "correct label is: I\n",
      "    label: A   pred: 0.00000000000   pre soft: -533.1389149229\n",
      "    label: E   pred: 0.00000000000   pre soft: -685.0237749286\n",
      "    label: I   pred: 0.00000000000   pre soft: 811.7883902980\n",
      "    label: O   pred: 0.00000000000   pre soft: -10.1909474511\n",
      "    label: U   pred: 0.00000000000   pre soft: -258.8796907267\n",
      "    label: R   pred: 1.00000000000   pre soft: 105.3111448922\n",
      "    label: M   pred: 0.00000000000   pre soft: 354.2495611711\n",
      "    label: B   pred: 0.00000000000   pre soft: -332.7414427576\n"
     ]
    }
   ],
   "source": [
    "n_prova = 276 # 582 very bad\n",
    "myDebug.debug_confrontBias(n_prova, bias_stm, bias_pc, Model_OL_all_mixed.label) # max 771\n",
    "\n",
    "print(f'correct label is: {label_train[n_prova]}')\n",
    "for i in range (0, len(Model_OL_all_mixed.label)):\n",
    "    print(f'    label: {Model_OL_all_mixed.label[i]}   pred: {pred_pc[n_prova,i]:.11f}   pre soft: {pre_soft_pc[n_prova,i]:.10f}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confront weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_confrontWeights(numero, weight_stm, weight_pc, vec, selected_w):\n",
    "\n",
    "    vec_weig_stm = weight_stm[numero,vec]\n",
    "    vec_weig_pc  = weight_pc[numero,vec]\n",
    "    \n",
    "    col_OK    = '\\033[92m' #GREEN\n",
    "    col_WARN  = '\\033[93m' #YELLOW\n",
    "    col_FAIL  = '\\033[91m' #RED\n",
    "    col_RESET = '\\033[0m'  #RESET COLOR\n",
    "\n",
    "    print(f'Iteration number {numero}')\n",
    "    print('n weight')\n",
    "\n",
    "    for i in range(0, len(vec)):\n",
    "        if(vec_weig_pc[i]>0):\n",
    "            print(f'  {selected_w[i]}               {vec_weig_pc[i]:.11f}       PC')\n",
    "        else:\n",
    "            print(f'  {selected_w[i]}              {vec_weig_pc[i]:.11f}       PC')\n",
    "\n",
    "        if(vec_weig_stm[i]>0):\n",
    "            print(f'                  {vec_weig_stm[i]:.11f}       STM')\n",
    "        else:\n",
    "            print(f'                 {vec_weig_stm[i]:.11f}       STM')\n",
    "\n",
    "            \n",
    "        max_val = max( np.abs(vec_weig_pc[i]), np.abs(vec_weig_stm[i]) )\n",
    "        if( (np.abs(vec_weig_stm[i]-vec_weig_pc[i])/max_val) < 0.05):\n",
    "           \n",
    "            if(vec_weig_stm[i]-vec_weig_pc[i]>0):\n",
    "                print(f'\\033[1m{col_OK}                  {(vec_weig_stm[i]-vec_weig_pc[i]):.11f}       difference{col_RESET}\\033[0m ')\n",
    "            else:\n",
    "                print(f'\\033[1m{col_OK}                 {(vec_weig_stm[i]-vec_weig_pc[i]):.11f}       difference{col_RESET}\\033[0m ')\n",
    "        else:\n",
    "            if((vec_weig_stm[i]-vec_weig_pc[i])>0):\n",
    "                print(f'\\033[1m{col_FAIL}                  {(vec_weig_stm[i]-vec_weig_pc[i]):.11f}       difference{col_RESET}\\033[0m ')\n",
    "            else:\n",
    "                print(f'\\033[1m{col_FAIL}                 {(vec_weig_stm[i]-vec_weig_pc[i]):.11f}       difference{col_RESET}\\033[0m ')\n",
    "      \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00335936 -0.0032618   0.00655359 ...         nan         nan\n",
      "          nan]\n",
      " [-0.00335936 -0.0032618   0.00655359 ...         nan         nan\n",
      "          nan]\n",
      " [-0.00335936 -0.0032618   0.00655359 ...         nan         nan\n",
      "          nan]\n",
      " ...\n",
      " [-0.00330936 -0.00317952  0.00665362 ...         nan         nan\n",
      "          nan]\n",
      " [-0.00330936 -0.00317952  0.00665362 ...         nan         nan\n",
      "          nan]\n",
      " [-0.00330936 -0.00317952  0.00665362 ...         nan         nan\n",
      "          nan]]\n"
     ]
    }
   ],
   "source": [
    "print(weight_stm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0\n",
      "n weight\n",
      "  46               0.08765075356       PC\n",
      "                 nan       STM\n",
      "\u001b[1m\u001b[91m                 nan       difference\u001b[0m\u001b[0m \n",
      "\n",
      "  13              0.00000000000       PC\n",
      "                 nan       STM\n",
      "\u001b[1m\u001b[91m                 nan       difference\u001b[0m\u001b[0m \n",
      "\n",
      "  107              0.00000000000       PC\n",
      "                 nan       STM\n",
      "\u001b[1m\u001b[91m                 nan       difference\u001b[0m\u001b[0m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "weight_num  = [46,55, 68]\n",
    "\n",
    "n_prova = 0 \n",
    "\n",
    "debug_confrontWeights(n_prova, weight_stm, weight_pc, weight_num, selected_w) # max 771\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_pc[0,45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_temp = np.zeros(80)\n",
    "for i in range(0, 8):\n",
    "    vec_temp[i*10] = int(random.uniform(i*128,(i+1)*128))\n",
    "    vec_temp[i*10+1] = int(random.uniform(i*128,(i+1)*128))\n",
    "    vec_temp[i*10+2] = int(random.uniform(i*128,(i+1)*128))\n",
    "    vec_temp[i*10+3] = int(random.uniform(i*128,(i+1)*128))\n",
    "    vec_temp[i*10+4] = int(random.uniform(i*128,(i+1)*128))\n",
    "    vec_temp[i*10+5] = int(random.uniform(i*128,(i+1)*128))\n",
    "    vec_temp[i*10+6] = int(random.uniform(i*128,(i+1)*128))\n",
    "    vec_temp[i*10+7] = int(random.uniform(i*128,(i+1)*128))\n",
    "    vec_temp[i*10+8] = int(random.uniform(i*128,(i+1)*128))\n",
    "    vec_temp[i*10+9] = int(random.uniform(i*128,(i+1)*128))\n",
    "    \n",
    "#print(vec_temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Model_KERAS.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myDebug.debug_plotHistoryBias(0, bias_stm, bias_pc)\n",
    "myDebug.debug_plotHistoryBias(1, bias_stm, bias_pc)\n",
    "myDebug.debug_plotHistoryBias(2, bias_stm, bias_pc)\n",
    "myDebug.debug_plotHistoryBias(3, bias_stm, bias_pc)\n",
    "myDebug.debug_plotHistoryBias(4, bias_stm, bias_pc)\n",
    "myDebug.debug_plotHistoryBias(5, bias_stm, bias_pc)\n",
    "myDebug.debug_plotHistoryBias(6, bias_stm, bias_pc)\n",
    "myDebug.debug_plotHistoryBias(7, bias_stm, bias_pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(OL_mini==1):\n",
    "    Model_OL_mini = Custom_Layer(model)\n",
    "    Model_OL_mini.title = 'OL + mini batch'\n",
    "    Model_OL_mini.filename = 'OL_batches'\n",
    "    Model_OL_mini.l_rate = 0.0001\n",
    "\n",
    "    trainOneEpoch_OL_miniBatch(Model_OL_mini, data_train, label_train, batch_size_OL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(LWF==1):\n",
    "    Model_LWF_1 = Custom_Layer(model)\n",
    "    Model_LWF_1.title = 'LWF'\n",
    "    Model_LWF_1.filename = 'LWF'   \n",
    "    Model_LWF_1.l_rate = 0.0017 #0.001\n",
    "\n",
    "    trainOneEpochOL_LWF(Model_LWF_1, data_train, label_train)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LWF + mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(LWF_mini==1):\n",
    "    Model_LWF_2 = Custom_Layer(model)\n",
    "    Model_LWF_2.title = 'LWF + mini batch'\n",
    "    Model_LWF_2.filename = 'LWF_batches'\n",
    "    Model_LWF_2.l_rate = 0.000001\n",
    "\n",
    "    trainOneEpochOL_LWF_v2(Model_LWF_2, data_train, label_train, batch_size_OL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(OL_v2==1):\n",
    "    Model_OL_v2 = Custom_Layer(model)\n",
    "    Model_OL_v2.title = 'OL v2' \n",
    "    Model_OL_v2.filename = 'OL_v2'\n",
    "    Model_OL_v2.l_rate = 0.00005\n",
    "\n",
    "    trainOneEpoch_OL_v2(Model_OL_v2, data_train, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL v2 + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(OL_v2_mini==1):\n",
    "    Model_OL_v2_miniBatch = Custom_Layer(model)\n",
    "    Model_OL_v2_miniBatch.title = 'OL v2 + mini batch'\n",
    "    Model_OL_v2_miniBatch.filename = 'OL_v2_batches'\n",
    "    Model_OL_v2_miniBatch.l_rate = 0.001\n",
    "\n",
    "    trainOneEpoch_OL_v2_miniBatch(Model_OL_v2_miniBatch, data_train, label_train, batch_size_OL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with CWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(CWR==1):\n",
    "    Model_CWR = Custom_Layer(model) \n",
    "    Model_CWR.title = 'CWR'\n",
    "    Model_CWR.filename = 'CWR'\n",
    "    Model_CWR.l_rate = 0.00005\n",
    "\n",
    "    trainOneEpoch_CWR(Model_CWR, data_train, label_train, batch_size_OL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATION PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERAS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(KERAS==1):\n",
    "    myTest.test_OLlayer(Model_KERAS, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_KERAS)   \n",
    "    myMatrix.plot_confMatrix(Model_KERAS)\n",
    "    myTable.table_params(Model_KERAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(OL_vowels==1):\n",
    "    myTest.test_OLlayer(Model_OL_vowels, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_OL_vowels)\n",
    "    myMatrix.plot_confMatrix(Model_OL_vowels)\n",
    "    myTable.table_params(Model_OL_vowels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(OL==1):\n",
    "    myTest.test_OLlayer(Model_OL_all_mixed, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_OL_all_mixed)\n",
    "    myMatrix.plot_confMatrix(Model_OL_all_mixed)\n",
    "    myTable.table_params(Model_OL_all_mixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(OL_mini==1):\n",
    "    myTest.test_OLlayer(Model_OL_mini, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_OL_mini)\n",
    "    myMatrix.plot_confMatrix(Model_OL_mini)\n",
    "    myTable.table_params(Model_OL_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(LWF==1):\n",
    "    myTest.test_OLlayer(Model_LWF_1, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_LWF_1)\n",
    "    myMatrix.plot_confMatrix(Model_LWF_1)\n",
    "    myTable.table_params(Model_LWF_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LWF_mini==1):\n",
    "    myTest.test_OLlayer(Model_LWF_2, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_LWF_2)\n",
    "    myMatrix.plot_confMatrix(Model_LWF_2)\n",
    "    myTable.table_params(Model_LWF_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(OL_v2==1):\n",
    "    myTest.test_OLlayer(Model_OL_v2, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_OL_v2)\n",
    "    myMatrix.plot_confMatrix(Model_OL_v2)\n",
    "    myTable.table_params(Model_OL_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2 + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(OL_v2_mini==1):\n",
    "    myTest.test_OLlayer(Model_OL_v2_miniBatch, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_OL_v2_miniBatch)\n",
    "    myMatrix.plot_confMatrix(Model_OL_v2_miniBatch)\n",
    "    myTable.table_params(Model_OL_v2_miniBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(CWR==1):\n",
    "    myTest.test_OLlayer(Model_CWR, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_CWR)\n",
    "    myMatrix.plot_confMatrix(Model_CWR)\n",
    "    myTable.table_params(Model_CWR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All bar plots together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myBar.plot_barChart_All()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot is a recap of all the methods trained. Note that it will be displayed only if all the training have been performed in this runtime. \n",
    "The table contains some additiona information and not only the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(KERAS and OL_vowels and OL and OL_mini and LWF and LWF_mini and OL_v2 and OL_v2_mini and CWR):\n",
    "    \n",
    "    myTable.table_simulationResult(Model_KERAS, Model_OL_vowels, Model_OL_all_mixed, Model_OL_mini, \n",
    "               Model_LWF_1, Model_LWF_2, Model_OL_v2, Model_OL_v2_miniBatch, Model_CWR)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below is used for storing the result of the simulation formermed in this runtime. This is used for another plotting function that will display the average accuracy of each method across multiple runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down in txt files all the results across 10 or so simulations          \n",
    "WRITE_SIMU_RES = 0\n",
    "                \n",
    "if(WRITE_SIMU_RES==1):\n",
    "    myWrite.save_simulationResult('Keras',     Model_KERAS)\n",
    "    myWrite.save_simulationResult('OL_vowels', Model_OL_vowels)\n",
    "    myWrite.save_simulationResult('OL',        Model_OL_all_mixed)\n",
    "    myWrite.save_simulationResult('OL_mini',   Model_OL_mini)\n",
    "    myWrite.save_simulationResult('LWF',       Model_LWF_1)\n",
    "    myWrite.save_simulationResult('LWF_mini',  Model_LWF_2)\n",
    "    myWrite.save_simulationResult('OL_v2',     Model_OL_v2)\n",
    "    myWrite.save_simulationResult('OL_v2_min', Model_OL_v2_miniBatch)\n",
    "    myWrite.save_simulationResult('CWR',       Model_CWR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the average accuracy over several runtimes\n",
    "\n",
    "#myBar.plot_barChart_SimuRes(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below plots some pie charts that shows how the dataset are composed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_PLOTS = 0\n",
    "if(ENABLE_PLOTS==1):\n",
    "    \n",
    "    \n",
    "    vowels_data_tf, vowels_label_tf = myParse.loadDataFromTxt('vowels_TF')\n",
    "    TF_data_train, _, TF_data_test, _ = myParse.parseTrainTest(vowels_data_tf, vowels_label_tf, 0.7)\n",
    "\n",
    "\n",
    "    # Plot of the pie chart of the dataset TF e OL\n",
    "    dataset_shapes = np.zeros(8)\n",
    "    label_vow = ['A','E','I','O','U']\n",
    "\n",
    "    for i in range(0,vowels_data.shape[0]):\n",
    "        for j in range(0,len(label_vow)):\n",
    "            if(label_vow[j] == vowels_label[i]):\n",
    "                dataset_shapes[j] += 1\n",
    "                break\n",
    "    for i in range(0,vowels_data_tf.shape[0]):\n",
    "        for j in range(0,len(label_vow)):\n",
    "            if(label_vow[j] == vowels_label_tf[i]):\n",
    "                dataset_shapes[j] += 1\n",
    "                break\n",
    "\n",
    "    dataset_shapes[5] = B_data.shape[0]\n",
    "    dataset_shapes[6] = R_data.shape[0]\n",
    "    dataset_shapes[7] = M_data.shape[0]\n",
    "    myPie.plot_pieChart_datasetAll(dataset_shapes)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    dataset_shapes = np.zeros([8])\n",
    "    dataset_shapes[0] = OL_data_train_vow.shape[0]\n",
    "    dataset_shapes[1] = OL_data_test_vow.shape[0]\n",
    "    dataset_shapes[2] = B_train_data.shape[0]\n",
    "    dataset_shapes[3] = B_test_data.shape[0]\n",
    "    dataset_shapes[4] = R_train_data.shape[0]\n",
    "    dataset_shapes[5] = R_test_data.shape[0]\n",
    "    dataset_shapes[6] = M_train_data.shape[0]\n",
    "    dataset_shapes[7] = M_test_data.shape[0]\n",
    "    # Plot of the pie chart of the dataset OL\n",
    "    myPie.plot_pieChart_DatasetOL(dataset_shapes)\n",
    "\n",
    "    # Plot of the pie chart of the dataset TF\n",
    "    myPie.plot_pieChart_DatasetTF(TF_data_train.shape[0],TF_data_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
