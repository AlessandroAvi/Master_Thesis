{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import myLib_barChart as myBar\n",
    "import myLib_confMatrix as myMatrix\n",
    "import myLib_parseData as myParse\n",
    "import myLib_pieChart as myPie\n",
    "import myLib_table as myTable\n",
    "import myLib_testModel as myTest\n",
    "from myLib_testModel import letterToSoftmax\n",
    "import myLib_writeFile as myWrite\n",
    "import myLib_debugFiles as myDebug\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEfinition of string values for the bold print\n",
    "S_BOLD = '\\033[1m'\n",
    "E_BOLD = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET AND PREPARE TRAIN - TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the functions loadDataFromTxt and parseTrainTest are called. These allow to load the dataset from the txt files into matrices and then separate them in smaller matrices for testing and training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['A' 'E' 'I' 'O' 'U']\n",
      "\n",
      "Raw shape        -> (103600, 5)\n",
      "Tot samples      -> 518\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (361, 600)\n",
      "Test data shape   -> (155, 600)\n"
     ]
    }
   ],
   "source": [
    "vowels_data, vowels_label = myParse.loadDataFromTxt('vowels_OL')\n",
    "OL_data_train_vow, OL_label_train_vow, OL_data_test_vow, OL_label_test_vow = myParse.parseTrainTest(vowels_data, vowels_label, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['B']\n",
      "\n",
      "Raw shape        -> (39400, 5)\n",
      "Tot samples      -> 197\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (136, 600)\n",
      "Test data shape   -> (59, 600)\n"
     ]
    }
   ],
   "source": [
    "B_data, B_label = myParse.loadDataFromTxt('B_dataset')\n",
    "B_train_data, B_train_label, B_test_data, B_test_label = myParse.parseTrainTest(B_data, B_label, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['M']\n",
      "\n",
      "Raw shape        -> (39000, 5)\n",
      "Tot samples      -> 195\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (135, 600)\n",
      "Test data shape   -> (58, 600)\n"
     ]
    }
   ],
   "source": [
    "M_data, M_label = myParse.loadDataFromTxt('M_dataset')\n",
    "M_train_data, M_train_label, M_test_data, M_test_label = myParse.parseTrainTest(M_data, M_label, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['R']\n",
      "\n",
      "Raw shape        -> (39000, 5)\n",
      "Tot samples      -> 195\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (135, 600)\n",
      "Test data shape   -> (58, 600)\n"
     ]
    }
   ],
   "source": [
    "R_data, R_label = myParse.loadDataFromTxt('R_dataset')\n",
    "R_train_data, R_train_label, R_test_data, R_test_label = myParse.parseTrainTest(R_data, R_label, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a dataset of all letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this smaller section all the previous matrices are stacked together and then shuffled in order to create two big matrices that contain all the letters for training and testing. The training dataset is also shuffled, in order to shuffle it differently change the seed value inside the function myParse.shuffleDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_all has shape  (767, 600)\n",
      "label_all has shape (767,)\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix that contains all the train data\n",
    "data_all = OL_data_train_vow\n",
    "data_all = np.vstack(( data_all, B_train_data))\n",
    "data_all = np.vstack(( data_all, R_train_data))\n",
    "data_all = np.vstack(( data_all, M_train_data))\n",
    "# Create an array that contains all the train labels\n",
    "label_all = OL_label_train_vow\n",
    "label_all = np.hstack(( label_all, B_train_label))\n",
    "label_all = np.hstack(( label_all, R_train_label))\n",
    "label_all = np.hstack(( label_all, M_train_label))\n",
    "# Shuffle the matrix and the label\n",
    "data_all, label_all = myParse.shuffleDataset(data_all, label_all)\n",
    "\n",
    "print('data_all has shape  ' + str(data_all.shape))\n",
    "print('label_all has shape ' + str(label_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_test has shape  (330, 600)\n",
      "label_test has shape (330,)\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix that contains all the train data\n",
    "data_test = OL_data_test_vow\n",
    "data_test = np.vstack(( data_test, B_test_data))\n",
    "data_test = np.vstack(( data_test, R_test_data))\n",
    "data_test = np.vstack(( data_test, M_test_data))\n",
    "# Create an array that contains all the train labels\n",
    "label_test = OL_label_test_vow\n",
    "label_test = np.hstack(( label_test, B_test_label))\n",
    "label_test = np.hstack(( label_test, R_test_label))\n",
    "label_test = np.hstack(( label_test, M_test_label))\n",
    "\n",
    "print('data_test has shape  ' + str(data_test.shape))\n",
    "print('label_test has shape ' + str(label_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method for loading the dataset is to load it from the txt file \"training_file\". This file is an already shuffled dataset. I can use this for both feeding data in this simulation and also to the STM in order to have the closes behaviour possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['A' 'B' 'E' 'I' 'M' 'O' 'R' 'U']\n",
      "\n",
      "Raw shape        -> (118600, 5)\n",
      "Tot samples      -> 593\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (413, 600)\n",
      "Test data shape   -> (178, 600)\n"
     ]
    }
   ],
   "source": [
    "data, label = myParse.loadDataFromTxt('training_file')\n",
    "data_train, label_train, data_test, label_test = myParse.parseTrainTest(data, label, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class Data_Container is just a container that I created in order to have all the dataset in a single object. This is useful for the plotting functions because it allows me to give as input to the function just one object and not the entire list of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Container(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.R_test_data       = R_test_data\n",
    "        self.R_test_label      = R_test_label\n",
    "        self.B_test_data       = B_test_data\n",
    "        self.B_test_label      = B_test_label\n",
    "        self.M_test_data       = M_test_data\n",
    "        self.M_test_label      = M_test_label\n",
    "        self.R_test_data       = R_test_data\n",
    "        self.OL_data_test_vow  = OL_data_test_vow\n",
    "        self.OL_label_test_vow = OL_label_test_vow\n",
    "        \n",
    "OL_testing_data = Data_Container()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the content of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block is used only to check the type of letters that are inside the datasets that I imported. It's used in order to see if the datasets are created and saved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOWELS DATASET SANITY CHECK\n",
      "    The letters found are:              ['O', 'U', 'A', 'E', 'I']\n",
      "    And for each letter the counter is: [103. 103. 103. 103. 103.   1.   0.   0.   0.]\n",
      "\n",
      "B DATASET SANITY CHECK\n",
      "    The letters found are:              ['B']\n",
      "    And for each letter the counter is: [194.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      "\n",
      "R DATASET SANITY CHECK\n",
      "    The letters found are:              ['R']\n",
      "    And for each letter the counter is: [192.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      "\n",
      "M DATASET SANITY CHECK\n",
      "    The letters found are:              ['M']\n",
      "    And for each letter the counter is: [192.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      "\n",
      "TRAIN DATASET SANITY CHECK\n",
      "    The letters found are:              ['O', 'A', 'B', 'R', 'M', 'I', 'E', 'U']\n",
      "    And for each letter the counter is: [65. 73. 26. 22. 19. 69. 70. 68.  1.]\n",
      "\n",
      "TEST DATASET SANITY CHECK\n",
      "    The letters found are:              ['A', 'U', 'O', 'I', 'E', 'M', 'R', 'B']\n",
      "    And for each letter the counter is: [28. 34. 38. 35. 33.  4.  4.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print('VOWELS DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(vowels_label)\n",
    "print('\\nB DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(B_label)\n",
    "print('\\nR DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(R_label)\n",
    "print('\\nM DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(M_label)\n",
    "print('\\nTRAIN DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(label_train)\n",
    "print('\\nTEST DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD TF TRAINED MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I load the frozen model. The frozen model is the NN that has been trained with keras on the PC. The script that trains this model is called 'run_trainFroznModel.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.abspath('')\n",
    "MODEL_PATH = ROOT_PATH + \"/Saved_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(MODEL_PATH + 'Original_model/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TINY OL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the main part of the continual learning study is found. Here can be found the functions used for implementing the different algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is an implementation of the softmx function. I had to use this because I noticed that the sofmtax function used from keras and other methods for computing the sotmax operation gave different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc_softmax(array):\n",
    "    \"\"\" Computes softmax of an array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array_like\n",
    "        Is the array of which I want to compute the softmax operation\n",
    "    \"\"\"\n",
    "    \n",
    "    if(len(array.shape)==2):\n",
    "        array = array[0]\n",
    "        \n",
    "    size    = len(array)\n",
    "    ret_ary = np.zeros([len(array)])\n",
    "    m       = array[0]\n",
    "    sum_val = 0\n",
    "\n",
    "    for i in range(0, size):\n",
    "        if(m<array[i]):\n",
    "            m = array[i]\n",
    "\n",
    "    for i in range(0, size):\n",
    "        sum_val += np.exp(array[i] - m)\n",
    "\n",
    "    constant = m + np.log(sum_val)\n",
    "    for i in range(0, size):\n",
    "        ret_ary[i] = np.exp(array[i] - constant)\n",
    "        \n",
    "    return ret_ary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyOL class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is just a container for all the informations that are required in order to use correctly a tinyOL model. The idea is to createa  container in which everything is stored and then simply change the method for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Layer(object):\n",
    "    def __init__(self, model):\n",
    "\n",
    "        # Related to the layer\n",
    "        self.ML_frozen = keras.models.Sequential(model.layers[:-1])  # extract the last layer from the original model\n",
    "        self.ML_frozen.compile()\n",
    "        self.W = np.array(model.layers[-1].get_weights()[0])    # extract the weights from the last layer\n",
    "        self.b = np.array(model.layers[-1].get_weights()[1])    # extract the biases from the last layer\n",
    "        self.label = ['A', 'E', 'I', 'O', 'U']                  # the origina model knows only the vowels\n",
    "        self.l_rate = 0                                         # learning rate that changes depending on the algorithm\n",
    "        self.W_counter = np.zeros(self.W.shape)\n",
    "        \n",
    "        self.width = self.W.shape[0]        # shape of the weights matrix\n",
    "        \n",
    "        \n",
    "        # Related to the results fo the model\n",
    "        self.confusion_matrix = []          # container for the confusion matrix\n",
    "        self.correct_ary = []               # array that contains the number of correct prediction for each letter\n",
    "        self.mistake_ary = []               # array that contains the number of mistaken prediction for each letter\n",
    "        self.totals_ary = []                # array that contains the number of total prediction for each letter\n",
    "        \n",
    "        self.macro_avrg_precision = 0       \n",
    "        self.macro_avrg_recall = 0\n",
    "        self.macro_avrg_F1score = 0\n",
    "        \n",
    "        self.title = ''       # title that will be displayed on plots\n",
    "        self.filename = ''    # name of the files to be saved (plots, charts, conf matrix)\n",
    "        \n",
    "        \n",
    "    # Function that is used for the prediction of the model saved in this class\n",
    "    def predict(self, x):\n",
    "        mat_prod = np.array(np.matmul(x, self.W) + self.b)\n",
    "        return  myFunc_softmax(mat_prod) # othwerwise do it with keras|also remove np.array()| tf.nn.softmax(mat_prod) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyOL functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used in all methods before the feed forward of the OL layer. This function is required because it checks if the input letter is already known. If this is not true it will increse the dimension of the last layre (weight matrix and biases array) and also save the new letter in the 'known classes' array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkLabelKnown(model, current_label):\n",
    "    \n",
    "    found = 0\n",
    "    \n",
    "    for i in range(0, len(model.label)):\n",
    "        if(current_label == model.label[i]):\n",
    "            found = 1\n",
    "        \n",
    "        \n",
    "    # If the label is not known\n",
    "    if(found==0):\n",
    "        print(f'\\n\\n    New letter detected -> letter \\033[1m{current_label}\\033[0m \\n')\n",
    "\n",
    "        model.label.append(current_label)   # Add new letter to label\n",
    "        \n",
    "        # Increase weights and biases dimensions\n",
    "        model.W = np.hstack((model.W, np.zeros([model.width,1])))\n",
    "        model.b = np.hstack((model.b, np.zeros([1])))\n",
    "        \n",
    "        model.W_counter = np.hstack((model.W_counter, np.zeros((model.width,1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here the functions that implement the different methods can be found. The explanation of the code od these function is not here but it can be found in the paper \"Continuous learning in single incremental taskscenarios\" and some schemes can be found in my presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb = data_train.shape[0]\n",
    "\n",
    "frozenOut_pc  = np.zeros((numb, 128))\n",
    "weight_pc     = np.zeros((numb, 80))\n",
    "bias_pc       = np.zeros((numb,8))\n",
    "preSoftmax_pc = np.zeros((numb,8))\n",
    "softmax_pc    = np.zeros((numb,8))\n",
    "\n",
    "weight_letter_b = np.zeros((128, numb))\n",
    "\n",
    "\n",
    "\n",
    "selected_w = [46,13,107,3,57,65,127,81,89,70,\n",
    "                143,239,142,158,207,189,172,230,156,208,\n",
    "                374,359,375,371,303,298,350,257,349,333,\n",
    "                402,502,485,461,489,479,454,508,485,480,\n",
    "                527,565,614,517,528,613,625,623,587,521,\n",
    "                712,742,685,746,759,747,754,702,653,640,\n",
    "                775,809,798,853,804,840,828,788,890,819,\n",
    "                906,1019,911,1005,1016,953,1016,987,961,1023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL(model, x, y_true):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with OL METHOD - STOCHASTICH\\n')\n",
    "   \n",
    "    cntr = 1\n",
    "    learn_rate  = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "                \n",
    "    # Cycle over all samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label)\n",
    "               \n",
    "        # PPREDICTION\n",
    "        y_ML   = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])           \n",
    "        \n",
    "        temp = np.copy(np.array(np.matmul(y_ML, model.W) + model.b))\n",
    "        temp = temp[0]\n",
    "        \n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "        \n",
    "        for j in range(0,model.W.shape[0]):\n",
    "            # Update weights\n",
    "            deltaW = np.multiply(cost, y_ML[0,j])\n",
    "            dW     = np.multiply(deltaW, learn_rate)\n",
    "            model.W_counter[j,:] += dW\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "        # Update biases\n",
    "        db      = np.multiply(cost, learn_rate)\n",
    "        model.b = model.b-db\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # TO BE REMOVED LATER **********\n",
    "        # SAVE THE WEIGHTS IN A MATRIX\n",
    "        if(i<numb):\n",
    "            \n",
    "            frozenOut_pc[i,:] = y_ML[0,:]\n",
    "                    \n",
    "            for q in range(0, 8):\n",
    "                if(q<model.W.shape[1]):\n",
    "                    bias_pc[i,q]       = np.copy(model.b[q])\n",
    "                    softmax_pc[i,q]    = np.copy(y_pred[q])\n",
    "                    preSoftmax_pc[i,q] = np.copy(temp[q])\n",
    "\n",
    "            for q in range(0, 80):\n",
    "                if(int(selected_w[q]/128) < model.W.shape[1] ):\n",
    "                    weight_pc[i,q] = np.copy(model.W[selected_w[q]%128, int(selected_w[q]/128)])\n",
    "        # *********************************\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL MINI BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL_miniBatch(model, x, y_true, batch_size):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with OL METHOD - MINI BATCH\\n')\n",
    "    \n",
    "    cntr=1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "    sum_gradW = np.zeros([model.W.shape[0], 8])\n",
    "    sum_gradB = np.zeros([1, 8])\n",
    "            \n",
    "    # Cycle over all samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label)        \n",
    "                \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "        \n",
    "        if(i%batch_size==0):\n",
    "                model.W = model.W - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,:w]\n",
    "                model.b = model.b - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,:w]\n",
    "\n",
    "                sum_gradW = np.zeros([h, 8])  #reset each batch  \n",
    "                sum_gradB = np.zeros([1, 8])  #reset each batch   \n",
    "        \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "\n",
    "        for j in range(0,h): \n",
    "            # Update weights\n",
    "            tmp = np.multiply(cost, y_ML[0,j]) \n",
    "            deltaW = np.zeros([1,8])\n",
    "            deltaW[0,:w] = tmp  \n",
    "            sum_gradW[j,:] += deltaW[0,:]\n",
    "\n",
    "        # Update biases\n",
    "        deltaB = np.zeros([1,8])\n",
    "        deltaB[0,:w] = cost\n",
    "        sum_gradB += deltaB\n",
    "\n",
    "        # If last iteration\n",
    "        if(i==tot_samples-1):\n",
    "            model.W = model.W - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,:w]\n",
    "            model.b = model.b - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,:w]\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def trainOneEpoch_OL_v2(model, x, y_true):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with CWR METHOD - STOCASTICH \\n')\n",
    "    \n",
    "    cntr=1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "                \n",
    "    # Cycle over every sample\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "                \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "        cost[0] = 0\n",
    "        cost[1] = 0\n",
    "        cost[2] = 0\n",
    "        cost[3] = 0\n",
    "\n",
    "        for j in range(0,model.W.shape[0]):\n",
    "            # Update weights\n",
    "            deltaW = np.multiply(cost, y_ML[0,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW[:]\n",
    "\n",
    "        # Update biases\n",
    "        db = np.multiply(cost, learn_rate)\n",
    "        model.b[5:] = model.b[5:]-db[5:]\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2 MINI BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL_v2_miniBatch(model, x, y_true, batch_size):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with CWR - MINI BATCH \\n ')  \n",
    "\n",
    "    cntr=1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "    sum_gradW = np.zeros([model.W.shape[0], 8])\n",
    "    sum_gradB = np.zeros([1, 8])\n",
    "           \n",
    "    # Cycle over all input samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "                \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "        \n",
    "        # If beginning of batch\n",
    "        if(i%batch_size==0):\n",
    "                model.W[:,5:] = model.W[:,5:] - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,5:w]\n",
    "                model.b[5:]   = model.b[5:]   - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,5:w]\n",
    "                sum_gradW = np.zeros([h, 8])  # reset\n",
    "                sum_gradB = np.zeros([1, 8])  # reset\n",
    "            \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "\n",
    "        for j in range(0,h):  \n",
    "            # Update weights\n",
    "            tmp = np.multiply(cost, y_ML[0,j]) \n",
    "            deltaW = np.zeros([1,8])\n",
    "            deltaW[0,:tmp.shape[0]] = tmp  \n",
    "            sum_gradW[j,:] += deltaW[0,:]\n",
    "\n",
    "        # Update biases\n",
    "        deltaB = np.zeros([1,8])\n",
    "        deltaB[0,:cost.shape[0]] = cost\n",
    "        sum_gradB += deltaB\n",
    "\n",
    "        # If last iteration\n",
    "        if(i==tot_samples-1):\n",
    "            model.W[:,5:] = model.W[:,5:] - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,5:w]\n",
    "            model.b[5:]   = model.b[5:]   - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,5:w]\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpochOL_LWF(model, x, y_true):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with LWF - STOCHASTIC\\n ') \n",
    "    \n",
    "    lam  = 0\n",
    "    cntr = 1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "    y_LWF = np.zeros([1, 8])    # Define container for LWF\n",
    "\n",
    "    # DEFINE ORIGINAL WEIGHTS AND BIASES\n",
    "    LWF_w = model.W\n",
    "    LWF_b = model.b\n",
    "         \n",
    "    # Cycle over every sample\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "                \n",
    "        w = model.W.shape[1]\n",
    "        h = model.W.shape[0]\n",
    "        \n",
    "        # va da 1 a 0\n",
    "        lam = 100/(100+cntr)    #1-i/493    #  1/(20+cntr)        #\n",
    "             \n",
    "        # PREDICTIONS\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "        \n",
    "        mat_prod = np.array(np.matmul(y_ML, LWF_w) + LWF_b)\n",
    "        y_LWF[0,:5] = my_Softmax(mat_prod)   \n",
    "          \n",
    "        \n",
    "        # BACKPROPAGATION        \n",
    "        cost_norm = y_pred-y_true_soft\n",
    "        cost_LWF  = y_pred-y_LWF[0,:w]\n",
    "\n",
    "        for j in range(0,h):\n",
    "            # Update weights\n",
    "            deltaW_norm = np.multiply(cost_norm,1-lam)\n",
    "            deltaW_LWF  = np.multiply(cost_LWF, lam)\n",
    "            deltaW      = np.multiply(deltaW_norm+deltaW_LWF, y_ML[0,j])\n",
    "            dW          = np.multiply(deltaW, learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "        # Update biases\n",
    "        db_norm = np.multiply(cost_norm, 1-lam)\n",
    "        db_LWF  = np.multiply(cost_LWF, lam)\n",
    "        db      = np.multiply(db_norm+db_LWF, learn_rate)\n",
    "        model.b = model.b-db\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF MINI BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpochOL_LWF_v2(model, x, y_true, batch_size):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with LWF - MINI BATCH\\n')\n",
    "    \n",
    "    lam  = 0\n",
    "    cntr = 1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "        \n",
    "    w = model.W.shape[1]\n",
    "        \n",
    "    LWF_w = np.zeros([model.W.shape[0], 8]) # alocate\n",
    "    LWF_b = np.zeros([1, 8]) \n",
    "    y_LWF = np.zeros([1, 8])\n",
    "    \n",
    "    LWF_w[:,:w] = model.W   # copy from TF\n",
    "    LWF_b[0,:w] = model.b\n",
    "    \n",
    "    # For every sample in the dataset given\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "                \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "        \n",
    "        # END OF BATCH\n",
    "        if(i%batch_size==0 and i!=0):            \n",
    "            LWF_w[:,:w] = np.copy(model.W)    # update the LWF w matrix\n",
    "            LWF_b[0,:w] = np.copy(model.b)    # update the LWF b matrix\n",
    "                \n",
    "        #lam = 1/(2+cntr)\n",
    "        if(cntr<batch_size):\n",
    "            lam = 1\n",
    "        else:\n",
    "            lam = batch_size/cntr  #(cntr/493)   va da 0 a 1\n",
    "    \n",
    "        # PREDICTION - Frozen + OL\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])  \n",
    "        \n",
    "        mat_prod = np.array(np.matmul(y_ML, LWF_w[:,:w]) + LWF_b[0,:w])\n",
    "        y_LWF[0,:w] = my_Softmax(mat_prod)        \n",
    "\n",
    "\n",
    "         \n",
    "        # ---- BACKPROPAGATION | MINI BATCH + LWF        \n",
    "        cost_norm = y_pred-y_true_soft\n",
    "        cost_LWF  = y_pred-y_LWF[0,:w]\n",
    "        \n",
    "        lam_cost_norm = np.multiply(cost_norm, 1-lam)\n",
    "        lam_cost_LWF  = np.multiply(cost_LWF,  lam)\n",
    "\n",
    "        for j in range(0,h):\n",
    "\n",
    "            # Update weights\n",
    "            deltaW = np.multiply(lam_cost_norm+lam_cost_LWF, y_ML[0,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW          \n",
    "            \n",
    "        # Update biases \n",
    "        db = np.multiply(lam_cost_norm+lam_cost_LWF, learn_rate)\n",
    "        model.b = model.b-db   \n",
    "                        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_CWR(model, x, y_true, batch_size):\n",
    "        \n",
    "    print('**********************************\\nPerforming training CWR \\n ')  \n",
    "\n",
    "    cntr=1\n",
    "    learn_rate = model.l_rate\n",
    "    tot_samples = x.shape[0]\n",
    "    TW = np.zeros([model.W.shape[0], 8])\n",
    "    TB = np.zeros([1, 8])\n",
    "    found_lett = np.zeros([1,8])\n",
    "               \n",
    "    # Cycle over all input samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        current_label = y_true[i]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "        \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "                \n",
    "        # If beginning of batch\n",
    "        if(i%batch_size==0 and i!=0): \n",
    "            for k in range(0, w):\n",
    "                if(found_lett[0,k]!=0):\n",
    "                    tempW = np.multiply(model.W[:,k], found_lett[0,k])\n",
    "                    tempB = np.multiply(model.b[k]  , found_lett[0,k])\n",
    "                    model.W[:,k] = np.multiply(tempW+TW[:,k], 1/(found_lett[0,k]+1))\n",
    "                    model.b[k]   = np.multiply(tempB+TB[0,k], 1/(found_lett[0,k]+1))\n",
    "                    \n",
    "            TW[:h,:w]  = np.copy(model.W)\n",
    "            TB[0,:w]   = np.copy(model.b)\n",
    "            found_lett = np.zeros([1,8])  # reset\n",
    "        elif(i==0):\n",
    "            TW = np.zeros([h, 8])         # reset  \n",
    "            TB = np.zeros([1, 8])         # reset  \n",
    "            found_lett = np.zeros([1,8])  # reset\n",
    "                \n",
    "        found_lett[0,np.argmax(y_true_soft[i,:])] += 1  # update the letter counter\n",
    "            \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        mat_prod = np.array(np.matmul(y_ML, TW) + TB)\n",
    "        y_pred = my_Softmax(mat_prod)        \n",
    "\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred[:w]-y_true_soft\n",
    "\n",
    "        # Update weights\n",
    "        for j in range(0,h):\n",
    "            deltaW = np.multiply(cost, y_ML[0,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            TW[j,:w] = TW[j,:w] - dW\n",
    "\n",
    "        # Update biases\n",
    "        db = np.multiply(cost, learn_rate)\n",
    "        TB[0,:w] = TB[0,:w]-db\n",
    "\n",
    "        # If last iteration\n",
    "        if(i==tot_samples-1):\n",
    "            for k in range(5, w):\n",
    "                if(found_lett[0,k]!=0):\n",
    "                    tempW = np.multiply(model.W[:,k], found_lett[0,k])\n",
    "                    tempB = np.multiply(model.b[k]  , found_lett[0,k])\n",
    "                    model.W[:,k] = np.multiply(tempW+TW[:,k], 1/(found_lett[0,k]+1))\n",
    "                    model.b[k]   = np.multiply(tempB+TB[0,k], 1/(found_lett[0,k]+1))\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dfine some important values for the training and then actually train each single OL layer witha  different method. The 0 and 1 below are used in order to activate or deactivate the training and the following plots of a specific method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_OL = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE WHICH TRAINING AND PLOTS TO SHOW\n",
    "\n",
    "KERAS      = 1\n",
    "OL_vowels  = 0\n",
    "OL         = 1\n",
    "OL_mini    = 0\n",
    "LWF        = 0\n",
    "LWF_mini   = 0\n",
    "OL_v2      = 0\n",
    "OL_v2_mini = 0\n",
    "CWR        = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define KERAS model (just create the class, actually don't train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(KERAS==1):\n",
    "    Model_KERAS = Custom_Layer(model)\n",
    "    Model_KERAS.title = 'KERAS'\n",
    "    Model_KERAS.filename = 'KERAS'\n",
    "    Model_KERAS.label = ['A','E','I','O','U','B','R','M']\n",
    "    # DO NOT PERFORM TRAINING, KEEP IT AS IT IS, IT'S THE ORIGINAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL only on vowels (just to see if the OL model makes the keras better or worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(OL_vowels==1):\n",
    "    Model_OL_vowels = Custom_Layer(model)\n",
    "    Model_OL_vowels.title = 'VOWELS'\n",
    "    Model_OL_vowels.filename = 'OL_vowels'\n",
    "    Model_OL_vowels.l_rate = 0.000005\n",
    "    \n",
    "    trainOneEpoch_OL(Model_OL_vowels, OL_data_train_vow, OL_label_train_vow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Performing training with OL METHOD - STOCHASTICH\n",
      "\n",
      "    Currently at 0.97% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mB\u001b[0m \n",
      "\n",
      "    Currently at 1.94% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mR\u001b[0m \n",
      "\n",
      "    Currently at 2.42% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mM\u001b[0m \n",
      "\n",
      "    Currently at 100.0% of dataset"
     ]
    }
   ],
   "source": [
    "if(OL==1):\n",
    "    Model_OL_all_mixed = Custom_Layer(model)\n",
    "    Model_OL_all_mixed.title = 'OL' \n",
    "    Model_OL_all_mixed.filename = 'OL'\n",
    "    Model_OL_all_mixed.l_rate = 0.00005 # 0.000005 true value jupyter\n",
    "\n",
    "    trainOneEpoch_OL(Model_OL_all_mixed, data_train, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights, biases and the out of frozen layer from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_stm      = myDebug.debug_loadBiasSMT()\n",
    "weight_stm    = myDebug.debug_loadWeightsSTM()\n",
    "frozenOut_stm = myDebug.debug_loadFrozenOutSMT()\n",
    "softmax_stm   = myDebug.debug_loadSoftmaxSMT()\n",
    "preSoftmax_stm= myDebug.debug_loadPreSoftmaxSMT()\n",
    "\n",
    "max_dim = data_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dato che alla prima iterazione l'output della lettera B è gia diverso da 0 c'è qualcosa di sbagliato nei weight di B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.18150768,  0.08938491, -0.18608522,  0.00855171, -0.20496836,\n",
       "       -0.13263984, -0.11427391,  0.0067899 , -0.04600576,  0.06259806,\n",
       "       -0.17774622,  0.20866856, -0.08623615,  0.15338975,  0.06580155,\n",
       "       -0.14114515, -0.0773444 , -0.07616314, -0.15799688,  0.10825997,\n",
       "        0.18257415, -0.09591679, -0.04218199,  0.0850042 , -0.06225193,\n",
       "        0.03240436,  0.00809215,  0.09971575,  0.03389216,  0.16350794,\n",
       "        0.07712426, -0.09988065,  0.19423795,  0.06641196, -0.14970118,\n",
       "       -0.00242217,  0.19173016, -0.1505501 , -0.14120479,  0.09060922,\n",
       "       -0.12305073,  0.07586488,  0.00695998,  0.2076253 , -0.18489452,\n",
       "        0.06615217, -0.02116856, -0.15732081,  0.05944382, -0.1359596 ,\n",
       "        0.0045421 , -0.12043767,  0.01178388,  0.09726822,  0.10742972,\n",
       "        0.01626321, -0.10799398,  0.18720427, -0.04222888, -0.05897719,\n",
       "       -0.18580498,  0.00622943,  0.14662448, -0.14586577,  0.22263236,\n",
       "       -0.13876693, -0.1427201 , -0.04754504, -0.1980899 ,  0.11662541,\n",
       "        0.06082182,  0.00351247,  0.1489184 , -0.17637366,  0.12651971,\n",
       "       -0.19909295,  0.0169177 ,  0.07956323, -0.08019288,  0.19565085,\n",
       "        0.01683383,  0.03841801, -0.13813546, -0.03738134,  0.10971349,\n",
       "        0.07611035, -0.10436986, -0.07624139,  0.18885925, -0.05588534,\n",
       "        0.13521582,  0.10620876,  0.01246906, -0.1663256 ,  0.06750153,\n",
       "       -0.17112765,  0.20460644, -0.1828549 , -0.11388695,  0.2126195 ,\n",
       "        0.11640836,  0.00921892,  0.04200497,  0.0932034 , -0.19385979,\n",
       "        0.07400116, -0.05336886, -0.04404627,  0.09321794, -0.20833279,\n",
       "        0.16183928,  0.14595917, -0.21101741, -0.08179875, -0.00356172,\n",
       "        0.05651382,  0.09046339, -0.16457835, -0.08008265, -0.1946936 ,\n",
       "       -0.12719   , -0.20033161,  0.03874925, -0.14574926, -0.0739996 ,\n",
       "       -0.06319712,  0.13971111, -0.00220605], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_KERAS.W[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.136711008"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_stm[0,65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels is: A\n",
      "The max difference is 0.0002134921875267537\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias pc  -0.0006639 -0.0006874 -0.0004739 0.0029768 0.0010335 0.0000000 0.0000000 0.0000000\n",
      "bias stm -0.0006639 -0.0006874 -0.0004739 0.0029768 0.0010335 0.0000000 0.0000000 0.0000000\n",
      "\n",
      "weight pc  -0.1387669 0.2102083 0.0005138 -0.0532225 -0.0373260 -0.0002955 0.0000000 0.0000000\n",
      "weight stm -0.1387669 0.2102083 0.0005138 -0.0532225 -0.0373260 -0.0002955 -0.1460544 0.1254706\n",
      "\n",
      "pre softmax pc  1074.3494018 -427.2478627 -456.5349751 -234.1416650 -630.2025131 65.9032766 0.0000000 0.0000000\n",
      "pre softmax stm 1074.3493120 -427.2479360 -456.5357760 -234.1416800 -630.2024960 65.9033120 0.0000000 0.0000000\n",
      "\n",
      "softmax pc  1.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n",
      "softmax stm 1.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n"
     ]
    }
   ],
   "source": [
    "test_n = 6\n",
    "\n",
    "print(f'labels is: {label_train[test_n]}')\n",
    "myDebug.plot_frozenDifference(test_n, frozenOut_pc, frozenOut_stm)\n",
    "print(f'bias pc  {bias_pc[test_n,0]:.7f} {bias_pc[test_n,1]:.7f} {bias_pc[test_n,2]:.7f} {bias_pc[test_n,3]:.7f} {bias_pc[test_n,4]:.7f} {bias_pc[test_n,5]:.7f} {bias_pc[test_n,6]:.7f} {bias_pc[test_n,7]:.7f}')\n",
    "print(f'bias stm {bias_stm[test_n,0]:.7f} {bias_stm[test_n,1]:.7f} {bias_stm[test_n,2]:.7f} {bias_stm[test_n,3]:.7f} {bias_stm[test_n,4]:.7f} {bias_stm[test_n,5]:.7f} {bias_stm[test_n,6]:.7f} {bias_stm[test_n,7]:.7f}')\n",
    "print()\n",
    "print(f'weight pc  {weight_pc[test_n,5]:.7f} {weight_pc[test_n,15]:.7f} {weight_pc[test_n,25]:.7f} {weight_pc[test_n,35]:.7f} {weight_pc[test_n,45]:.7f} {weight_pc[test_n,55]:.7f} {weight_pc[test_n,65]:.7f} {weight_pc[test_n,75]:.7f}')\n",
    "print(f'weight stm {weight_stm[test_n,5]:.7f} {weight_stm[test_n,15]:.7f} {weight_stm[test_n,25]:.7f} {weight_stm[test_n,35]:.7f} {weight_stm[test_n,45]:.7f} {weight_stm[test_n,55]:.7f} {weight_stm[test_n,65]:.7f} {weight_stm[test_n,75]:.7f}')\n",
    "print()\n",
    "print(f'pre softmax pc  {preSoftmax_pc[test_n,0]:.7f} {preSoftmax_pc[test_n,1]:.7f} {preSoftmax_pc[test_n,2]:.7f} {preSoftmax_pc[test_n,3]:.7f} {preSoftmax_pc[test_n,4]:.7f} {preSoftmax_pc[test_n,5]:.7f} {preSoftmax_pc[test_n,6]:.7f} {preSoftmax_pc[test_n,7]:.7f}')\n",
    "print(f'pre softmax stm {preSoftmax_stm[test_n,0]:.7f} {preSoftmax_stm[test_n,1]:.7f} {preSoftmax_stm[test_n,2]:.7f} {preSoftmax_stm[test_n,3]:.7f} {preSoftmax_stm[test_n,4]:.7f} {preSoftmax_stm[test_n,5]:.7f} {preSoftmax_stm[test_n,6]:.7f} {preSoftmax_stm[test_n,7]:.7f}')\n",
    "print()\n",
    "print(f'softmax pc  {softmax_pc[test_n,0]:.7f} {softmax_pc[test_n,1]:.7f} {softmax_pc[test_n,2]:.7f} {softmax_pc[test_n,3]:.7f} {softmax_pc[test_n,4]:.7f} {softmax_pc[test_n,5]:.7f} {softmax_pc[test_n,6]:.7f} {softmax_pc[test_n,7]:.7f}')\n",
    "print(f'softmax stm {softmax_stm[test_n,0]:.7f} {softmax_stm[test_n,1]:.7f} {softmax_stm[test_n,2]:.7f} {softmax_stm[test_n,3]:.7f} {softmax_stm[test_n,4]:.7f} {softmax_stm[test_n,5]:.7f} {softmax_stm[test_n,6]:.7f} {softmax_stm[test_n,7]:.7f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 413 is out of bounds for axis 0 with size 413",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19116/810801454.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m770\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreSoftmax_pc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpreSoftmax_stm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0miteratore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 413 is out of bounds for axis 0 with size 413"
     ]
    }
   ],
   "source": [
    "# search where is the first big difference\n",
    "iteratore = np.zeros(8)\n",
    "for j in range(0,8):\n",
    "    for i in range(0, 770):\n",
    "        diff = preSoftmax_pc[i,j] - preSoftmax_stm[i,j]\n",
    "        if(diff>1):\n",
    "            iteratore[j]=i\n",
    "            break\n",
    "        \n",
    "print(f'The big difference in the pre softmax is found at iter: {iteratore}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search where is the first big difference\n",
    "iteratore = np.zeros(8)\n",
    "for j in range(0,8):\n",
    "    for i in range(0, max_dim):\n",
    "        diff = softmax_pc[i,j] - softmax_stm[i,j]\n",
    "        if(diff>0.0001):\n",
    "            iteratore[j]=i\n",
    "            break\n",
    "        \n",
    "print(f'The big difference in the softmax is found at iter:     {iteratore}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myDebug.debug_plotHistoryPreSoftmax(0, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(1, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(2, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(3, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(4, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(5, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(6, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(7, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confront SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myDebug.debug_plotHistorySoftmax(0, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(1, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(2, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(3, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(4, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(5, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(6, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(7, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confront BIASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_prova = 767\n",
    "myDebug.debug_confrontBias(n_prova, bias_stm, bias_pc, Model_OL_all_mixed.label) # max 771\n",
    "\n",
    "print(f'correct label is: {label_train[n_prova]}')\n",
    "for i in range (0, len(Model_OL_all_mixed.label)):\n",
    "    print(f'    label: {Model_OL_all_mixed.label[i]}   pred: {softmax_pc[n_prova,i]:.11f}   pre soft: {preSoftmax_pc[n_prova,i]:.10f}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bias_stm[100,5]-bias_pc[100,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myDebug.debug_plotHistoryBias(0, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(1, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(2, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(3, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(4, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(5, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(6, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(7, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confront weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_num  = list(range(60,70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_prova = 100\n",
    "\n",
    "myDebug.debug_confrontWeights(n_prova, weight_stm, weight_pc, weight_num, selected_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "myDebug.debug_plotHistoryWeight(10, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(5, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(22, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(75, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(66, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(47, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(33, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(64, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(54, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(71, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(41, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(23, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(35, weight_stm, weight_pc, max_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FROZEN OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 0\n",
    "itr_1   = 0\n",
    "itr_2   = 0\n",
    "for i in range(0,max_dim):\n",
    "    diff = frozenOut_pc[i,:] - frozenOut_stm[i,:]\n",
    "    for j in range(0, len(diff)):\n",
    "        diff[j] = np.abs(diff[j])\n",
    "    if(max(diff)> max_val):\n",
    "        max_val = max(diff)\n",
    "        itr_1 = i\n",
    "        itr_2 = np.argmax(diff)\n",
    "        \n",
    "print(f'The max values of difference of all {frozenOut_stm.shape[0]} iteration is: {S_BOLD}{max_val:.11f}{E_BOLD}')\n",
    "print(f'The max difference is found at the iteration number: {S_BOLD}{itr_1}{E_BOLD}')\n",
    "print(f'At the position in the frozen output: {S_BOLD}{itr_2}/128{E_BOLD}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDebug.plot_frozenDifference(2, frozenOut_pc, frozenOut_stm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_container = np.zeros(frozenOut_stm.shape)\n",
    "\n",
    "for j in range(0, frozenOut_stm.shape[0]):\n",
    "    for i in range(0, frozenOut_stm.shape[1]):\n",
    "        max_val = max(frozenOut_pc[j,i], frozenOut_pc[j,i])\n",
    "        if(max_val != 0):\n",
    "            diff = frozenOut_pc[j,i]-frozenOut_stm[j,i]\n",
    "            new_container[j,i] = diff/max_val\n",
    "          \n",
    "        \n",
    "max_val =0\n",
    "for j in range(0, frozenOut_stm.shape[0]):\n",
    "    if(max_val < max(new_container[i,:])):\n",
    "        max_val = max(new_container[i,:])\n",
    "        \n",
    "print(f'The max percentage error committed between stm and laptop is of: {max_val:.11f}')\n",
    "print(f'Percentage at position, 237, 79 is {new_container[237,79]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END DEBUG *********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(OL_mini==1):\n",
    "    Model_OL_mini = Custom_Layer(model)\n",
    "    Model_OL_mini.title = 'OL + mini batch'\n",
    "    Model_OL_mini.filename = 'OL_batches'\n",
    "    Model_OL_mini.l_rate = 0.0001\n",
    "\n",
    "    trainOneEpoch_OL_miniBatch(Model_OL_mini, data_train, label_train, batch_size_OL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(LWF==1):\n",
    "    Model_LWF_1 = Custom_Layer(model)\n",
    "    Model_LWF_1.title = 'LWF'\n",
    "    Model_LWF_1.filename = 'LWF'   \n",
    "    Model_LWF_1.l_rate = 0.0017 #0.001\n",
    "\n",
    "    trainOneEpochOL_LWF(Model_LWF_1, data_train, label_train)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LWF + mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(LWF_mini==1):\n",
    "    Model_LWF_2 = Custom_Layer(model)\n",
    "    Model_LWF_2.title = 'LWF + mini batch'\n",
    "    Model_LWF_2.filename = 'LWF_batches'\n",
    "    Model_LWF_2.l_rate = 0.000001\n",
    "\n",
    "    trainOneEpochOL_LWF_v2(Model_LWF_2, data_train, label_train, batch_size_OL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(OL_v2==1):\n",
    "    Model_OL_v2 = Custom_Layer(model)\n",
    "    Model_OL_v2.title = 'OL v2' \n",
    "    Model_OL_v2.filename = 'OL_v2'\n",
    "    Model_OL_v2.l_rate = 0.00005\n",
    "\n",
    "    trainOneEpoch_OL_v2(Model_OL_v2, data_train, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL v2 + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(OL_v2_mini==1):\n",
    "    Model_OL_v2_miniBatch = Custom_Layer(model)\n",
    "    Model_OL_v2_miniBatch.title = 'OL v2 + mini batch'\n",
    "    Model_OL_v2_miniBatch.filename = 'OL_v2_batches'\n",
    "    Model_OL_v2_miniBatch.l_rate = 0.001\n",
    "\n",
    "    trainOneEpoch_OL_v2_miniBatch(Model_OL_v2_miniBatch, data_train, label_train, batch_size_OL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with CWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(CWR==1):\n",
    "    Model_CWR = Custom_Layer(model) \n",
    "    Model_CWR.title = 'CWR'\n",
    "    Model_CWR.filename = 'CWR'\n",
    "    Model_CWR.l_rate = 0.0009   # 0.00005\n",
    "\n",
    "    trainOneEpoch_CWR(Model_CWR, data_train, label_train, batch_size_OL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATION PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERAS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(KERAS==1):\n",
    "    myTest.test_OLlayer(Model_KERAS, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_KERAS)   \n",
    "    myMatrix.plot_confMatrix(Model_KERAS)\n",
    "    myTable.table_params(Model_KERAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(OL_vowels==1):\n",
    "    myTest.test_OLlayer(Model_OL_vowels, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_OL_vowels)\n",
    "    myMatrix.plot_confMatrix(Model_OL_vowels)\n",
    "    myTable.table_params(Model_OL_vowels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(OL==1):\n",
    "    myTest.test_OLlayer(Model_OL_all_mixed, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_OL_all_mixed)\n",
    "    myMatrix.plot_confMatrix(Model_OL_all_mixed)\n",
    "    myTable.table_params(Model_OL_all_mixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(OL_mini==1):\n",
    "    myTest.test_OLlayer(Model_OL_mini, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_OL_mini)\n",
    "    myMatrix.plot_confMatrix(Model_OL_mini)\n",
    "    myTable.table_params(Model_OL_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(LWF==1):\n",
    "    myTest.test_OLlayer(Model_LWF_1, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_LWF_1)\n",
    "    myMatrix.plot_confMatrix(Model_LWF_1)\n",
    "    myTable.table_params(Model_LWF_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LWF_mini==1):\n",
    "    myTest.test_OLlayer(Model_LWF_2, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_LWF_2)\n",
    "    myMatrix.plot_confMatrix(Model_LWF_2)\n",
    "    myTable.table_params(Model_LWF_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(OL_v2==1):\n",
    "    myTest.test_OLlayer(Model_OL_v2, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_OL_v2)\n",
    "    myMatrix.plot_confMatrix(Model_OL_v2)\n",
    "    myTable.table_params(Model_OL_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2 + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(OL_v2_mini==1):\n",
    "    myTest.test_OLlayer(Model_OL_v2_miniBatch, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_OL_v2_miniBatch)\n",
    "    myMatrix.plot_confMatrix(Model_OL_v2_miniBatch)\n",
    "    myTable.table_params(Model_OL_v2_miniBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(CWR==1):\n",
    "    myTest.test_OLlayer(Model_CWR, data_test, label_test)\n",
    "    myBar.plot_barChart(Model_CWR)\n",
    "    myMatrix.plot_confMatrix(Model_CWR)\n",
    "    myTable.table_params(Model_CWR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All bar plots together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myBar.plot_barChart_All()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot is a recap of all the methods trained. Note that it will be displayed only if all the training have been performed in this runtime. \n",
    "The table contains some additiona information and not only the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(KERAS and OL_vowels and OL and OL_mini and LWF and LWF_mini and OL_v2 and OL_v2_mini and CWR):\n",
    "    \n",
    "    myTable.table_simulationResult(Model_KERAS, Model_OL_vowels, Model_OL_all_mixed, Model_OL_mini, \n",
    "               Model_LWF_1, Model_LWF_2, Model_OL_v2, Model_OL_v2_miniBatch, Model_CWR)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below is used for storing the result of the simulation formermed in this runtime. This is used for another plotting function that will display the average accuracy of each method across multiple runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down in txt files all the results across 10 or so simulations          \n",
    "WRITE_SIMU_RES = 0\n",
    "                \n",
    "if(WRITE_SIMU_RES==1):\n",
    "    myWrite.save_simulationResult('Keras',     Model_KERAS)\n",
    "    myWrite.save_simulationResult('OL_vowels', Model_OL_vowels)\n",
    "    myWrite.save_simulationResult('OL',        Model_OL_all_mixed)\n",
    "    myWrite.save_simulationResult('OL_mini',   Model_OL_mini)\n",
    "    myWrite.save_simulationResult('LWF',       Model_LWF_1)\n",
    "    myWrite.save_simulationResult('LWF_mini',  Model_LWF_2)\n",
    "    myWrite.save_simulationResult('OL_v2',     Model_OL_v2)\n",
    "    myWrite.save_simulationResult('OL_v2_min', Model_OL_v2_miniBatch)\n",
    "    myWrite.save_simulationResult('CWR',       Model_CWR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the average accuracy over several runtimes\n",
    "\n",
    "#myBar.plot_barChart_SimuRes(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below plots some pie charts that shows how the dataset are composed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ENABLE_PLOTS = 0\n",
    "if(ENABLE_PLOTS==1):\n",
    "    \n",
    "    \n",
    "    vowels_data_tf, vowels_label_tf = myParse.loadDataFromTxt('vowels_TF')\n",
    "    TF_data_train, _, TF_data_test, _ = myParse.parseTrainTest(vowels_data_tf, vowels_label_tf, 0.7)\n",
    "\n",
    "\n",
    "    # Plot of the pie chart of the dataset TF e OL\n",
    "    dataset_shapes = np.zeros(8)\n",
    "    label_vow = ['A','E','I','O','U']\n",
    "\n",
    "    for i in range(0,vowels_data.shape[0]):\n",
    "        for j in range(0,len(label_vow)):\n",
    "            if(label_vow[j] == vowels_label[i]):\n",
    "                dataset_shapes[j] += 1\n",
    "                break\n",
    "    for i in range(0,vowels_data_tf.shape[0]):\n",
    "        for j in range(0,len(label_vow)):\n",
    "            if(label_vow[j] == vowels_label_tf[i]):\n",
    "                dataset_shapes[j] += 1\n",
    "                break\n",
    "\n",
    "    dataset_shapes[5] = B_data.shape[0]\n",
    "    dataset_shapes[6] = R_data.shape[0]\n",
    "    dataset_shapes[7] = M_data.shape[0]\n",
    "    myPie.plot_pieChart_datasetAll(dataset_shapes)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    dataset_shapes = np.zeros([8])\n",
    "    dataset_shapes[0] = OL_data_train_vow.shape[0]\n",
    "    dataset_shapes[1] = OL_data_test_vow.shape[0]\n",
    "    dataset_shapes[2] = B_train_data.shape[0]\n",
    "    dataset_shapes[3] = B_test_data.shape[0]\n",
    "    dataset_shapes[4] = R_train_data.shape[0]\n",
    "    dataset_shapes[5] = R_test_data.shape[0]\n",
    "    dataset_shapes[6] = M_train_data.shape[0]\n",
    "    dataset_shapes[7] = M_test_data.shape[0]\n",
    "    # Plot of the pie chart of the dataset OL\n",
    "    myPie.plot_pieChart_DatasetOL(dataset_shapes)\n",
    "\n",
    "    # Plot of the pie chart of the dataset TF\n",
    "    myPie.plot_pieChart_DatasetTF(TF_data_train.shape[0],TF_data_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
