{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import myLib_barChart as myBar\n",
    "import myLib_confMatrix as myMatrix\n",
    "import myLib_parseData as myParse\n",
    "import myLib_pieChart as myPie\n",
    "import myLib_table as myTable\n",
    "import myLib_testModel as myTest\n",
    "from myLib_testModel import letterToSoftmax\n",
    "import myLib_writeFile as myWrite\n",
    "import myLib_debugFiles as myDebug\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEfinition of string values for the bold print\n",
    "S_BOLD = '\\033[1m'\n",
    "E_BOLD = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET AND PREPARE TRAIN - TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the functions loadDataFromTxt and parseTrainTest are called. These allow to load the dataset from the txt files into matrices and then separate them in smaller matrices for testing and training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['A' 'E' 'I' 'O' 'U']\n",
      "\n",
      "Raw shape        -> (103600, 5)\n",
      "Tot samples      -> 518\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (361, 600)\n",
      "Test data shape   -> (155, 600)\n"
     ]
    }
   ],
   "source": [
    "vowels_data, vowels_label = myParse.loadDataFromTxt('vowels_OL')\n",
    "OL_data_train_vow, OL_label_train_vow, OL_data_test_vow, OL_label_test_vow = myParse.parseTrainTest(vowels_data, vowels_label, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['B']\n",
      "\n",
      "Raw shape        -> (39400, 5)\n",
      "Tot samples      -> 197\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (136, 600)\n",
      "Test data shape   -> (59, 600)\n"
     ]
    }
   ],
   "source": [
    "B_data, B_label = myParse.loadDataFromTxt('B_dataset')\n",
    "B_train_data, B_train_label, B_test_data, B_test_label = myParse.parseTrainTest(B_data, B_label, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['M']\n",
      "\n",
      "Raw shape        -> (39000, 5)\n",
      "Tot samples      -> 195\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (135, 600)\n",
      "Test data shape   -> (58, 600)\n"
     ]
    }
   ],
   "source": [
    "M_data, M_label = myParse.loadDataFromTxt('M_dataset')\n",
    "M_train_data, M_train_label, M_test_data, M_test_label = myParse.parseTrainTest(M_data, M_label, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['R']\n",
      "\n",
      "Raw shape        -> (39000, 5)\n",
      "Tot samples      -> 195\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (135, 600)\n",
      "Test data shape   -> (58, 600)\n"
     ]
    }
   ],
   "source": [
    "R_data, R_label = myParse.loadDataFromTxt('R_dataset')\n",
    "R_train_data, R_train_label, R_test_data, R_test_label = myParse.parseTrainTest(R_data, R_label, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a dataset of all letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this smaller section all the previous matrices are stacked together and then shuffled in order to create two big matrices that contain all the letters for training and testing. The training dataset is also shuffled, in order to shuffle it differently change the seed value inside the function myParse.shuffleDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_all has shape  (767, 600)\n",
      "label_all has shape (767,)\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix that contains all the train data\n",
    "data_all = OL_data_train_vow\n",
    "data_all = np.vstack(( data_all, B_train_data))\n",
    "data_all = np.vstack(( data_all, R_train_data))\n",
    "data_all = np.vstack(( data_all, M_train_data))\n",
    "# Create an array that contains all the train labels\n",
    "label_all = OL_label_train_vow\n",
    "label_all = np.hstack(( label_all, B_train_label))\n",
    "label_all = np.hstack(( label_all, R_train_label))\n",
    "label_all = np.hstack(( label_all, M_train_label))\n",
    "# Shuffle the matrix and the label\n",
    "data_all, label_all = myParse.shuffleDataset(data_all, label_all)\n",
    "\n",
    "print('data_all has shape  ' + str(data_all.shape))\n",
    "print('label_all has shape ' + str(label_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_test has shape  (330, 600)\n",
      "label_test has shape (330,)\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix that contains all the train data\n",
    "data_test = OL_data_test_vow\n",
    "data_test = np.vstack(( data_test, B_test_data))\n",
    "data_test = np.vstack(( data_test, R_test_data))\n",
    "data_test = np.vstack(( data_test, M_test_data))\n",
    "# Create an array that contains all the train labels\n",
    "label_test = OL_label_test_vow\n",
    "label_test = np.hstack(( label_test, B_test_label))\n",
    "label_test = np.hstack(( label_test, R_test_label))\n",
    "label_test = np.hstack(( label_test, M_test_label))\n",
    "\n",
    "print('data_test has shape  ' + str(data_test.shape))\n",
    "print('label_test has shape ' + str(label_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method for loading the dataset is to load it from the txt file \"training_file\". This file is an already shuffled dataset. I can use this for both feeding data in this simulation and also to the STM in order to have the closes behaviour possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['A' 'B' 'E' 'I' 'M' 'O' 'R' 'U']\n",
      "\n",
      "Raw shape        -> (201000, 5)\n",
      "Tot samples      -> 1005\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (702, 600)\n",
      "Test data shape   -> (301, 600)\n"
     ]
    }
   ],
   "source": [
    "data, label = myParse.loadDataFromTxt('training_file')\n",
    "data_train, label_train, data_test, label_test = myParse.parseTrainTest(data, label, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class Data_Container is just a container that I created in order to have all the dataset in a single object. This is useful for the plotting functions because it allows me to give as input to the function just one object and not the entire list of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Container(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.R_test_data       = R_test_data\n",
    "        self.R_test_label      = R_test_label\n",
    "        self.B_test_data       = B_test_data\n",
    "        self.B_test_label      = B_test_label\n",
    "        self.M_test_data       = M_test_data\n",
    "        self.M_test_label      = M_test_label\n",
    "        self.R_test_data       = R_test_data\n",
    "        self.OL_data_test_vow  = OL_data_test_vow\n",
    "        self.OL_label_test_vow = OL_label_test_vow\n",
    "        \n",
    "OL_testing_data = Data_Container()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the content of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block is used only to check the type of letters that are inside the datasets that I imported. It's used in order to see if the datasets are created and saved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOWELS DATASET SANITY CHECK\n",
      "    The letters found are:              ['O', 'U', 'A', 'E', 'I']\n",
      "    And for each letter the counter is: [103. 103. 103. 103. 103.   1.   0.   0.   0.]\n",
      "\n",
      "B DATASET SANITY CHECK\n",
      "    The letters found are:              ['B']\n",
      "    And for each letter the counter is: [194.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      "\n",
      "R DATASET SANITY CHECK\n",
      "    The letters found are:              ['R']\n",
      "    And for each letter the counter is: [192.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      "\n",
      "M DATASET SANITY CHECK\n",
      "    The letters found are:              ['M']\n",
      "    And for each letter the counter is: [192.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      "\n",
      "TRAIN DATASET SANITY CHECK\n",
      "    The letters found are:              ['U', 'I', 'R', 'M', 'B', 'A', 'O', 'E']\n",
      "    And for each letter the counter is: [ 73.  73. 112. 112. 105.  81.  74.  71.   1.]\n",
      "\n",
      "TEST DATASET SANITY CHECK\n",
      "    The letters found are:              ['I', 'M', 'U', 'B', 'R', 'O', 'A', 'E']\n",
      "    And for each letter the counter is: [30. 45. 30. 59. 53. 30. 21. 32.  1.]\n"
     ]
    }
   ],
   "source": [
    "print('VOWELS DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(vowels_label)\n",
    "print('\\nB DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(B_label)\n",
    "print('\\nR DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(R_label)\n",
    "print('\\nM DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(M_label)\n",
    "print('\\nTRAIN DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(label_train)\n",
    "print('\\nTEST DATASET SANITY CHECK')\n",
    "myParse.sanityCheckDataset(label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD TF TRAINED MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I load the frozen model. The frozen model is the NN that has been trained with keras on the PC. The script that trains this model is called 'run_trainFroznModel.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.abspath('')\n",
    "MODEL_PATH = ROOT_PATH + \"/Saved_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(MODEL_PATH + 'Original_model/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TINY OL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the main part of the continual learning study is found. Here can be found the functions used for implementing the different algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is an implementation of the softmx function. I had to use this because I noticed that the sofmtax function used from keras and other methods for computing the sotmax operation gave different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc_softmax(array):\n",
    "    \"\"\" Computes softmax of an array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array_like\n",
    "        Is the array of which I want to compute the softmax operation\n",
    "    \"\"\"\n",
    "    \n",
    "    if(len(array.shape)==2):\n",
    "        array = array[0]\n",
    "        \n",
    "    size    = len(array)\n",
    "    ret_ary = np.zeros([len(array)])\n",
    "    m       = array[0]\n",
    "    sum_val = 0\n",
    "\n",
    "    for i in range(0, size):\n",
    "        if(m<array[i]):\n",
    "            m = array[i]\n",
    "\n",
    "    for i in range(0, size):\n",
    "        sum_val += np.exp(array[i] - m)\n",
    "\n",
    "    constant = m + np.log(sum_val)\n",
    "    for i in range(0, size):\n",
    "        ret_ary[i] = np.exp(array[i] - constant)\n",
    "        \n",
    "    return ret_ary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyOL class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is just a container for all the informations that are required in order to use correctly a tinyOL model. The idea is to createa  container in which everything is stored and then simply change the method for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Layer(object):\n",
    "    def __init__(self, model):\n",
    "\n",
    "        # Related to the layer\n",
    "        self.ML_frozen = keras.models.Sequential(model.layers[:-1])  # extract the last layer from the original model\n",
    "        self.ML_frozen.compile()\n",
    "        \n",
    "        self.W = np.array(model.layers[-1].get_weights()[0])    # extract the weights from the last layer\n",
    "        self.b = np.array(model.layers[-1].get_weights()[1])    # extract the biases from the last layer\n",
    "               \n",
    "        self.W_2 = np.zeros(self.W.shape)\n",
    "        self.b_2 = np.zeros(self.b.shape)\n",
    "        \n",
    "        self.label = ['A', 'E', 'I', 'O', 'U']                  # the original model knows only the vowels\n",
    "        self.std_label = ['A', 'E', 'I', 'O', 'U', 'B', 'R', 'M']\n",
    "        \n",
    "        self.l_rate = 0                                         # learning rate that changes depending on the algorithm        \n",
    "\n",
    "        self.batch_size = 0\n",
    "        \n",
    "        # Related to the results fo the model\n",
    "        self.conf_matr = np.zeros((8,8))    # container for the confusion matrix       \n",
    "        self.macro_avrg_precision = 0       \n",
    "        self.macro_avrg_recall = 0\n",
    "        self.macro_avrg_F1score = 0\n",
    "        \n",
    "        self.title = ''       # title that will be displayed on plots\n",
    "        self.filename = ''    # name of the files to be saved (plots, charts, conf matrix)\n",
    "        \n",
    "        \n",
    "    # Function that is used for the prediction of the model saved in this class\n",
    "    def predict(self, x):\n",
    "        mat_prod = np.array(np.matmul(x, self.W) + self.b)\n",
    "        return  myFunc_softmax(mat_prod) # othwerwise do it with keras|also remove np.array()| tf.nn.softmax(mat_prod) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyOL functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used in all methods before the feed forward of the OL layer. This function is required because it checks if the input letter is already known. If this is not true it will increse the dimension of the last layre (weight matrix and biases array) and also save the new letter in the 'known classes' array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkLabelKnown(model, current_label):\n",
    "    \n",
    "    found = 0\n",
    "    \n",
    "    for i in range(0, len(model.label)):\n",
    "        if(current_label == model.label[i]):\n",
    "            found = 1\n",
    "        \n",
    "        \n",
    "    # If the label is not known\n",
    "    if(found==0):\n",
    "        print(f'\\n\\n    New letter detected -> letter \\033[1m{current_label}\\033[0m \\n')\n",
    "\n",
    "        model.label.append(current_label)   # Add new letter to label\n",
    "                \n",
    "        # Increase weights and biases dimensions\n",
    "        model.W = np.hstack((model.W, np.zeros([128,1])))# width = 5, height = 128\n",
    "        model.b = np.hstack((model.b, np.zeros([1])))\n",
    "        \n",
    "        model.W_2 = np.hstack((model.W_2, np.zeros([128,1])))\n",
    "        model.b_2 = np.hstack((model.b_2, np.zeros([1])))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here the functions that implement the different methods can be found. The explanation of the code od these function is not here but it can be found in the paper \"Continuous learning in single incremental taskscenarios\" and some schemes can be found in my presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb = data_train.shape[0]\n",
    "\n",
    "frozenOut_pc  = np.zeros((numb, 128))\n",
    "weight_pc     = np.zeros((numb, 80))\n",
    "bias_pc       = np.zeros((numb,8))\n",
    "preSoftmax_pc = np.zeros((numb,8))\n",
    "softmax_pc    = np.zeros((numb,8))\n",
    "\n",
    "weight_letter_b = np.zeros((128, numb))\n",
    "\n",
    "selected_w = [46,13,107,3,57,65,127,81,89,70,\n",
    "                143,239,142,158,207,189,172,230,156,208,\n",
    "                374,359,375,371,303,298,350,257,349,333,\n",
    "                402,502,485,461,489,479,454,508,485,480,\n",
    "                527,565,614,517,528,613,625,623,587,521,\n",
    "                712,742,685,746,759,747,754,702,653,640,\n",
    "                775,809,798,853,804,840,828,788,890,819,\n",
    "                906,1019,911,1005,1016,953,1016,987,961,1023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL(model, x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with OL METHOD - STOCHASTICH\\n')\n",
    "   \n",
    "    cntr = 1\n",
    "    learn_rate  = model.l_rate\n",
    "    train_samples = x_train.shape[0]\n",
    "    test_samples = x_test.shape[0]\n",
    "    tot_samples =  train_samples + test_samples\n",
    "                \n",
    "    # Cycle over all samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        if(i<train_samples):\n",
    "            current_label = y_train[i]\n",
    "        else:\n",
    "            current_label = y_test[i-train_samples]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label)\n",
    "               \n",
    "        # PPREDICTION\n",
    "        if(i<train_samples):\n",
    "            y_ML   = model.ML_frozen.predict(x_train[i,:].reshape(1,x_train.shape[1]))\n",
    "            \n",
    "            temp = np.copy(np.array(np.matmul(y_ML, model.W) + model.b))\n",
    "            temp = temp[0]\n",
    "        else:\n",
    "            y_ML   = model.ML_frozen.predict(x_test[i-train_samples,:].reshape(1,x_test.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])           \n",
    "        \n",
    "\n",
    "        \n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "        \n",
    "        for j in range(0,model.W.shape[0]):\n",
    "            # Update weights\n",
    "            deltaW = np.multiply(cost, y_ML[0,j])\n",
    "            dW     = np.multiply(deltaW, learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "        # Update biases\n",
    "        db      = np.multiply(cost, learn_rate)\n",
    "        model.b = model.b-db\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # TO BE REMOVED LATER **********\n",
    "        # SAVE THE WEIGHTS IN A MATRIX\n",
    "        if(i<numb):\n",
    "            \n",
    "            frozenOut_pc[i,:] = y_ML[0,:]\n",
    "                    \n",
    "            for q in range(0, 8):\n",
    "                if(q<model.W.shape[1]):\n",
    "                    bias_pc[i,q]       = np.copy(model.b[q])\n",
    "                    softmax_pc[i,q]    = np.copy(y_pred[q])\n",
    "                    preSoftmax_pc[i,q] = np.copy(temp[q])\n",
    "\n",
    "            for q in range(0, 80):\n",
    "                if(int(selected_w[q]/128) < model.W.shape[1] ):\n",
    "                    weight_pc[i,q] = np.copy(model.W[selected_w[q]%128, int(selected_w[q]/128)])\n",
    "        # *********************************\n",
    "        \n",
    "        \n",
    "        \n",
    "        # if the train data is finished still train the model but save the results\n",
    "        if(i>=train_samples):\n",
    "            \n",
    "            # Find the max iter for both true label and prediction\n",
    "            if(np.amax(y_true_soft) != 0):\n",
    "                max_i_true = np.argmax(y_true_soft)\n",
    "\n",
    "            if(np.amax(y_pred) != 0):\n",
    "                max_i_pred = np.argmax(y_pred)\n",
    "\n",
    "            # Fill up the confusion matrix\n",
    "            for k in range(0,len(model.label)):\n",
    "                if(model.label[max_i_pred] == model.std_label[k]):\n",
    "                    p = np.copy(k)\n",
    "                if(model.label[max_i_true] == model.std_label[k]):\n",
    "                    t = np.copy(k)\n",
    "\n",
    "            model.conf_matr[t,p] += 1    \n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/tot_samples,4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL MINI BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL_miniBatch(model, x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with OL METHOD - MINI BATCH\\n')\n",
    "    \n",
    "    cntr=1\n",
    "    learn_rate = model.l_rate\n",
    "    batch_size = model.batch_size\n",
    "    \n",
    "    train_samples = x_train.shape[0]\n",
    "    test_samples  = x_test.shape[0]\n",
    "    tot_samples   = train_samples + test_samples\n",
    "    \n",
    "    model.W_2 = np.zeros((model.W.shape))\n",
    "    model.b_2 = np.zeros((model.b.shape))\n",
    "\n",
    "            \n",
    "    # Cycle over all samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        if(i<train_samples):\n",
    "            current_label = y_train[i]\n",
    "        else:\n",
    "            current_label = y_test[i-train_samples]\n",
    "\n",
    "        checkLabelKnown(model, current_label)\n",
    "        \n",
    "        y_true_soft = letterToSoftmax(current_label, model.label)        \n",
    "                \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "        \n",
    "        # PREDICTION\n",
    "        if(i<train_samples):\n",
    "            y_ML = model.ML_frozen.predict(x_train[i,:].reshape(1,x_train.shape[1]))\n",
    "        else:\n",
    "            y_ML = model.ML_frozen.predict(x_test[i-train_samples,:].reshape(1,x_test.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "\n",
    "        for j in range(0,h): \n",
    "            # Update weights\n",
    "            deltaW = np.multiply(cost, y_ML[0,j]) \n",
    "            model.W_2[j,:] += deltaW\n",
    "\n",
    "        # Update biases\n",
    "        model.b_2 += cost\n",
    "        \n",
    "        if(i%batch_size==0 and i!=0):\n",
    "            model.W = model.W - np.multiply(model.W_2, 1/batch_size*learn_rate)\n",
    "            model.b = model.b - np.multiply(model.b_2, 1/batch_size*learn_rate)\n",
    "\n",
    "            model.W_2 = np.zeros((model.W.shape))  #reset each batch  \n",
    "            model.b_2 = np.zeros((model.b.shape))  #reset each batch   \n",
    "            \n",
    "        # UPDATE THE CONFUSION MATRIX\n",
    "        # if the train data is finished still train the model but save the results\n",
    "        if(i>=train_samples):\n",
    "            \n",
    "            # Find the max iter for both true label and prediction\n",
    "            if(np.amax(y_true_soft) != 0):\n",
    "                max_i_true = np.argmax(y_true_soft)\n",
    "\n",
    "            if(np.amax(y_pred) != 0):\n",
    "                max_i_pred = np.argmax(y_pred)\n",
    "\n",
    "            # Fill up the confusion matrix\n",
    "            for k in range(0,len(model.label)):\n",
    "                if(model.label[max_i_pred] == model.std_label[k]):\n",
    "                    p = np.copy(k)\n",
    "                if(model.label[max_i_true] == model.std_label[k]):\n",
    "                    t = np.copy(k)\n",
    "\n",
    "            model.conf_matr[t,p] += 1    \n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/tot_samples,4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def trainOneEpoch_OL_v2(model, x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with CWR METHOD - STOCASTICH \\n')\n",
    "    \n",
    "    cntr = 1\n",
    "    learn_rate = model.l_rate\n",
    "    \n",
    "    train_samples = x_train.shape[0]\n",
    "    test_samples  = x_test.shape[0]\n",
    "    tot_samples   = train_samples + test_samples\n",
    "                \n",
    "    # Cycle over every sample\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        if(i<train_samples):\n",
    "            current_label = y_train[i]  \n",
    "        else:\n",
    "            current_label = y_test[i-train_samples]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label)\n",
    "        \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "                \n",
    "        # PREDICTION\n",
    "        if(i<train_samples):\n",
    "            y_ML = model.ML_frozen.predict(x_train[i,:].reshape(1,x_train.shape[1]))   \n",
    "        else:\n",
    "            y_ML = model.ML_frozen.predict(x_test[i-train_samples,:].reshape(1,x_test.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "        cost[0] = 0   # A\n",
    "        cost[1] = 0   # E\n",
    "        cost[2] = 0   # I\n",
    "        cost[3] = 0   # O\n",
    "        cost[4] = 0   # U\n",
    "\n",
    "        # Update weights\n",
    "        for j in range(0,h):\n",
    "\n",
    "            dW = np.multiply(cost, y_ML[0,j]*learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "        # Update biases\n",
    "        db = np.multiply(cost, learn_rate)\n",
    "        model.b = model.b-db\n",
    "        \n",
    "        \n",
    "        # if the train data is finished still train the model but save the results\n",
    "        if(i>=train_samples):\n",
    "            \n",
    "            # Find the max iter for both true label and prediction\n",
    "            if(np.amax(y_true_soft) != 0):\n",
    "                max_i_true = np.argmax(y_true_soft)\n",
    "\n",
    "            if(np.amax(y_pred) != 0):\n",
    "                max_i_pred = np.argmax(y_pred)\n",
    "\n",
    "            # Fill up the confusion matrix\n",
    "            for k in range(0,len(model.label)):\n",
    "                if(model.label[max_i_pred] == model.std_label[k]):\n",
    "                    p = np.copy(k)\n",
    "                if(model.label[max_i_true] == model.std_label[k]):\n",
    "                    t = np.copy(k)\n",
    "\n",
    "            model.conf_matr[t,p] += 1 \n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/tot_samples,4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2 MINI BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL_v2_miniBatch(model, x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with CWR - MINI BATCH \\n ')  \n",
    "\n",
    "    cntr = 1\n",
    "    learn_rate = model.l_rate\n",
    "    batch_size = model.batch_size\n",
    "    \n",
    "    \n",
    "    train_samples = x_train.shape[0]\n",
    "    test_samples  = x_test.shape[0]\n",
    "    tot_samples   = train_samples + test_samples\n",
    "    \n",
    "    model.W_2 = np.zeros((model.W.shape))\n",
    "    model.b_2 = np.zeros((model.b.shape))\n",
    "           \n",
    "    # Cycle over all input samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        if(i<train_samples):\n",
    "            current_label = y_train[i]  \n",
    "        else:\n",
    "            current_label = y_test[i-train_samples]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "                \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "            \n",
    "        # PREDICTION\n",
    "        if(i<train_samples):\n",
    "            y_ML = model.ML_frozen.predict(x_train[i,:].reshape(1,x_train.shape[1]))   \n",
    "        else:\n",
    "            y_ML = model.ML_frozen.predict(x_test[i-train_samples,:].reshape(1,x_test.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "        cost[0] = 0   # A\n",
    "        cost[1] = 0   # E\n",
    "        cost[2] = 0   # I\n",
    "        cost[3] = 0   # O\n",
    "        cost[4] = 0   # U\n",
    "\n",
    "        for j in range(0,h):  \n",
    "            # Update weights\n",
    "            deltaW = np.multiply(cost, y_ML[0,j]) \n",
    "            model.W_2[j,:] += deltaW\n",
    "\n",
    "        # Update biases\n",
    "        model.b_2 += cost\n",
    "\n",
    "        \n",
    "        # If beginning of batch\n",
    "        if(i%batch_size==0 and i!=0):\n",
    "            for j in range(0, 5):\n",
    "                for k in range(0, h):\n",
    "                    model.W_2[k,j] = 0\n",
    "                model.b_2[j] = 0\n",
    "                    \n",
    "            model.W   = model.W - np.multiply(model.W_2, 1/batch_size*learn_rate)\n",
    "            model.b   = model.b - np.multiply(model.b_2, 1/batch_size*learn_rate)\n",
    "            model.W_2 = np.zeros((model.W.shape))  # reset\n",
    "            model.b_2 = np.zeros((model.b.shape))  # reset\n",
    "        \n",
    "            \n",
    "        # UPDATE CONFUSION MATRIX\n",
    "        # if the train data is finished still train the model but save the results\n",
    "        if(i>=train_samples):\n",
    "            \n",
    "            # Find the max iter for both true label and prediction\n",
    "            if(np.amax(y_true_soft) != 0):\n",
    "                max_i_true = np.argmax(y_true_soft)\n",
    "\n",
    "            if(np.amax(y_pred) != 0):\n",
    "                max_i_pred = np.argmax(y_pred)\n",
    "\n",
    "            # Fill up the confusion matrix\n",
    "            for k in range(0,len(model.label)):\n",
    "                if(model.label[max_i_pred] == model.std_label[k]):\n",
    "                    p = np.copy(k)\n",
    "                if(model.label[max_i_true] == model.std_label[k]):\n",
    "                    t = np.copy(k)\n",
    "\n",
    "            model.conf_matr[t,p] += 1 \n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/tot_samples,4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpochOL_LWF(model, x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with LWF - STOCHASTIC\\n ') \n",
    "    \n",
    "    lam  = 0\n",
    "    cntr = 1\n",
    "    learn_rate = model.l_rate\n",
    "    \n",
    "    train_samples = x_train.shape[0]\n",
    "    test_samples  = x_test.shape[0]\n",
    "    tot_samples   = train_samples + test_samples\n",
    "    \n",
    "    y_LWF = np.zeros([1, 8])    # Define container for LWF\n",
    "\n",
    "    # DEFINE ORIGINAL WEIGHTS AND BIASES\n",
    "    model.W_2 = np.copy(model.W)\n",
    "    model.b_2 = np.copy(model.b)\n",
    "         \n",
    "    # Cycle over every sample\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        if(i<train_samples):\n",
    "            current_label = y_train[i]  \n",
    "        else:\n",
    "            current_label = y_test[i-train_samples]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "                \n",
    "        w = model.W.shape[1]\n",
    "        h = model.W.shape[0]\n",
    "        \n",
    "        # PREDICTION WITH CURRENT WEIGHTS\n",
    "        if(i<train_samples):\n",
    "            y_ML = model.ML_frozen.predict(x_train[i,:].reshape(1,x_train.shape[1]))   \n",
    "        else:\n",
    "            y_ML = model.ML_frozen.predict(x_test[i-train_samples,:].reshape(1,x_test.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "        # PREDICTION WITH LWF WEIGHTS\n",
    "        y_LWF = myFunc_softmax(np.array(np.matmul(y_ML, model.W_2) + model.b_2))\n",
    "        \n",
    "        lam = 100/(100+cntr)   \n",
    "        \n",
    "        # BACKPROPAGATION        \n",
    "        cost_norm = y_pred-y_true_soft\n",
    "        cost_LWF  = y_pred-y_LWF\n",
    "\n",
    "\n",
    "        for j in range(0,h):\n",
    "            # Update weights\n",
    "            deltaW_norm  = np.multiply(cost_norm,1-lam)\n",
    "            deltaW_LWF   = np.multiply(cost_LWF, lam)\n",
    "            dW           = np.multiply(deltaW_norm+deltaW_LWF, y_ML[0,j]*learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "        # Update biases\n",
    "        db_norm = np.multiply(cost_norm, 1-lam)\n",
    "        db_LWF  = np.multiply(cost_LWF, lam)\n",
    "        db      = np.multiply(db_norm+db_LWF, learn_rate)\n",
    "        model.b = model.b-db\n",
    "        \n",
    "        \n",
    "        # if the train data is finished still train the model but save the results\n",
    "        if(i>=train_samples):\n",
    "            \n",
    "            # Find the max iter for both true label and prediction\n",
    "            if(np.amax(y_true_soft) != 0):\n",
    "                max_i_true = np.argmax(y_true_soft)\n",
    "\n",
    "            if(np.amax(y_pred) != 0):\n",
    "                max_i_pred = np.argmax(y_pred)\n",
    "\n",
    "            # Fill up the confusion matrix\n",
    "            for k in range(0,len(model.label)):\n",
    "                if(model.label[max_i_pred] == model.std_label[k]):\n",
    "                    p = np.copy(k)\n",
    "                if(model.label[max_i_true] == model.std_label[k]):\n",
    "                    t = np.copy(k)\n",
    "\n",
    "            model.conf_matr[t,p] += 1 \n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/tot_samples,4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF MINI BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpochOL_LWF_v2(model, x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with LWF - MINI BATCH\\n')\n",
    "    \n",
    "    lam  = 0\n",
    "    cntr = 1\n",
    "    learn_rate = model.l_rate\n",
    "    batch_size = model.batch_size\n",
    "    \n",
    "    train_samples = x_train.shape[0]\n",
    "    test_samples  = x_test.shape[0]\n",
    "    tot_samples   = train_samples + test_samples\n",
    "                \n",
    "    y_LWF = np.zeros([1, 8])\n",
    "    \n",
    "    model.W_2 = np.copy(model.W)   # copy from TF\n",
    "    model.b_2 = np.copy(model.b)\n",
    "        \n",
    "        \n",
    "    # For every sample in the dataset given\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        if(i<train_samples):\n",
    "            current_label = y_train[i]  \n",
    "        else:\n",
    "            current_label = y_test[i-train_samples]\n",
    "        \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "                        \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "                   \n",
    "        # PREDICTION WITH CURRENT WEIGHTS\n",
    "        if(i<train_samples):\n",
    "            y_ML = model.ML_frozen.predict(x_train[i,:].reshape(1,x_train.shape[1]))   \n",
    "        else:\n",
    "            y_ML = model.ML_frozen.predict(x_test[i-train_samples,:].reshape(1,x_test.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])  \n",
    "        # PREDICTION WITH LWF WEIGHTS\n",
    "        mat_prod = np.array(np.matmul(y_ML, model.W_2) + model.b_2)\n",
    "        y_LWF = myFunc_softmax(mat_prod)        \n",
    "\n",
    "        if(cntr<batch_size):\n",
    "            lam = 1\n",
    "        else:\n",
    "            lam = batch_size/cntr  #(cntr/493)   va da 0 a 1\n",
    "         \n",
    "        # ---- BACKPROPAGATION | MINI BATCH + LWF        \n",
    "        cost_norm = y_pred-y_true_soft\n",
    "        cost_LWF  = y_pred-y_LWF\n",
    "        \n",
    "        lam_cost_norm = np.multiply(cost_norm, 1-lam)\n",
    "        lam_cost_LWF  = np.multiply(cost_LWF,  lam)\n",
    "\n",
    "        for j in range(0,h):\n",
    "\n",
    "            # Update weights\n",
    "            deltaW = np.multiply(lam_cost_norm+lam_cost_LWF, y_ML[0,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW          \n",
    "            \n",
    "        # Update biases \n",
    "        db = np.multiply(lam_cost_norm+lam_cost_LWF, learn_rate)\n",
    "        model.b = model.b-db   \n",
    "        \n",
    "        # END OF BATCH\n",
    "        if(i%batch_size==0 and i!=0):            \n",
    "            model.W_2 = np.copy(model.W)    # update the LWF w matrix\n",
    "            model.b_2 = np.copy(model.b)    # update the LWF b matrix\n",
    "        \n",
    "        \n",
    "        # if the train data is finished still train the model but save the results\n",
    "        if(i>=train_samples):\n",
    "            \n",
    "            # Find the max iter for both true label and prediction\n",
    "            if(np.amax(y_true_soft) != 0):\n",
    "                max_i_true = np.argmax(y_true_soft)\n",
    "\n",
    "            if(np.amax(y_pred) != 0):\n",
    "                max_i_pred = np.argmax(y_pred)\n",
    "\n",
    "            # Fill up the confusion matrix\n",
    "            for k in range(0,len(model.label)):\n",
    "                if(model.label[max_i_pred] == model.std_label[k]):\n",
    "                    p = np.copy(k)\n",
    "                if(model.label[max_i_true] == model.std_label[k]):\n",
    "                    t = np.copy(k)\n",
    "\n",
    "            model.conf_matr[t,p] += 1 \n",
    "                        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/tot_samples,4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_CWR(model, x_train, x_test, y_train, y_test):\n",
    "        \n",
    "    print('**********************************\\nPerforming training CWR \\n ')  \n",
    "\n",
    "    cntr = 1\n",
    "    learn_rate = model.l_rate\n",
    "    batch_size = model.batch_size\n",
    "\n",
    "    train_samples = x_train.shape[0]\n",
    "    test_samples  = x_test.shape[0]\n",
    "    tot_samples   = train_samples + test_samples\n",
    "    \n",
    "    model.W_2  = np.zeros((model.W.shape))\n",
    "    model.b_2  = np.zeros((model.b.shape))\n",
    "    found_lett = np.zeros(8)\n",
    "                   \n",
    "    # Cycle over all input samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        if(i<train_samples):\n",
    "            current_label = y_train[i]  \n",
    "        else:\n",
    "            current_label = y_test[i-train_samples] \n",
    "            \n",
    "        checkLabelKnown(model, current_label)\n",
    "        y_true_soft = letterToSoftmax(current_label, model.label) \n",
    "        \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "                \n",
    "        found_lett[np.argmax(y_true_soft)] += 1  # update the letter counter\n",
    "            \n",
    "        # PREDICTION\n",
    "        if(i<train_samples):\n",
    "            y_ML = model.ML_frozen.predict(x_train[i,:].reshape(1,x_train.shape[1]))   \n",
    "        else:\n",
    "            y_ML = model.ML_frozen.predict(x_test[i-train_samples,:].reshape(1,x_test.shape[1]))\n",
    "        y_pred = myFunc_softmax(np.array(np.matmul(y_ML, model.W_2) + model.b_2))      \n",
    "\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft\n",
    "\n",
    "        # Update weights\n",
    "        for j in range(0,h):\n",
    "            deltaW = np.multiply(cost, y_ML[0,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            model.W_2[j,:] = model.W_2[j,:] - dW\n",
    "\n",
    "        # Update biases\n",
    "        db = np.multiply(cost, learn_rate)\n",
    "        model.b_2 = model.b_2-db\n",
    "        \n",
    "        \n",
    "        # If beginning of batch\n",
    "        if(i%batch_size==0 and i!=0): \n",
    "            for k in range(0, w):\n",
    "                if(found_lett[k]!=0):\n",
    "                    tempW = np.multiply(model.W[:,k], found_lett[k])\n",
    "                    tempB = np.multiply(model.b[k]  , found_lett[k])\n",
    "                    model.W[:,k] = np.multiply(tempW+model.W_2[:,k], 1/(found_lett[k]+1))\n",
    "                    model.b[k]   = np.multiply(tempB+model.b_2[k],   1/(found_lett[k]+1))\n",
    "                    \n",
    "            model.W_2  = np.copy(model.W)\n",
    "            model.b_2  = np.copy(model.b)\n",
    "            found_lett = np.zeros(8)  # reset                   \n",
    "                    \n",
    "                    \n",
    "        # if the train data is finished still train the model but save the results\n",
    "        if(i>=train_samples):\n",
    "            \n",
    "            # Find the max iter for both true label and prediction\n",
    "            if(np.amax(y_true_soft) != 0):\n",
    "                max_i_true = np.argmax(y_true_soft)\n",
    "\n",
    "            if(np.amax(y_pred) != 0):\n",
    "                max_i_pred = np.argmax(y_pred)\n",
    "\n",
    "            # Fill up the confusion matrix\n",
    "            for k in range(0,len(model.label)):\n",
    "                if(model.label[max_i_pred] == model.std_label[k]):\n",
    "                    p = np.copy(k)\n",
    "                if(model.label[max_i_true] == model.std_label[k]):\n",
    "                    t = np.copy(k)\n",
    "\n",
    "            model.conf_matr[t,p] += 1 \n",
    "                    \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/tot_samples,4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dfine some important values for the training and then actually train each single OL layer witha  different method. The 0 and 1 below are used in order to activate or deactivate the training and the following plots of a specific method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_OL = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE WHICH TRAINING AND PLOTS TO SHOW\n",
    "\n",
    "KERAS      = 0\n",
    "OL_vowels  = 1\n",
    "OL         = 1\n",
    "OL_mini    = 1\n",
    "LWF        = 1\n",
    "LWF_mini   = 1\n",
    "OL_v2      = 1\n",
    "OL_v2_mini = 1\n",
    "CWR        = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define KERAS model (just create the class, actually don't train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(KERAS==1):\n",
    "    Model_KERAS = Custom_Layer(model)\n",
    "    Model_KERAS.title = 'KERAS'\n",
    "    Model_KERAS.filename = 'KERAS'\n",
    "    Model_KERAS.label = ['A','E','I','O','U','B','R','M']\n",
    "    Model_KERAS.batch_size = batch_size_OL\n",
    "    # DO NOT PERFORM TRAINING, KEEP IT AS IT IS, IT'S THE ORIGINAL MODEL\n",
    "    \n",
    "    myTest.test_OLlayer(Model_KERAS, OL_data_train_vow, OL_label_train_vow)\n",
    "    myBar.plot_barChart(Model_KERAS)   \n",
    "    myMatrix.plot_confMatrix(Model_KERAS)\n",
    "    myTable.table_params(Model_KERAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL only on vowels (just to see if the OL model makes the keras better or worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Performing training with OL METHOD - STOCHASTICH\n",
      "\n",
      "    Currently at 25.58% of dataset"
     ]
    }
   ],
   "source": [
    "if(OL_vowels==1):\n",
    "    Model_OL_vowels = Custom_Layer(model)\n",
    "    Model_OL_vowels.title = 'VOWELS'\n",
    "    Model_OL_vowels.filename = 'OL_vowels'\n",
    "    Model_OL_vowels.l_rate = 0.000005\n",
    "    Model_OL_vowels.batch_size = batch_size_OL\n",
    "    \n",
    "    trainOneEpoch_OL(Model_OL_vowels, OL_data_train_vow, OL_data_test_vow, OL_label_train_vow, OL_label_test_vow)\n",
    "    \n",
    "    myWrite.save_confMatrix(Model_OL_vowels)\n",
    "    myBar.plot_barChart(Model_OL_vowels)\n",
    "    myMatrix.plot_confMatrix(Model_OL_vowels)\n",
    "    myTable.table_params(Model_OL_vowels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(OL==1):\n",
    "    Model_OL_all_mixed = Custom_Layer(model)\n",
    "    Model_OL_all_mixed.title = 'OL' \n",
    "    Model_OL_all_mixed.filename = 'OL'\n",
    "    Model_OL_all_mixed.l_rate = 0.00005 # 0.000005 true value jupyter\n",
    "    Model_OL_all_mixed.batch_size = batch_size_OL\n",
    "\n",
    "    trainOneEpoch_OL(Model_OL_all_mixed, data_train, data_test, label_train, label_test)\n",
    "    \n",
    "    myWrite.save_confMatrix(Model_OL_all_mixed)\n",
    "    myBar.plot_barChart(Model_OL_all_mixed)\n",
    "    myMatrix.plot_confMatrix(Model_OL_all_mixed)\n",
    "    myTable.table_params(Model_OL_all_mixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(OL_mini==1):\n",
    "    Model_OL_mini = Custom_Layer(model)\n",
    "    Model_OL_mini.title = 'OL + mini batch'\n",
    "    Model_OL_mini.filename = 'OL_batches'\n",
    "    Model_OL_mini.l_rate = 0.0005\n",
    "    Model_OL_mini.batch_size = batch_size_OL\n",
    "\n",
    "    trainOneEpoch_OL_miniBatch(Model_OL_mini, data_train, data_test, label_train, label_test)\n",
    "    \n",
    "    myWrite.save_confMatrix(Model_OL_mini)\n",
    "    myBar.plot_barChart(Model_OL_mini)\n",
    "    myMatrix.plot_confMatrix(Model_OL_mini)\n",
    "    myTable.table_params(Model_OL_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(OL_v2==1):\n",
    "    Model_OL_v2 = Custom_Layer(model)\n",
    "    Model_OL_v2.title = 'OL v2' \n",
    "    Model_OL_v2.filename = 'OL_v2'\n",
    "    Model_OL_v2.l_rate = 0.001\n",
    "    Model_OL_v2.batch_size = batch_size_OL\n",
    "\n",
    "    trainOneEpoch_OL_v2(Model_OL_v2, data_train, data_test, label_train, label_test)\n",
    "    \n",
    "    myWrite.save_confMatrix(Model_OL_v2)\n",
    "    myBar.plot_barChart(Model_OL_v2)\n",
    "    myMatrix.plot_confMatrix(Model_OL_v2)\n",
    "    myTable.table_params(Model_OL_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL V2 + mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(OL_v2_mini==1):\n",
    "    Model_OL_v2_miniBatch = Custom_Layer(model)\n",
    "    Model_OL_v2_miniBatch.title = 'OL v2 + mini batch'\n",
    "    Model_OL_v2_miniBatch.filename = 'OL_v2_batches'\n",
    "    Model_OL_v2_miniBatch.l_rate = 0.0005\n",
    "    Model_OL_v2_miniBatch.batch_size = batch_size_OL\n",
    "\n",
    "    trainOneEpoch_OL_v2_miniBatch(Model_OL_v2_miniBatch, data_train, data_test, label_train, label_test)\n",
    "    \n",
    "    myWrite.save_confMatrix(Model_OL_v2_miniBatch)\n",
    "    myBar.plot_barChart(Model_OL_v2_miniBatch)\n",
    "    myMatrix.plot_confMatrix(Model_OL_v2_miniBatch)\n",
    "    myTable.table_params(Model_OL_v2_miniBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(LWF==1):\n",
    "    Model_LWF_1 = Custom_Layer(model)\n",
    "    Model_LWF_1.title = 'LWF'\n",
    "    Model_LWF_1.filename = 'LWF'   \n",
    "    Model_LWF_1.l_rate = 0.00015 \n",
    "    Model_LWF_1.batch_size = batch_size_OL\n",
    "\n",
    "    trainOneEpochOL_LWF(Model_LWF_1, data_train, data_test, label_train, label_test)   \n",
    "    \n",
    "    myWrite.save_confMatrix(Model_LWF_1)\n",
    "    myBar.plot_barChart(Model_LWF_1)\n",
    "    myMatrix.plot_confMatrix(Model_LWF_1)\n",
    "    myTable.table_params(Model_LWF_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LWF + mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(LWF_mini==1):\n",
    "    Model_LWF_2 = Custom_Layer(model)\n",
    "    Model_LWF_2.title = 'LWF + mini batch'\n",
    "    Model_LWF_2.filename = 'LWF_batches'\n",
    "    Model_LWF_2.l_rate = 0.0005\n",
    "    Model_LWF_2.batch_size = batch_size_OL\n",
    "\n",
    "    trainOneEpochOL_LWF_v2(Model_LWF_2, data_train, data_test, label_train, label_test)\n",
    "    \n",
    "    myWrite.save_confMatrix(Model_LWF_2)\n",
    "    myBar.plot_barChart(Model_LWF_2)\n",
    "    myMatrix.plot_confMatrix(Model_LWF_2)\n",
    "    myTable.table_params(Model_LWF_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with CWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(CWR==1):\n",
    "    Model_CWR = Custom_Layer(model) \n",
    "    Model_CWR.title = 'CWR'\n",
    "    Model_CWR.filename = 'CWR'\n",
    "    Model_CWR.l_rate = 0.01\n",
    "\n",
    "    Model_CWR.batch_size = batch_size_OL\n",
    "\n",
    "    trainOneEpoch_CWR(Model_CWR, data_train, data_test, label_train, label_test)\n",
    "    \n",
    "    myWrite.save_confMatrix(Model_CWR)\n",
    "    myBar.plot_barChart(Model_CWR)\n",
    "    myMatrix.plot_confMatrix(Model_CWR)\n",
    "    myTable.table_params(Model_CWR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All bar plots together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myBar.plot_barChart_All()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot is a recap of all the methods trained. Note that it will be displayed only if all the training have been performed in this runtime. \n",
    "The table contains some additiona information and not only the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(KERAS and OL_vowels and OL and OL_mini and LWF and LWF_mini and OL_v2 and OL_v2_mini and CWR):\n",
    "    \n",
    "    myTable.table_simulationResult(Model_KERAS, Model_OL_vowels, Model_OL_all_mixed, Model_OL_mini, \n",
    "               Model_LWF_1, Model_LWF_2, Model_OL_v2, Model_OL_v2_miniBatch, Model_CWR)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below is used for storing the result of the simulation formermed in this runtime. This is used for another plotting function that will display the average accuracy of each method across multiple runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down in txt files all the results across 10 or so simulations          \n",
    "WRITE_SIMU_RES = 0\n",
    "                \n",
    "if(WRITE_SIMU_RES==1):\n",
    "    myWrite.save_simulationResult('Keras',     Model_KERAS)\n",
    "    myWrite.save_simulationResult('OL_vowels', Model_OL_vowels)\n",
    "    myWrite.save_simulationResult('OL',        Model_OL_all_mixed)\n",
    "    myWrite.save_simulationResult('OL_mini',   Model_OL_mini)\n",
    "    myWrite.save_simulationResult('LWF',       Model_LWF_1)\n",
    "    myWrite.save_simulationResult('LWF_mini',  Model_LWF_2)\n",
    "    myWrite.save_simulationResult('OL_v2',     Model_OL_v2)\n",
    "    myWrite.save_simulationResult('OL_v2_min', Model_OL_v2_miniBatch)\n",
    "    myWrite.save_simulationResult('CWR',       Model_CWR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the average accuracy over several runtimes\n",
    "\n",
    "#myBar.plot_barChart_SimuRes(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below plots some pie charts that shows how the dataset are composed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ENABLE_PLOTS = 0\n",
    "if(ENABLE_PLOTS==1):\n",
    "    \n",
    "    \n",
    "    vowels_data_tf, vowels_label_tf = myParse.loadDataFromTxt('vowels_TF')\n",
    "    TF_data_train, _, TF_data_test, _ = myParse.parseTrainTest(vowels_data_tf, vowels_label_tf, 0.7)\n",
    "\n",
    "\n",
    "    # Plot of the pie chart of the dataset TF e OL\n",
    "    dataset_shapes = np.zeros(8)\n",
    "    label_vow = ['A','E','I','O','U']\n",
    "\n",
    "    for i in range(0,vowels_data.shape[0]):\n",
    "        for j in range(0,len(label_vow)):\n",
    "            if(label_vow[j] == vowels_label[i]):\n",
    "                dataset_shapes[j] += 1\n",
    "                break\n",
    "    for i in range(0,vowels_data_tf.shape[0]):\n",
    "        for j in range(0,len(label_vow)):\n",
    "            if(label_vow[j] == vowels_label_tf[i]):\n",
    "                dataset_shapes[j] += 1\n",
    "                break\n",
    "\n",
    "    dataset_shapes[5] = B_data.shape[0]\n",
    "    dataset_shapes[6] = R_data.shape[0]\n",
    "    dataset_shapes[7] = M_data.shape[0]\n",
    "    myPie.plot_pieChart_datasetAll(dataset_shapes)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    dataset_shapes = np.zeros([8])\n",
    "    dataset_shapes[0] = OL_data_train_vow.shape[0]\n",
    "    dataset_shapes[1] = OL_data_test_vow.shape[0]\n",
    "    dataset_shapes[2] = B_train_data.shape[0]\n",
    "    dataset_shapes[3] = B_test_data.shape[0]\n",
    "    dataset_shapes[4] = R_train_data.shape[0]\n",
    "    dataset_shapes[5] = R_test_data.shape[0]\n",
    "    dataset_shapes[6] = M_train_data.shape[0]\n",
    "    dataset_shapes[7] = M_test_data.shape[0]\n",
    "    # Plot of the pie chart of the dataset OL\n",
    "    myPie.plot_pieChart_DatasetOL(dataset_shapes)\n",
    "\n",
    "    # Plot of the pie chart of the dataset TF\n",
    "    myPie.plot_pieChart_DatasetTF(TF_data_train.shape[0],TF_data_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights, biases and the out of frozen layer from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_stm       = myDebug.debug_loadBiasSMT()\n",
    "weight_stm     = myDebug.debug_loadWeightsSTM()\n",
    "frozenOut_stm  = myDebug.debug_loadFrozenOutSMT()\n",
    "softmax_stm    = myDebug.debug_loadSoftmaxSMT()\n",
    "preSoftmax_stm = myDebug.debug_loadPreSoftmaxSMT()\n",
    "\n",
    "max_dim = data_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK ITERATION NUMBER X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_n = 6\n",
    "\n",
    "print(f'labels is: {label_train[test_n]}')\n",
    "myDebug.plot_frozenDifference(test_n, frozenOut_pc, frozenOut_stm)\n",
    "print(f'bias pc  {bias_pc[test_n,0]:.7f} {bias_pc[test_n,1]:.7f} {bias_pc[test_n,2]:.7f} {bias_pc[test_n,3]:.7f} {bias_pc[test_n,4]:.7f} {bias_pc[test_n,5]:.7f} {bias_pc[test_n,6]:.7f} {bias_pc[test_n,7]:.7f}')\n",
    "print(f'bias stm {bias_stm[test_n,0]:.7f} {bias_stm[test_n,1]:.7f} {bias_stm[test_n,2]:.7f} {bias_stm[test_n,3]:.7f} {bias_stm[test_n,4]:.7f} {bias_stm[test_n,5]:.7f} {bias_stm[test_n,6]:.7f} {bias_stm[test_n,7]:.7f}')\n",
    "print()\n",
    "print(f'weight pc  {weight_pc[test_n,5]:.7f} {weight_pc[test_n,15]:.7f} {weight_pc[test_n,25]:.7f} {weight_pc[test_n,35]:.7f} {weight_pc[test_n,45]:.7f} {weight_pc[test_n,55]:.7f} {weight_pc[test_n,65]:.7f} {weight_pc[test_n,75]:.7f}')\n",
    "print(f'weight stm {weight_stm[test_n,5]:.7f} {weight_stm[test_n,15]:.7f} {weight_stm[test_n,25]:.7f} {weight_stm[test_n,35]:.7f} {weight_stm[test_n,45]:.7f} {weight_stm[test_n,55]:.7f} {weight_stm[test_n,65]:.7f} {weight_stm[test_n,75]:.7f}')\n",
    "print()\n",
    "print(f'pre softmax pc  {preSoftmax_pc[test_n,0]:.7f} {preSoftmax_pc[test_n,1]:.7f} {preSoftmax_pc[test_n,2]:.7f} {preSoftmax_pc[test_n,3]:.7f} {preSoftmax_pc[test_n,4]:.7f} {preSoftmax_pc[test_n,5]:.7f} {preSoftmax_pc[test_n,6]:.7f} {preSoftmax_pc[test_n,7]:.7f}')\n",
    "print(f'pre softmax stm {preSoftmax_stm[test_n,0]:.7f} {preSoftmax_stm[test_n,1]:.7f} {preSoftmax_stm[test_n,2]:.7f} {preSoftmax_stm[test_n,3]:.7f} {preSoftmax_stm[test_n,4]:.7f} {preSoftmax_stm[test_n,5]:.7f} {preSoftmax_stm[test_n,6]:.7f} {preSoftmax_stm[test_n,7]:.7f}')\n",
    "print()\n",
    "print(f'softmax pc  {softmax_pc[test_n,0]:.7f} {softmax_pc[test_n,1]:.7f} {softmax_pc[test_n,2]:.7f} {softmax_pc[test_n,3]:.7f} {softmax_pc[test_n,4]:.7f} {softmax_pc[test_n,5]:.7f} {softmax_pc[test_n,6]:.7f} {softmax_pc[test_n,7]:.7f}')\n",
    "print(f'softmax stm {softmax_stm[test_n,0]:.7f} {softmax_stm[test_n,1]:.7f} {softmax_stm[test_n,2]:.7f} {softmax_stm[test_n,3]:.7f} {softmax_stm[test_n,4]:.7f} {softmax_stm[test_n,5]:.7f} {softmax_stm[test_n,6]:.7f} {softmax_stm[test_n,7]:.7f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search where is the first big difference in PRE SOFTMAC\n",
    "iteratore = np.zeros(8)\n",
    "for j in range(0,8):\n",
    "    for i in range(0, max_dim):\n",
    "        diff = preSoftmax_pc[i,j] - preSoftmax_stm[i,j]\n",
    "        if(diff>1):\n",
    "            iteratore[j]=i\n",
    "            break\n",
    "        \n",
    "print(f'The big difference in the pre softmax is found at iter: {iteratore}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search where is the first big difference in SOFTMAX\n",
    "iteratore = np.zeros(8)\n",
    "for j in range(0,8):\n",
    "    for i in range(0, max_dim):\n",
    "        diff = softmax_pc[i,j] - softmax_stm[i,j]\n",
    "        if(diff>0.0001):\n",
    "            iteratore[j]=i\n",
    "            break\n",
    "        \n",
    "print(f'The big difference in the softmax is found at iter:     {iteratore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK PRE SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myDebug.debug_plotHistoryPreSoftmax(0, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(1, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(2, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(3, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(4, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(5, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(6, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryPreSoftmax(7, preSoftmax_pc, preSoftmax_stm, Model_OL_all_mixed.label, max_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myDebug.debug_plotHistorySoftmax(0, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(1, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(2, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(3, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(4, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(5, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(6, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistorySoftmax(7, softmax_pc, softmax_stm, Model_OL_all_mixed.label, max_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK BIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_prova = max_dim-1\n",
    "myDebug.debug_confrontBias(n_prova, bias_stm, bias_pc, Model_OL_all_mixed.label) # max 771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myDebug.debug_plotHistoryBias(0, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(1, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(2, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(3, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(4, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(5, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(6, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)\n",
    "myDebug.debug_plotHistoryBias(7, bias_stm, bias_pc, Model_OL_all_mixed.label, max_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prova = max_dim-1\n",
    "\n",
    "#myDebug.debug_confrontWeights(n_prova, weight_stm, weight_pc, weight_num, selected_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myDebug.debug_plotHistoryWeight(10, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(5, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(22, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(75, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(66, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(47, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(33, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(64, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(54, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(71, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(41, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(23, weight_stm, weight_pc, max_dim)\n",
    "myDebug.debug_plotHistoryWeight(35, weight_stm, weight_pc, max_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK FROZEN OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 0\n",
    "itr_1   = 0\n",
    "itr_2   = 0\n",
    "for i in range(61,max_dim):\n",
    "    diff = frozenOut_pc[i,:] - frozenOut_stm[i,:]\n",
    "    for j in range(0, len(diff)):\n",
    "        diff[j] = np.abs(diff[j])\n",
    "    if(max(diff)> max_val):\n",
    "        max_val = max(diff)\n",
    "        itr_1 = i\n",
    "        itr_2 = np.argmax(diff)\n",
    "        \n",
    "print(f'The max values of difference of all {frozenOut_stm.shape[0]} iteration is: {S_BOLD}{max_val:.11f}{E_BOLD}')\n",
    "print(f'The max difference is found at the iteration number: {S_BOLD}{itr_1}{E_BOLD}')\n",
    "print(f'At the position in the frozen output: {S_BOLD}{itr_2}/128{E_BOLD}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDebug.plot_frozenDifference(88, frozenOut_pc, frozenOut_stm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_container = np.zeros(frozenOut_stm.shape)\n",
    "\n",
    "for j in range(0, frozenOut_stm.shape[0]):\n",
    "    for i in range(0, frozenOut_stm.shape[1]):\n",
    "        max_val = max(frozenOut_pc[j,i], frozenOut_pc[j,i])\n",
    "        if(max_val != 0):\n",
    "            diff = frozenOut_pc[j,i]-frozenOut_stm[j,i]\n",
    "            new_container[j,i] = diff/max_val\n",
    "          \n",
    "        \n",
    "max_val =0\n",
    "for j in range(0, frozenOut_stm.shape[0]):\n",
    "    if(max_val < max(new_container[i,:])):\n",
    "        max_val = max(new_container[i,:])\n",
    "        \n",
    "print(f'The max percentage error committed between stm and laptop is of: {max_val:.11f}')\n",
    "print(f'Percentage at position, 237, 79 is {new_container[237,79]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
