{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import optimizers\n",
    "from PIL import Image\n",
    "\n",
    "import myTestingPlot as myPlotT\n",
    "import myDatasetPlot as myPlotD\n",
    "import myDatasetParse as myParse\n",
    "import myWriteFile as myWrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['A' 'E' 'I' 'O' 'U']\n",
      "\n",
      "Raw shape        -> (180000, 5)\n",
      "Columns          -> ['acquisition', 'letter', 'ax', 'ay', 'az']\n",
      "\n",
      "Tot samples      -> 900\n",
      "1 Sample is long -> 200\n",
      "\n",
      "\n",
      "**** OL data\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (101, 600)\n",
      "Train label shape -> (101,)\n",
      "\n",
      "Test data shape   -> (42, 600)\n",
      "Test label shape  -> (42,)\n",
      "\n",
      "**** TF data\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (529, 600)\n",
      "Train label shape -> (529,)\n",
      "\n",
      "Test data shape   -> (226, 600)\n",
      "Test label shape  -> (226,)\n"
     ]
    }
   ],
   "source": [
    "tmp1, tmp2 = myParse.loadDataFromTxt('augmented_vowels')\n",
    "\n",
    "# Shuffle the matrix of all letters\n",
    "vowels_data = np.zeros(tmp1.shape)\n",
    "vowels_label = np.empty(tmp2.shape, dtype=str) \n",
    "\n",
    "index_ary = list(range(0, tmp1.shape[0]))\n",
    "index_ary = random.sample(index_ary, len(index_ary)) \n",
    "\n",
    "for i in range(0, tmp1.shape[0]):\n",
    "    vowels_data[i,:] = tmp1[index_ary[i],:]\n",
    "    vowels_label[i]  = tmp2[index_ary[i]]\n",
    "    \n",
    "# Separate in 60% and 40% for training the TF model and the OL model\n",
    "sep = int((vowels_data.shape[0])*0.16)\n",
    "\n",
    "print('\\n**** OL data')\n",
    "OL_data  = vowels_data[:sep, :]\n",
    "OL_label = vowels_label[:sep]\n",
    "OL_data_train_vow, OL_label_train_vow, OL_data_test_vow, OL_label_test_vow = myParse.parseTrainValid(OL_data, OL_label)\n",
    "\n",
    "\n",
    "print('\\n**** TF data')\n",
    "TF_data  = vowels_data[sep:, :]\n",
    "TF_label = vowels_label[sep:]\n",
    "TF_data_train, TF_label_train, TF_data_test, TF_label_test = myParse.parseTrainValid(TF_data, TF_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['B']\n",
      "\n",
      "Raw shape        -> (29400, 5)\n",
      "Columns          -> ['acquisition', 'letter', 'ax', 'ay', 'az']\n",
      "\n",
      "Tot samples      -> 147\n",
      "1 Sample is long -> 200\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (102, 600)\n",
      "Train label shape -> (102,)\n",
      "\n",
      "Test data shape   -> (43, 600)\n",
      "Test label shape  -> (43,)\n"
     ]
    }
   ],
   "source": [
    "B_data, B_label = myParse.loadDataFromTxt('B_dataset')\n",
    "B_train_data, B_train_label, B_test_data, B_test_label = myParse.parseTrainValid(B_data, B_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['M']\n",
      "\n",
      "Raw shape        -> (29000, 5)\n",
      "Columns          -> ['acquisition', 'letter', 'ax', 'ay', 'az']\n",
      "\n",
      "Tot samples      -> 145\n",
      "1 Sample is long -> 200\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (101, 600)\n",
      "Train label shape -> (101,)\n",
      "\n",
      "Test data shape   -> (42, 600)\n",
      "Test label shape  -> (42,)\n"
     ]
    }
   ],
   "source": [
    "M_data, M_label = myParse.loadDataFromTxt('M_dataset')\n",
    "M_train_data, M_train_label, M_test_data, M_test_label = myParse.parseTrainValid(M_data, M_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Dataset for letter ['R']\n",
      "\n",
      "Raw shape        -> (29000, 5)\n",
      "Columns          -> ['acquisition', 'letter', 'ax', 'ay', 'az']\n",
      "\n",
      "Tot samples      -> 145\n",
      "1 Sample is long -> 200\n",
      "\n",
      "\n",
      "*** Separate train-valid\n",
      "\n",
      "Train data shape  -> (101, 600)\n",
      "Train label shape -> (101,)\n",
      "\n",
      "Test data shape   -> (42, 600)\n",
      "Test label shape  -> (42,)\n"
     ]
    }
   ],
   "source": [
    "R_data, R_label = myParse.loadDataFromTxt('R_dataset')\n",
    "R_train_data, R_train_label, R_test_data, R_test_label = myParse.parseTrainValid(R_data, R_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset of all letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New dataset of ordered data has shape (405, 600)\n",
      "New dataset of ordered label has shape(405,)\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix that contains all the train data\n",
    "\n",
    "order_data_all = OL_data_train_vow\n",
    "order_data_all = np.vstack(( order_data_all, B_train_data))\n",
    "order_data_all = np.vstack(( order_data_all, R_train_data))\n",
    "order_data_all = np.vstack(( order_data_all, M_train_data))\n",
    "\n",
    "order_label_all = OL_label_train_vow\n",
    "order_label_all = np.hstack(( order_label_all, B_train_label))\n",
    "order_label_all = np.hstack(( order_label_all, R_train_label))\n",
    "order_label_all = np.hstack(( order_label_all, M_train_label))\n",
    "\n",
    "print('\\nNew dataset of ordered data has shape ' + str(order_data_all.shape))\n",
    "print('New dataset of ordered label has shape' + str(order_label_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the matrix of all letters\n",
    "mixed_data_all = np.zeros(order_data_all.shape)\n",
    "mixed_label_all = np.empty(order_label_all.shape, dtype=str) \n",
    "\n",
    "index_ary = list(range(0, order_data_all.shape[0]))\n",
    "index_ary = random.sample(index_ary, len(index_ary)) \n",
    "\n",
    "for i in range(0, order_data_all.shape[0]):\n",
    "    mixed_data_all[i,:] = order_data_all[index_ary[i],:]\n",
    "    mixed_label_all[i]  = order_label_all[index_ary[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset of only new letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New dataset of mixed data has shape (304, 600)\n",
      "New dataset of mixed label has shape(304,)\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix that contains all the train data\n",
    "\n",
    "order_data_new = B_train_data\n",
    "order_data_new = np.vstack(( order_data_new, R_train_data))\n",
    "order_data_new = np.vstack(( order_data_new, M_train_data))\n",
    "\n",
    "order_label_new = B_train_label\n",
    "order_label_new = np.hstack(( order_label_new, R_train_label))\n",
    "order_label_new = np.hstack(( order_label_new, M_train_label))\n",
    "\n",
    "print('\\nNew dataset of mixed data has shape ' + str(order_data_new.shape))\n",
    "print('New dataset of mixed label has shape' + str(order_label_new.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the matrix of all letters\n",
    "mixed_data_new = np.zeros(order_data_new.shape)\n",
    "mixed_label_new = np.empty(order_label_new.shape, dtype=str) \n",
    "\n",
    "index_ary = list(range(0, order_data_new.shape[0]))\n",
    "index_ary = random.sample(index_ary, len(index_ary)) \n",
    "\n",
    "for i in range(0, order_data_new.shape[0]):\n",
    "    mixed_data_new[i,:] = order_data_new[index_ary[i],:]\n",
    "    mixed_label_new[i]  = order_label_new[index_ary[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Container(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.R_test_data       = R_test_data\n",
    "        self.R_test_label      = R_test_label\n",
    "        self.B_test_data       = B_test_data\n",
    "        self.B_test_label      = B_test_label\n",
    "        self.M_test_data       = M_test_data\n",
    "        self.M_test_label      = M_test_label\n",
    "        self.R_test_data       = R_test_data\n",
    "        self.OL_data_test_vow  = OL_data_test_vow\n",
    "        self.OL_label_test_vow = OL_label_test_vow\n",
    "        \n",
    "OL_testing_data = Data_Container()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lettToSoft(ary, labels):\n",
    "    ret_ary = np.zeros([len(ary), len(labels)])\n",
    "    \n",
    "    for i in range(0, len(ary)):\n",
    "        for j in range(0, len(labels)):\n",
    "            if(ary[i]==labels[j]):\n",
    "                ret_ary[i,j] = 1\n",
    "\n",
    "            \n",
    "    return ret_ary   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic parameters\n",
    "optimizer = 'Adam'\n",
    "loss    = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "vowels = ['A', 'E', 'I', 'O', 'U']\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model structure\n",
    "\n",
    "# Buoni risultati con 3 layer da 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation = 'relu', input_shape =(TF_data_train.shape[1],),name='input_layer'))\n",
    "#model.add(Dense(128, activation = 'relu', name='hidden1'))\n",
    "model.add(Dense(300, activation = 'relu', name='hidden2'))\n",
    "model.add(Dense(5, activation='softmax' , name = 'output_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (Dense)          (None, 128)               76928     \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 300)               38700     \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 5)                 1505      \n",
      "=================================================================\n",
      "Total params: 117,133\n",
      "Trainable params: 117,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer= optimizer, loss=loss, metrics=metrics) #use sparse is each letter is an integer (es a->1 b->2 c->3 ..)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "27/27 - 1s - loss: 152.4297 - accuracy: 0.5343 - val_loss: 38.5472 - val_accuracy: 0.7264\n",
      "Epoch 2/20\n",
      "27/27 - 0s - loss: 20.2493 - accuracy: 0.8014 - val_loss: 21.3206 - val_accuracy: 0.8396\n",
      "Epoch 3/20\n",
      "27/27 - 0s - loss: 9.8617 - accuracy: 0.8936 - val_loss: 13.8802 - val_accuracy: 0.8585\n",
      "Epoch 4/20\n",
      "27/27 - 0s - loss: 4.7147 - accuracy: 0.9385 - val_loss: 10.6132 - val_accuracy: 0.9245\n",
      "Epoch 5/20\n",
      "27/27 - 0s - loss: 4.1019 - accuracy: 0.9433 - val_loss: 9.4374 - val_accuracy: 0.9245\n",
      "Epoch 6/20\n",
      "27/27 - 0s - loss: 3.9372 - accuracy: 0.9433 - val_loss: 5.6411 - val_accuracy: 0.9434\n",
      "Epoch 7/20\n",
      "27/27 - 0s - loss: 1.2395 - accuracy: 0.9787 - val_loss: 3.1562 - val_accuracy: 0.9717\n",
      "Epoch 8/20\n",
      "27/27 - 0s - loss: 0.6287 - accuracy: 0.9858 - val_loss: 6.9907 - val_accuracy: 0.9340\n",
      "Epoch 9/20\n",
      "27/27 - 0s - loss: 2.5494 - accuracy: 0.9598 - val_loss: 13.8993 - val_accuracy: 0.8491\n",
      "Epoch 10/20\n",
      "27/27 - 0s - loss: 4.2083 - accuracy: 0.9385 - val_loss: 6.4131 - val_accuracy: 0.9623\n",
      "Epoch 11/20\n",
      "27/27 - 0s - loss: 2.3753 - accuracy: 0.9598 - val_loss: 4.1863 - val_accuracy: 0.9623\n",
      "Epoch 12/20\n",
      "27/27 - 0s - loss: 1.3081 - accuracy: 0.9693 - val_loss: 3.8468 - val_accuracy: 0.9528\n",
      "Epoch 13/20\n",
      "27/27 - 0s - loss: 2.5694 - accuracy: 0.9551 - val_loss: 7.6420 - val_accuracy: 0.9434\n",
      "Epoch 14/20\n",
      "27/27 - 0s - loss: 4.2454 - accuracy: 0.9574 - val_loss: 6.6990 - val_accuracy: 0.9057\n",
      "Epoch 15/20\n",
      "27/27 - 0s - loss: 2.7442 - accuracy: 0.9764 - val_loss: 7.3223 - val_accuracy: 0.9245\n",
      "Epoch 16/20\n",
      "27/27 - 0s - loss: 1.1344 - accuracy: 0.9787 - val_loss: 2.1328 - val_accuracy: 0.9623\n",
      "Epoch 17/20\n",
      "27/27 - 0s - loss: 1.3310 - accuracy: 0.9835 - val_loss: 4.3926 - val_accuracy: 0.9717\n",
      "Epoch 18/20\n",
      "27/27 - 0s - loss: 1.5763 - accuracy: 0.9693 - val_loss: 13.9823 - val_accuracy: 0.8679\n",
      "Epoch 19/20\n",
      "27/27 - 0s - loss: 3.4315 - accuracy: 0.9551 - val_loss: 5.5534 - val_accuracy: 0.9528\n",
      "Epoch 20/20\n",
      "27/27 - 0s - loss: 1.1274 - accuracy: 0.9740 - val_loss: 14.9271 - val_accuracy: 0.8774\n",
      "\n",
      "Evaluation:\n",
      "8/8 - 0s - loss: 19.0773 - accuracy: 0.8230\n"
     ]
    }
   ],
   "source": [
    "# Perform training\n",
    "train_hist = model.fit(TF_data_train, lettToSoft(TF_label_train, vowels), epochs=epochs, batch_size=batch_size, validation_split=0.2 , verbose=2)\n",
    "print('\\nEvaluation:')\n",
    "results = model.evaluate(TF_data_test, lettToSoft(TF_label_test, vowels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss = train_hist.history['loss']\n",
    "hist_val_loss = train_hist.history['val_loss']\n",
    "epoch_list = list(range(epochs))\n",
    "plt.figure(1)\n",
    "plt.plot(epoch_list, hist_loss, 'bo', label='Training loss')\n",
    "plt.plot(epoch_list, hist_val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on random vowel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True       -> A\n",
      "Prediction -> A\n"
     ]
    }
   ],
   "source": [
    "rand_n = int(random.uniform(0, TF_data_test.shape[0]))\n",
    "sample_data = TF_data_test[rand_n,:].reshape(1,TF_data_test.shape[1])\n",
    "sample_label = TF_label_test[rand_n]\n",
    "\n",
    "pred = model.predict(sample_data)\n",
    "print(f'True       -> {sample_label}')\n",
    "print(f'Prediction -> {vowels[np.argmax(pred)]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct guesses 186  -> 82.0%\n",
      "Total mistaken guesses 40 -> 18.0%\n"
     ]
    }
   ],
   "source": [
    "myPlotT.plotTest(TF_data_test, TF_label_test, model, vowels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveParams(SAVE_MODEL_PATH, model):\n",
    "    \n",
    "    new_file = open(SAVE_MODEL_PATH + '/params.txt', \"w\")\n",
    "\n",
    "    new_file.write(\"PARAMETERS SAVED FROM THE TRAINING\")\n",
    "    new_file.write(\"\\n Batch size: \" + str(batch_size))\n",
    "    new_file.write(\"\\n Epochs: \" + str(epochs))\n",
    "    new_file.write(\"\\n Validation split: \" + str(0.2))\n",
    "    new_file.write(\"\\n Metrics: \" + str(metrics))\n",
    "    new_file.write(\"\\n Optimizer: \" + optimizer)\n",
    "    new_file.write(\"\\n Loss: \" + loss + \"\\n\\n\")\n",
    "\n",
    "    model.summary(print_fn=lambda x: new_file.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODEL_PATH = \"C:/Users/massi/UNI/Magistrale/Anno 5/Semestre 2/Tesi/Code/Python/Saved_models/model/\"\n",
    "model.save(SAVE_MODEL_PATH + \"model.h5\")\n",
    "saveParams(SAVE_MODEL_PATH, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "learn_rate = 0.0003\n",
    "lam = 0.4\n",
    "batch_size_new = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Layer(object):\n",
    "    def __init__(self, model):\n",
    "\n",
    "        self.ML_frozen = keras.models.Sequential(model.layers[:-1])\n",
    "        self.ML_frozen.compile()\n",
    "        self.W = np.array(model.layers[-1].get_weights()[0])\n",
    "        self.b = np.array(model.layers[-1].get_weights()[1])\n",
    "        self.label = ['A', 'E', 'I', 'O', 'U']\n",
    "        \n",
    "        self.width = self.W.shape[0]\n",
    "\n",
    "    def predict(self, x):\n",
    "        mat_prod = np.matmul(x, self.W) + self.b\n",
    "        return tf.nn.softmax(mat_prod)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkNewClass(model, y_true, y_true_soft, i, gauss):\n",
    "    \n",
    "    # Check if letter is new\n",
    "    found = 0\n",
    "    for k in range(0, len(model.label)):\n",
    "        if (y_true[i] == model.label[k]):\n",
    "            found = 1\n",
    "\n",
    "    # If first time seeing this letter\n",
    "    if (found == 0):\n",
    "\n",
    "        model.label.append(y_true[i])   # Add new letter to label\n",
    "        print(f'\\n\\n    New letter detected -> letter \\033[1m{y_true[i]}\\033[0m \\n')\n",
    "    \n",
    "        if(gauss==0):\n",
    "            model.W = np.hstack((model.W, np.zeros([model.width,1])))\n",
    "            model.b = np.hstack((model.b, np.zeros([1])))\n",
    "        elif(gauss==1):\n",
    "            gaussW = np.zeros([model.width,1])\n",
    "            for i in range(0,model.width):\n",
    "                gaussW[i,0] = random.gauss(0, 0.01)\n",
    "            gaussB = random.gauss(0, 0.01)\n",
    "            model.W = np.hstack((model.W, gaussW))\n",
    "            model.b = np.hstack((model.b, gaussB))\n",
    "        elif(gauss==2):  # sporcariaper far funzionare LWF\n",
    "            model.W = np.hstack((model.W, np.zeros([model.width,1])))\n",
    "            model.b = np.hstack((model.b, np.zeros([1])))\n",
    "            \n",
    "                   \n",
    "        print(f'    Now W is -> {model.W.shape}              and b is -> {model.b.shape}\\n\\n')\n",
    "\n",
    "        y_true_soft = lettToSoft(y_true, model.label)\n",
    "                \n",
    "    return y_true_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL(model, x, y_true, learn_rate):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with OL METHOD - STOCHASTICH\\n')\n",
    "   \n",
    "    cntr=1\n",
    "    tot_samples = x.shape[0]\n",
    "    \n",
    "    y_true_soft = lettToSoft(y_true, model.label)  # Transform the true label letters in softmax\n",
    "        \n",
    "    # Cycle over all samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        y_true_soft = checkNewClass(model, y_true, y_true_soft, i, 0)   # Check if letter is new\n",
    "        \n",
    "        # PPREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])        \n",
    "          \n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft[i,:]\n",
    "\n",
    "        for j in range(0,model.W.shape[0]):\n",
    "            # Update weights\n",
    "            deltaW = np.multiply(cost, y_ML[0,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "        # Update biases\n",
    "        db = np.multiply(cost, learn_rate)\n",
    "        model.b = model.b-db\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL_miniBatch(model, x, y_true, learn_rate, batch_size):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with OL METHOD - MINI BATCH\\n')\n",
    "    \n",
    "    cntr=1\n",
    "    tot_samples = x.shape[0]\n",
    "    sum_gradW = np.zeros([model.W.shape[0], 8])\n",
    "    sum_gradB = np.zeros([1, 8])\n",
    "    \n",
    "    y_true_soft = lettToSoft(y_true, model.label)     # Transform the true label letters in softmax array\n",
    "        \n",
    "    # Cycle over all samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        y_true_soft = checkNewClass(model, y_true, y_true_soft, i, 0)   # Check if letter is new\n",
    "        \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "        \n",
    "        if(i%batch_size==0):\n",
    "                model.W = model.W - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,:w]\n",
    "                model.b = model.b - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,:w]\n",
    "\n",
    "                sum_gradW = np.zeros([h, 8])  #reset each batch  \n",
    "                sum_gradB = np.zeros([1, 8])  #reset each batch   \n",
    "        \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft[i,:]\n",
    "\n",
    "        for j in range(0,h): \n",
    "            # Update weights\n",
    "            tmp = np.multiply(cost, y_ML[0,j]) \n",
    "            deltaW = np.zeros([1,8])\n",
    "            deltaW[0,:w] = tmp  \n",
    "            sum_gradW[j,:] += deltaW[0,:]\n",
    "\n",
    "        # Update biases\n",
    "        deltaB = np.zeros([1,8])\n",
    "        deltaB[0,:w] = cost\n",
    "        sum_gradB += deltaB\n",
    "\n",
    "        # If last iteration\n",
    "        if(i==tot_samples-1):\n",
    "            model.W = model.W - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,:w]\n",
    "            model.b = model.b - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,:w]\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def trainOneEpoch_OL_v2(model, x, y_true, learn_rate):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with CWR METHOD - STOCASTICH \\n')\n",
    "    \n",
    "    cntr=1\n",
    "    tot_samples = x.shape[0]\n",
    "    \n",
    "    y_true_soft = lettToSoft(y_true, model.label)    # Transform the true label letters in softmax\n",
    "            \n",
    "    # Cycle over every sample\n",
    "    for i in range(0, tot_samples):\n",
    "        y_true_soft = checkNewClass(model, y_true, y_true_soft, i, 1)    # Check if letter is new\n",
    "        \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft[i,:]  \n",
    "\n",
    "        for j in range(0,model.W.shape[0]):\n",
    "            # Update weights\n",
    "            deltaW = np.multiply(cost, y_ML[0,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            model.W[j,5:] = model.W[j,5:]-dW[5:]\n",
    "\n",
    "        # Update biases\n",
    "        db = np.multiply(cost, learn_rate)\n",
    "        model.b[5:] = model.b[5:]-db[5:]\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_OL_v2_miniBatch(model, x, y_true, learn_rate, batch_size):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with CWR - MINI BATCH \\n ')  \n",
    "\n",
    "    cntr=1\n",
    "    tot_samples = x.shape[0]\n",
    "    sum_gradW = np.zeros([model.W.shape[0], 8])\n",
    "    sum_gradB = np.zeros([1, 8])\n",
    "    \n",
    "    # Transform the true label letters in softmax array\n",
    "    y_true_soft = lettToSoft(y_true, model.label)\n",
    "           \n",
    "    # Cycle over all input samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        y_true_soft = checkNewClass(model, y_true, y_true_soft, i, 0) # Check if letter is new\n",
    "        \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "        \n",
    "        # If beginning of batch\n",
    "        if(i%batch_size==0):\n",
    "                model.W[:,5:] = model.W[:,5:] - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,5:w]\n",
    "                model.b[5:]   = model.b[5:]   - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,5:w]\n",
    "                sum_gradW = np.zeros([h, 8])  # reset\n",
    "                sum_gradB = np.zeros([1, 8])  # reset\n",
    "            \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred-y_true_soft[i,:]\n",
    "\n",
    "        for j in range(0,h):  \n",
    "            # Update weights\n",
    "            tmp = np.multiply(cost, y_ML[0,j]) \n",
    "            deltaW = np.zeros([1,8])\n",
    "            deltaW[0,:tmp.shape[0]] = tmp  \n",
    "            sum_gradW[j,:] += deltaW[0,:]\n",
    "\n",
    "        # Update biases\n",
    "        deltaB = np.zeros([1,8])\n",
    "        deltaB[0,:cost.shape[0]] = cost\n",
    "        sum_gradB += deltaB\n",
    "\n",
    "        # If last iteration\n",
    "        if(i==tot_samples-1):\n",
    "            model.W[:,5:] = model.W[:,5:] - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h,5:w]\n",
    "            model.b[5:]   = model.b[5:]   - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,5:w]\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpochOL_LWF(model, x, y_true, learn_rate):\n",
    "    \n",
    "    print('**********************************')\n",
    "    print('Performing training with OL METHOD - LWF')\n",
    "    print()\n",
    "    \n",
    "    lam  = 0\n",
    "    cntr = 1\n",
    "    tot_samples = x.shape[0]\n",
    "    y_LWF = np.zeros([tot_samples, 8])    # Define container for LWF\n",
    "\n",
    "    # Perform nitial prediciton for LWF\n",
    "    print('   Performing prediction of all dataset')\n",
    "    for u in range(0, x.shape[0]):\n",
    "        y_ML = model.ML_frozen.predict(x[u,:].reshape(1,x.shape[1]))\n",
    "        y_LWF[u,:5] = model.predict(y_ML[0,:])\n",
    "    \n",
    "    \n",
    "    y_true_soft = lettToSoft(y_true, model.label)    # Transform the true label letters in softmax\n",
    "         \n",
    "    # Cycle over every sample\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        y_true_soft = checkNewClass(model, y_true, y_true_soft, i, 2)   # Check if letter is new\n",
    "        \n",
    "        w = model.W.shape[1]\n",
    "        h = model.W.shape[0]\n",
    "        if(w==6):\n",
    "            lam = 2/3\n",
    "        elif(w==7):\n",
    "            lam = 3/4\n",
    "        elif(w==8):\n",
    "            lam = 4/5\n",
    "            \n",
    "             \n",
    "        # PREDICTION\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "      \n",
    "        \n",
    "        # BACKPROPAGATION        \n",
    "        cost_norm = y_pred-y_true_soft[i,:]\n",
    "        cost_LWF  = y_pred-y_LWF[i,:w]\n",
    "\n",
    "        for j in range(0,h):\n",
    "            # Update weights\n",
    "            deltaW_norm = np.multiply(cost_norm,1-lam)\n",
    "            deltaW_LWF  = np.multiply(cost_LWF, lam)\n",
    "            deltaW      = np.multiply(deltaW_norm+deltaW_LWF, y_ML[0,j])\n",
    "            dW          = np.multiply(deltaW, learn_rate)\n",
    "\n",
    "            model.W[j,:] = model.W[j,:]-dW\n",
    "\n",
    "            # Update biases\n",
    "            db_norm = np.multiply(cost_norm, 1-lam)\n",
    "            db_LWF  = np.multiply(cost_LWF, lam)\n",
    "            db      = np.multiply(db_norm+db_LWF, learn_rate)\n",
    "            model.b = model.b-db\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpochOL_LWF_v2(model, x, y_true, learn_rate, batch_size):\n",
    "    \n",
    "    print('**********************************\\nPerforming training with OL METHOD - LWF with MINI BATCH\\n')\n",
    "    \n",
    "    lam  = 0\n",
    "    cntr = 1\n",
    "    tot_samples = x.shape[0]\n",
    "    # Transform the true label letters in softmax array\n",
    "    y_true_soft = lettToSoft(y_true, model.label)\n",
    "        \n",
    "        \n",
    "    # Initialize now and then reset it once every batch_size\n",
    "    sum_gradW = np.zeros([model.W.shape[0], 8])\n",
    "    sum_gradB = np.zeros([1, 8])\n",
    "    \n",
    "    # Define a matrix that I can then fill with the initial inference\n",
    "    y_LWF = np.zeros([ x.shape[0], 8])\n",
    "    \n",
    "    # For every sample in the dataset given\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        \n",
    "        # Check if letter is new\n",
    "        y_true_soft = checkNewClass(model, y_true, y_true_soft, i, 0)\n",
    "        \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "        if(w==6):\n",
    "            lam = 2/3\n",
    "        elif(w==7):\n",
    "            lam = 3/4\n",
    "        elif(w==8):\n",
    "            lam = 4/5\n",
    "        \n",
    "        # Reset the matrices that keep track of the summation of gradient\n",
    "        if(i%batch_size==0):\n",
    "                model.W = model.W - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h, :w]\n",
    "                model.b = model.b - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,:w]\n",
    "            \n",
    "                sum_gradW = np.zeros([h, 8])\n",
    "                sum_gradB = np.zeros([1, 8])\n",
    "                \n",
    "                # Value that avoids the last prediction of LWF to be outside of dataset dimension\n",
    "                limit = 0\n",
    "                if(i+batch_size > tot_samples):\n",
    "                    limit = i+batch_size-tot_samples\n",
    "                    \n",
    "                \n",
    "                for k in range(0, batch_size-limit):\n",
    "                    # Prediction from ML frozen model\n",
    "                    y_ML = model.ML_frozen.predict(x[i+k,:].reshape(1,x.shape[1]))\n",
    "                    # Prediction from LWF \n",
    "                    y_LWF[k,:w] = model.predict(y_ML[0,:])\n",
    "      \n",
    "        # Prediction from ML frozen model\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        # Prediction from TinyOL layer\n",
    "        y_pred = model.predict(y_ML[0,:])\n",
    "         \n",
    "        # ---- BACKPROPAGATION | MINI BATCH + LWF\n",
    "        cost_norm = y_pred-y_true_soft[i,:]\n",
    "        cost_LWF  = y_pred-y_LWF[i,:w]\n",
    "\n",
    "        \n",
    "\n",
    "        for j in range(0,h):  # da 0 a 300\n",
    "\n",
    "            # Update weights\n",
    "            tmp_norm     = np.multiply(cost_norm, 1-lam)\n",
    "            tmp_LWF      = np.multiply(cost_LWF,  lam)\n",
    "            if(len(tmp_LWF)>5):\n",
    "                tmp_LWF[5] = 0\n",
    "            elif(len(tmp_LWF)>6):\n",
    "                tmp_LWF[6] = 0\n",
    "            elif(len(tmp_LWF)>7):\n",
    "                tmp_LWF[7] = 0\n",
    "            tmp_tot      = np.multiply(tmp_norm+tmp_LWF, y_ML[0,j])\n",
    "            deltaW       = deltaW = np.zeros([1,8])\n",
    "            deltaW[0,:w] = tmp_tot \n",
    "            \n",
    "            sum_gradW[j,:] += deltaW[0,:]            \n",
    "            \n",
    "\n",
    "\n",
    "            # Update biases\n",
    "            db_norm = np.multiply(cost_norm, 1-lam)\n",
    "            db_LWF  = np.multiply(cost_LWF,  lam)    \n",
    "            deltaB = np.zeros([1,8])\n",
    "            deltaB[0,:w] = db_norm+db_LWF\n",
    "            \n",
    "            sum_gradB += deltaB\n",
    "            \n",
    "        # if is last iteration , update the matrix\n",
    "        if(i==tot_samples-1):\n",
    "            model.W = model.W - np.multiply(sum_gradW, 1/batch_size*learn_rate)[:h, :w]\n",
    "            model.b = model.b - np.multiply(sum_gradB, 1/batch_size*learn_rate)[0,:w]\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneEpoch_CWR(model, x, y_true, learn_rate, batch_size):\n",
    "        \n",
    "    print('**********************************\\nPerforming training CWR \\n ')  \n",
    "\n",
    "    cntr=1\n",
    "    tot_samples = x.shape[0]\n",
    "    TW = np.zeros([model.W.shape[0], 8])\n",
    "    TB = np.zeros([1, 8])\n",
    "    found_lett = np.zeros([1,8])\n",
    "    \n",
    "    y_true_soft = lettToSoft(y_true, model.label)    # Transform the true label letters in softmax array\n",
    "           \n",
    "    # Cycle over all input samples\n",
    "    for i in range(0, tot_samples):\n",
    "        \n",
    "        y_true_soft = checkNewClass(model, y_true, y_true_soft, i, 0) # Check if letter is new\n",
    "        \n",
    "        h = model.W.shape[0]\n",
    "        w = model.W.shape[1]\n",
    "        \n",
    "        dummy = 0\n",
    "        \n",
    "        # If beginning of batch\n",
    "        if(i%batch_size==0 and i!=0): \n",
    "            for k in range(0, w):\n",
    "                if(found_lett[0,k]==0):\n",
    "                    dummy = 0\n",
    "                    #model.W[:,k] = np.copy(TW[:,k])\n",
    "                    #model.b[k]   = np.copy(TB[0,k])\n",
    "                else:\n",
    "                    tempW = np.multiply(model.W[:,k], found_lett[0,k])\n",
    "                    tempB = np.multiply(model.b[k], found_lett[0,k])\n",
    "                    model.W[:,k] = np.multiply(tempW+TW[:,k], 1/(found_lett[0,k]+1))\n",
    "                    model.b[k]   = np.multiply(tempB+TB[0,k], 1/(found_lett[0,k]+1))\n",
    "                \n",
    "            # reset TW nd TB with gaussian values\n",
    "            for k in range(0, 8):\n",
    "                TB[0, k] = random.gauss(0, 0.01)\n",
    "                for l in range(0, h):\n",
    "                    TW[l,k] = random.gauss(0, 0.01)\n",
    "            \n",
    "            #found_lett = np.zeros([1,8])  # reset\n",
    "        elif(i==0):\n",
    "            TW = np.zeros([h, 8])   # reset  \n",
    "            TB = np.zeros([1, 8])   # reset     \n",
    "                \n",
    "        found_lett[0,np.argmax(y_true_soft[i,:])] += 1  # update the letter counter\n",
    "            \n",
    "        # PREDICTION ON THE MATRIX TW AND TB\n",
    "        y_ML = model.ML_frozen.predict(x[i,:].reshape(1,x.shape[1]))\n",
    "        y_pred = tf.nn.softmax(np.matmul(y_ML, TW) + TB)\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        cost = y_pred[0,:w]-y_true_soft[i,:]\n",
    "\n",
    "        for j in range(0,h):  # da 0 a 300\n",
    "            deltaW = np.multiply(cost, y_ML[0,j])\n",
    "            dW = np.multiply(deltaW, learn_rate)\n",
    "            TW[j,:w] = TW[j,:w] - dW\n",
    "\n",
    "        # Update biases\n",
    "        db = np.multiply(cost, learn_rate)\n",
    "        TB[0,:w] = TB[0,:w]-db\n",
    "\n",
    "        # If last iteration\n",
    "        if(i==tot_samples-1):\n",
    "            for k in range(5, w):\n",
    "                if(found_lett[0,k]==0):\n",
    "                    dummy = 0\n",
    "                    #model.W[:,k] = np.copy(TW[:,k])\n",
    "                    #model.b[k]   = np.copy(TB[0,k])\n",
    "                else:\n",
    "                    tempW = np.multiply(model.W[:,k], found_lett[0,k])\n",
    "                    tempB = np.multiply(model.b[k], found_lett[0,k])\n",
    "                    model.W[:,k] = np.multiply(tempW+TW[:,k], 1/(found_lett[0,k]+1))\n",
    "                    model.b[k]   = np.multiply(tempB+TB[0,k], 1/(found_lett[0,k]+1))\n",
    "        \n",
    "        print(f\"\\r    Currently at {np.round(np.round(cntr/x.shape[0],4)*100,2)}% of dataset\", end=\"\")\n",
    "        cntr +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ML model (cut model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (Dense)          (None, 128)               76928     \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 300)               38700     \n",
      "=================================================================\n",
      "Total params: 115,628\n",
      "Trainable params: 115,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ML_model = keras.models.Sequential(model.layers[:-1])\n",
    "ML_model.summary()\n",
    "ML_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODEL_CUT_PATH = \"C:/Users/massi/UNI/Magistrale/Anno 5/Semestre 2/Tesi/Code/Python/Saved_models/Frozen_model/\"\n",
    "ML_model.save(SAVE_MODEL_CUT_PATH + \"model.h5\")\n",
    "saveParams(SAVE_MODEL_CUT_PATH, ML_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL only on vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Performing training with OL METHOD - STOCHASTICH\n",
      "\n",
      "    Currently at 100.0% of dataset"
     ]
    }
   ],
   "source": [
    "Model_OL_vowels = Custom_Layer(model)\n",
    "for i in range(0, num_epochs):\n",
    "    trainOneEpoch_OL(Model_OL_vowels, OL_data_train_vow, OL_label_train_vow, learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Performing training with OL METHOD - LWF\n",
      "\n",
      "   Performing prediction of all dataset\n",
      "\n",
      "\n",
      "    New letter detected -> letter \u001b[1mM\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 6)              and b is -> (6,)\n",
      "\n",
      "\n",
      "    Currently at 0.25% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mB\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 7)              and b is -> (7,)\n",
      "\n",
      "\n",
      "    Currently at 0.49% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mR\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 8)              and b is -> (8,)\n",
      "\n",
      "\n",
      "    Currently at 100.0% of dataset"
     ]
    }
   ],
   "source": [
    "Model_LWF_1 = Custom_Layer(model)\n",
    "for i in range(0, num_epochs):\n",
    "    trainOneEpochOL_LWF(Model_LWF_1, mixed_data_all, mixed_label_all, learn_rate)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LWF + mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Performing training with OL METHOD - LWF with MINI BATCH\n",
      "\n",
      "\n",
      "\n",
      "    New letter detected -> letter \u001b[1mM\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 6)              and b is -> (6,)\n",
      "\n",
      "\n",
      "    Currently at 0.25% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mB\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 7)              and b is -> (7,)\n",
      "\n",
      "\n",
      "    Currently at 0.49% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mR\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 8)              and b is -> (8,)\n",
      "\n",
      "\n",
      "    Currently at 100.0% of dataset"
     ]
    }
   ],
   "source": [
    "Model_LWF_2 = Custom_Layer(model)\n",
    "for i in range(0, num_epochs):\n",
    "    trainOneEpochOL_LWF_v2(Model_LWF_2, mixed_data_all, mixed_label_all, learn_rate, batch_size_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Performing training with OL METHOD - STOCHASTICH\n",
      "\n",
      "\n",
      "\n",
      "    New letter detected -> letter \u001b[1mB\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 6)              and b is -> (6,)\n",
      "\n",
      "\n",
      "    Currently at 33.55% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mR\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 7)              and b is -> (7,)\n",
      "\n",
      "\n",
      "    Currently at 66.78% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mM\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 8)              and b is -> (8,)\n",
      "\n",
      "\n",
      "    Currently at 100.0% of dataset"
     ]
    }
   ],
   "source": [
    "Model_OL_all_mixed = Custom_Layer(model)\n",
    "for i in range(0, num_epochs):\n",
    "    trainOneEpoch_OL(Model_OL_all_mixed, order_data_new, order_label_new, learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Performing training with OL METHOD - MINI BATCH\n",
      "\n",
      "    Currently at 24.94% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mB\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 6)              and b is -> (6,)\n",
      "\n",
      "\n",
      "    Currently at 50.12% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mR\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 7)              and b is -> (7,)\n",
      "\n",
      "\n",
      "    Currently at 75.06% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mM\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 8)              and b is -> (8,)\n",
      "\n",
      "\n",
      "    Currently at 100.0% of dataset"
     ]
    }
   ],
   "source": [
    "Model_OL_mini = Custom_Layer(model)\n",
    "for i in range(0, num_epochs):\n",
    "    trainOneEpoch_OL_miniBatch(Model_OL_mini, order_data_all, order_label_all, learn_rate, batch_size_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Performing training with CWR METHOD - STOCASTICH \n",
      "\n",
      "\n",
      "\n",
      "    New letter detected -> letter \u001b[1mM\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 6)              and b is -> (6,)\n",
      "\n",
      "\n",
      "    Currently at 0.25% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mB\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 7)              and b is -> (7,)\n",
      "\n",
      "\n",
      "    Currently at 0.49% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mR\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 8)              and b is -> (8,)\n",
      "\n",
      "\n",
      "    Currently at 100.0% of dataset"
     ]
    }
   ],
   "source": [
    "Model_OL_v2 = Custom_Layer(model)\n",
    "for i in range(0, num_epochs):\n",
    "    trainOneEpoch_OL_v2(Model_OL_v2, mixed_data_all, mixed_label_all, learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with OL v2 + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Performing training with CWR - MINI BATCH \n",
      " \n",
      "\n",
      "\n",
      "    New letter detected -> letter \u001b[1mM\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 6)              and b is -> (6,)\n",
      "\n",
      "\n",
      "    Currently at 0.25% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mB\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 7)              and b is -> (7,)\n",
      "\n",
      "\n",
      "    Currently at 0.49% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mR\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 8)              and b is -> (8,)\n",
      "\n",
      "\n",
      "    Currently at 100.0% of dataset"
     ]
    }
   ],
   "source": [
    "Model_OL_v2_miniBatch = Custom_Layer(model)\n",
    "for i in range(0, num_epochs):\n",
    "    trainOneEpoch_OL_v2_miniBatch(Model_OL_v2_miniBatch, mixed_data_all, mixed_label_all, learn_rate, batch_size_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with CWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Performing training CWR \n",
      " \n",
      "\n",
      "\n",
      "    New letter detected -> letter \u001b[1mM\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 6)              and b is -> (6,)\n",
      "\n",
      "\n",
      "    Currently at 0.25% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mB\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 7)              and b is -> (7,)\n",
      "\n",
      "\n",
      "    Currently at 0.49% of dataset\n",
      "\n",
      "    New letter detected -> letter \u001b[1mR\u001b[0m \n",
      "\n",
      "    Now W is -> (300, 8)              and b is -> (8,)\n",
      "\n",
      "\n",
      "    Currently at 100.0% of dataset"
     ]
    }
   ],
   "source": [
    "Model_CWR = Custom_Layer(model)\n",
    "for i in range(0, num_epochs):\n",
    "    trainOneEpoch_CWR(Model_CWR, mixed_data_all, mixed_label_all, 0.00035, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATION PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res00, res01, res02 = myPlotT.testOL_v2(model, OL_testing_data)\n",
    "myPlotT.plotTestOL(res00, res01, res02, 'KERAS', 'origModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res10, res11, res12 = myPlotT.testOL(Model_OL_vowels, OL_testing_data)\n",
    "myPlotT.plotTestOL(res10, res11, res12, 'VOWELS', 'trainVowels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res20, res21, res22 = myPlotT.testOL(Model_OL_all_mixed, OL_testing_data)\n",
    "myPlotT.plotTestOL(res20, res21, res22, 'OL', 'trainOL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res30, res31, res32 = myPlotT.testOL(Model_OL_mini, OL_testing_data)\n",
    "myPlotT.plotTestOL(res30, res31, res32, 'OL + mini batch', 'trainOL_mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res40, res41, res42 = myPlotT.testOL(Model_LWF_1, OL_testing_data)\n",
    "myPlotT.plotTestOL(res40, res41, res42, 'LWF', 'trainLWF_v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWF + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50, res51, res52 = myPlotT.testOL(Model_LWF_2, OL_testing_data)\n",
    "myPlotT.plotTestOL(res50, res51, res52, 'LWF + mini batch', 'trainLWF_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res60, res61, res62 = myPlotT.testOL(Model_OL_v2, OL_testing_data)\n",
    "myPlotT.plotTestOL(res60, res61, res62, 'OL v2', 'trainOL_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OL v2 + mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res70, res71, res72 = myPlotT.testOL(Model_OL_v2_miniBatch, OL_testing_data)\n",
    "myPlotT.plotTestOL(res70, res71, res72, 'OL v2 + mini batch', 'trainOL_v2_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res80, res81, res82 = myPlotT.testOL(Model_CWR, OL_testing_data)\n",
    "myPlotT.plotTestOL(res80, res81, res82, 'CWR', 'trainCWR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPlotT.plotAllTEst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('In this simulation the best trained model is:')\n",
    "print(f'ORIG:         {round(np.sum(res00)/np.sum(res02),4)*100}')\n",
    "print(f'VOWELS:       {round(np.sum(res10)/np.sum(res12),4)*100}')\n",
    "print(f'OL:           {round(np.sum(res20)/np.sum(res22),4)*100}')\n",
    "print(f'OL + mini:    {round(np.sum(res30)/np.sum(res32),4)*100}')\n",
    "print(f'LWF:          {round(np.sum(res40)/np.sum(res42),4)*100}')\n",
    "print(f'LWF + mini:   {round(np.sum(res50)/np.sum(res52),4)*100}')\n",
    "print(f'OL v2:        {round(np.sum(res60)/np.sum(res62),4)*100}')\n",
    "print(f'OL v2 + mini: {round(np.sum(res70)/np.sum(res72),4)*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down in txt files all the results across 10 or so simulations          \n",
    "WRITE_SIMU_RES = 0\n",
    "                \n",
    "if(WRITE_SIMU_RES==1):\n",
    "    myWrite.writeSimuRes('orig',      res00, res01, res02)\n",
    "    myWrite.writeSimuRes('vowels',    res10, res11, res12)\n",
    "    myWrite.writeSimuRes('OL',        res20, res21, res22)\n",
    "    myWrite.writeSimuRes('OL_mini',   res30, res31, res32)\n",
    "    myWrite.writeSimuRes('LWF',       res40, res41, res42)\n",
    "    myWrite.writeSimuRes('LWF_mini',  res50, res51, res52)\n",
    "    myWrite.writeSimuRes('OL_v2',     res60, res61, res62)\n",
    "    myWrite.writeSimuRes('OL_v2_min', res70, res71, res72)\n",
    "    myWrite.writeSimuRes('CWR',       res80, res81, res82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_PLOTS = 0\n",
    "\n",
    "if(ENABLE_PLOTS==1):\n",
    "\n",
    "    myPlotD.plotSimuRes(1)\n",
    "\n",
    "    myPlotD.plotDatasetStructure(TF_data_train,TF_data_test,\n",
    "                                 OL_data_train_vow,OL_data_test_vow,\n",
    "                                 B_train_data,B_test_data,\n",
    "                                 R_train_data,R_test_data,\n",
    "                                 M_train_data,M_test_data)\n",
    "\n",
    "    myPlotD.plotDatasetOL(OL_data_train_vow,OL_data_test_vow,\n",
    "                          B_train_data,B_test_data,\n",
    "                          R_train_data,R_test_data,\n",
    "                          M_train_data,M_test_data)\n",
    "\n",
    "    myPlotD.plotDatasetTF(TF_data_train,TF_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts for generating libraries for the STM code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_STM_FILES = 0\n",
    "\n",
    "if(WRITE_STM_FILES==1):\n",
    "    myWrite.writeSampleB()\n",
    "    myWrite.writeSampleMix()\n",
    "    myWrite.writeLastLayer(model)\n",
    "    myWrite.writeSampleVowels()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
